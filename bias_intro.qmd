---
title: "Sources of bias"
html:
  css: styles.css
---

Earlier, we discussed a wide range of [ethical considerations](paforimprovingss_ethics.qmd) that pertain to predictive analytics. Some of these considerations are related to bias, or fairness, of predictive models and how their results are used. This is an enormous, complex and extremely important topic. In this section, we provide an overview of key concepts.

**KH**: I personally hesitate to use the term "bias" because we are not always talking about statistical bias here.  I like "equity" or "fairness", but I understand sticking with bias!  If you do stick with bias though I think it is worth mentioning that sometimes algorithmic bias corresponds to statistical bias, but sometimes it is a different concept entirely.

**KH**: before you dive into bias, I would step back and provide a more basic introduction.  What is the definition of algorithmic bias?  What are the protected classes that we usually look at for bias metrics? (Sorry if you've already explained that elsewhere and I forgot!)

We begin with a discussion of the potential *sources* of bias in predictive analytics. The development and use of predictive analytics to improve services involves several steps. Each step presents different sources of bias:

-   [**Step 1: Estimating predicted probabilities of the outcome.**]{style="color:blue"}

-   [**Step 2: Translating results into recommendations for action.**]{style="color:blue"}

-   [**Step 3: Implementing decisions based on recommendations.**]{style="color:blue"}

The following pages discuss how bias may be introduced in each of these steps. To illustrate, we will use an example from the criminal justice system, when the consequences of bias are substantial for the lives of people affected. In particular, we will focus on the pretrial period. Across the country, release and detention decisions for defendants in the pretrial period are increasingly guided by algorithmic risk assessments. These assessments rely on data to estimate defendants' risks of negative outcomes if they are released pending trial. Negative outcomes include failing to appear for a court date, or being charged with new criminal activity. The algorithm's results are generally used by a judicial body to help determine whether a defendant will be released while waiting for a case to be resolved, and if so, under what conditions. The content is this section is summarized from @PorterRedcrossMiratrix2020.

**KH**: you might want to define pretrial period earlier in the paragraph--you define it at the end.

As we discuss the potential sources of bias in pretrial assessment, we'll focus on bias that can affect Black defendants for simplicity, although any of the ideas could be applied to other demographic subgroups.

![](images/ferriswheel.jpg){width="7in"}

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Predictive Analytics for Improving Social Services â€“ performance_metrics_placeholder</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Predictive Analytics for Improving Social Services</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Course overview</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./materials.html" rel="" target="">
 <span class="menu-text">Course materials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project.html" rel="" target="">
 <span class="menu-text">Assignments and project</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./references.html" rel="" target="">
 <span class="menu-text">References</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#understanding-performance-metrics" id="toc-understanding-performance-metrics" class="nav-link active" data-scroll-target="#understanding-performance-metrics">Understanding performance metrics</a>
  <ul class="collapse">
  <li><a href="#review-of-binary-prediction-metrics" id="toc-review-of-binary-prediction-metrics" class="nav-link" data-scroll-target="#review-of-binary-prediction-metrics">Review of binary prediction metrics</a></li>
  <li><a href="#auc-roc" id="toc-auc-roc" class="nav-link" data-scroll-target="#auc-roc">AUC ROC</a></li>
  <li><a href="#auc-pr" id="toc-auc-pr" class="nav-link" data-scroll-target="#auc-pr">AUC PR</a></li>
  </ul></li>
  <li><a href="#overall-learner-performance" id="toc-overall-learner-performance" class="nav-link" data-scroll-target="#overall-learner-performance">Overall learner performance</a>
  <ul class="collapse">
  <li><a href="#performance" id="toc-performance" class="nav-link" data-scroll-target="#performance">Performance</a></li>
  </ul></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section"></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<section id="understanding-performance-metrics" class="level1">
<h1>Understanding performance metrics</h1>
<p>The text below was pulled from 01_03 notebook.</p>
<section id="review-of-binary-prediction-metrics" class="level2">
<h2 class="anchored" data-anchor-id="review-of-binary-prediction-metrics">Review of binary prediction metrics</h2>
<p>First, we review some terms related to binary predictions.</p>
<ul>
<li>the <strong>true value</strong> for a unit (e.g., individual person) is the observed value (0 or 1).</li>
<li>the <strong>predicted probability</strong> for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.</li>
<li>the predicted probability can be converted into a <strong>predicted classification</strong> by comparing it to a selected threshold.</li>
<li>the <strong>threshold</strong> is a cutoff for classifying a positive outcome. All units with a predicted probability <em>equal to or above</em> the threshold are predicted to be a 1, and all units with a predicted probability <em>below</em> the threshold are predicted to be a 0. A common threshold of choice is 0.5.</li>
</ul>
<p>When we have predicted classifications, we can categorize all them into four categories:</p>
<ul>
<li>a <strong>true positive (TP)</strong> is when both the predicted outcome and the observed outcome are 1.</li>
<li>a <strong>false positive (FP)</strong> is when the predicted outcome is 1, but the observed outcome is 0.</li>
<li>a <strong>true negative (TN)</strong> is when both the predicted outcome and the observed outcome are 0.</li>
<li>a <strong>false negative (FN)</strong> is when the predicted outcome is 0, but the observed outcome is 1.</li>
</ul>
<p>Note that the notations <em>P</em> and <em>N</em> in the four categories above refer to <strong>predicted</strong> positives and <strong>predicted</strong> negatives. The <em>T</em> and <em>F</em> preceding the <em>P</em> and <em>N</em> is a determinant of whether the predicted positives and negatives are correct or not.</p>
<p>We can define two more useful notations:</p>
<ul>
<li><em>OP</em> is the number of <strong>observed</strong> positives (outcomes that are <code>1</code>).</li>
<li><em>ON</em> is the number of <strong>observed</strong> negatives (outcomes that are <code>0</code>).</li>
<li><em>PP</em> is the number of <strong>predicted</strong> positives (predictions that are <code>1</code>).</li>
<li><em>PN</em> is the number of <strong>predicted</strong> negatives (predictions that are <code>0</code>).</li>
</ul>
<p>Finally, we can also summarize these categories into rates.</p>
<ul>
<li>the <strong>true positive rate (TPR)</strong> is also known as <strong>sensitivity</strong> or <strong>recall</strong> and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.</li>
<li>the <strong>true negative rate (TNR)</strong> is also known as <strong>specificity</strong> and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.</li>
<li>the <strong>false positive rate (FPR)</strong> is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.</li>
<li><strong>specificity</strong> is <span class="math inline">\(1 - FPR\)</span>.</li>
<li>the <strong>false negative rate (FNR)</strong> is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.</li>
<li><strong>precision</strong> also known as <strong>positive predictive value</strong> is the number of true positives over the total number of predicted positives: TP / (TP + FP).</li>
<li><strong>negative predictive value</strong> is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).</li>
<li><strong>false discovery rate</strong> is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP.</li>
</ul>
</section>
<section id="auc-roc" class="level2">
<h2 class="anchored" data-anchor-id="auc-roc">AUC ROC</h2>
<p>AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.</p>
<p>An ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, <em>as the threshold changes.</em> The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as <code>1</code>, and all units with a predicted value below 0.95 will be classified as <code>0</code>. At this threshold, most of the units we classify as <code>1</code> will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as <code>1</code>, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a <em>high true positive rate</em> and a <em>low false positive rate</em>, so points farther to the left and farther up are markers of a good model.</p>
<p>We usually compare a modelâ€™s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.</p>
<p>In the case of <strong>imbalanced data</strong>, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.</p>
</section>
<section id="auc-pr" class="level2">
<h2 class="anchored" data-anchor-id="auc-pr">AUC PR</h2>
<p>AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for <strong>imbalanced data</strong> set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between <em>true positive rate</em> (on the Y-axis) and <em>false positive rate</em> (on the X-axis).</p>
<p>In <strong>imbalanced data</strong>, the <em>false positive rate</em> tends to remain <em>low</em> due to a large number of observed negatives. Recall that <em>false positive rate/FPR</em> = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive <strong>rate</strong> even as the <strong>number</strong> of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.</p>
<p>AUC PR, on the other hand, is a trade-off between <em>precision/predictive positive rate</em> (TP/(TP + FP)) and <em>recall/sensitivity</em> (TP/(TP + FN)). <em>Precision</em> is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).</p>
<p>Unlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis <em>as the threshold changes</em>.</p>
<p>As in shown in <strong>Figure 1.2</strong> below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as <code>1</code>. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as <code>1</code>. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).</p>
<p>As shown in <strong>Figure 1.3</strong>, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.</p>
<p>A no skills classifier is a learner that predicts every observation as <code>1</code>. Suppose the data is extremely imbalanced, and only <strong>0.3</strong> percent of the total observations are <code>1</code>. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifierâ€™s AUC PR curve is the horizontal line across the bottom of <strong>Figure 1.3</strong>.</p>
<p><strong>Figure 1.1</strong> An example of ROC and precision-recall curves across different learners.</p>
<p><img src="figures/compare.png" class="img-fluid"></p>
<p><strong>Figure 1.2</strong> An example of a PR curve.</p>
<p><img src="figures/auc_pr_threshold.png" class="img-fluid"></p>
<p><strong>Figure 1.3</strong> An example of a PR curve across perfect, good and no skills learners.</p>
<p><img src="figures/auc_pr_perfect_good_noskills.png" class="img-fluid"></p>
</section>
</section>
<section id="overall-learner-performance" class="level1">
<h1>Overall learner performance</h1>
<section id="performance" class="level2">
<h2 class="anchored" data-anchor-id="performance">Performance</h2>
<p>We now compare the learners according to both ROC AUC and PR AUC.According to the mean values of these metrics, the best learners are:</p>
<ul>
<li><strong>AUC_ROC</strong>: <code>r best_roc_auc</code></li>
<li><strong>AUC_PR</strong>: <code>r best_pr_auc</code></li>
</ul>
<p>It is common for these metrics to show different learners as the best performer. We may want to consider the results holistically to select the best learners. For example, consider the following toy result:</p>
<table class="table">
<thead>
<tr class="header">
<th>learner</th>
<th style="text-align: right;">AUC_ROC</th>
<th style="text-align: right;">AUC_PR</th>
<th style="text-align: right;">MSE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>learner_1</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">2.9</td>
</tr>
<tr class="even">
<td>learner_2</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">0.6</td>
<td style="text-align: right;">2.85</td>
</tr>
</tbody>
</table>
<p>If we look at all the metrics, learner_2 is the best according to MSE. However, it is only marginally better on MSE, and performs substantially worse on both AUC metrics compared to learner_1. Thus, we would probably not consider learner_2 to be one of our best performing learners.</p>
<p>After understanding broad learner performance, it is often helpful to select a subset of the learners to examine in more detail. For example, we may select only the best learners according to our performance metrics above. Alternatively, we may want to select learners that are more interpretable, even if they are not the best performers. For example, if a simple regression model performs <em>almost</em> as well as a more complicated algorithm, we may still want to select the regression model even if it is not the absolute best performer on our metrics.</p>
</section>
</section>
<section id="section" class="level1">
<h1></h1>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
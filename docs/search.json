[
  {
    "objectID": "performance_metrics_PLACEHOLDER.html",
    "href": "performance_metrics_PLACEHOLDER.html",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "The text below was pulled from 01_03 notebook.\n\n\nFirst, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP.\n\n\n\n\nAUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.\n\n\n\nAUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "href": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "First, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "href": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "href": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#performance",
    "href": "performance_metrics_PLACEHOLDER.html#performance",
    "title": "1 Understanding performance metrics",
    "section": "Performance",
    "text": "Performance\nWe now compare the learners according to both ROC AUC and PR AUC.According to the mean values of these metrics, the best learners are:\n\nAUC_ROC: r best_roc_auc\nAUC_PR: r best_pr_auc\n\nIt is common for these metrics to show different learners as the best performer. We may want to consider the results holistically to select the best learners. For example, consider the following toy result:\n\n\n\nlearner\nAUC_ROC\nAUC_PR\nMSE\n\n\n\n\nlearner_1\n0.8\n0.7\n2.9\n\n\nlearner_2\n0.7\n0.6\n2.85\n\n\n\nIf we look at all the metrics, learner_2 is the best according to MSE. However, it is only marginally better on MSE, and performs substantially worse on both AUC metrics compared to learner_1. Thus, we would probably not consider learner_2 to be one of our best performing learners.\nAfter understanding broad learner performance, it is often helpful to select a subset of the learners to examine in more detail. For example, we may select only the best learners according to our performance metrics above. Alternatively, we may want to select learners that are more interpretable, even if they are not the best performers. For example, if a simple regression model performs almost as well as a more complicated algorithm, we may still want to select the regression model even if it is not the absolute best performer on our metrics."
  },
  {
    "objectID": "scoping_usefindings.html",
    "href": "scoping_usefindings.html",
    "title": "How will findings be used?",
    "section": "",
    "text": "The social services department had an established practice of offering additional support to clients requiring extra assistance. This encompassed:\n\nInitiating check-in calls with clients,\nFacilitating supplementary casework sessions, and\nArranging extra workshops to enhance job search and skills.\n\nGiven the department’s constraints in terms of staff and financial resources, their aim was to effectively direct extra services to those in greatest need. They recognized that their current method of prioritizing such clients could be enhanced by distilling a wide range of potentially predictive information into a single “risk score.”\nA few things to note:\n\nThey did not want, nor did the project team recommend, for their decision-making process to become completely dependent on the risk scores.\nInstead, the risk scores could potentially provide a valuable piece of information (if we could validate good predictive performance and minimal bias).\nThe department planned to train caseworkers in how to interpret and incorporate the risk score into their decision-making and prioritizing.\nThe department also wanted to look at how risk scores varied across and within populations served by offices throughout the state. This analysis could help them understand which offices were more in need of support for reaching out to clients and for planning workshops.\n\nThis information about how the department plans to use the information can guide how we scope the predictive analytics project.\nRemember, we also want to frequently check in with our ethical considerations. What ethical considerations do we want to be careful about as we proceed?\n\n\n\n Back to top"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "This is assignment 1",
    "section": "",
    "text": "This is assignment 1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_define.html",
    "href": "paforimprovingss_define.html",
    "title": "What is predictive analytics (PA)?",
    "section": "",
    "text": "Predictive analytics (PA) (or predictive modeling) is the use of historical data to forecast future outcomes.\n\n\n\n\n\n\n\nThe complexity of PA may vary. For example, PA may rely on relatively simple approaches, such as models with a small number of measures (i.e. predictors, variables). Alternatively, in settings with large amounts of data, they may involve complex machine-learning algorithms.\nPA may make predictions of binary outcomes, which take just two values such as yes or no. For example, we can predict whether students will graduate from high school on time or not. With binary outcomes, predictive analytics can also be used to estimate the probability of the outcome occurring. For example, we can predict the probability that a student will graduate from high school. This probability is between 0 and 1.\nPA may also forecast continuous outcomes. For example, we can predict the grade point average (GPA) students will achieve.\nTo narrow the scope of the class, we will focus only on binary outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "1 Introduction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_old.html",
    "href": "index_old.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nInsert overview of these training materials here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Course overview",
    "section": "Syllabus",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#classroom-expectations-and-norms",
    "href": "index.html#classroom-expectations-and-norms",
    "title": "Course overview",
    "section": "Classroom expectations and norms",
    "text": "Classroom expectations and norms"
  },
  {
    "objectID": "index.html#six-ways-to-get-help",
    "href": "index.html#six-ways-to-get-help",
    "title": "Course overview",
    "section": "Six ways to get help",
    "text": "Six ways to get help"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Course overview",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Overview",
    "section": "",
    "text": "These materials are designed to help you understand the main topics we’ll cover in class. It’s best to read them before each class, so we can use our time to review important points and discuss any questions. I’ll let you know what to read before each class.\n\nHow to use these materials\nThe class is organized into the sections listed in the left panel of this page. Clicking the arrow to the right of each main section title will show the sub-sections. Generally, we will cover one or two main sections per class. Some sections are currently empty, but will be added as the class progresses.\nThe materials consist of text, links (to articles, blog posts, videos, etc.), and snippets of the code we will use for analyses. Please note:\n\nYou should use the hyperlinks in these materials unless otherwise noted. Most are short.\nIf hyperlinks are optional, this will be clearly stated. They provide an opportunity to do a deeper dive into a topic if you are interested.\nIn addition, you will see citations throughout. There is no expectation for you to read the cited papers unless you want to.\n\n\n\nAcknowledgements\nThese course materials were put together by me, Kristin Porter. All errors and omissions are mine. But I have also had lots of help - many sections draw on and even copy valuable text written by Zarni Htet at MDRC and Kristen Hunter at the University of South Wales, Sydney. The framework presented in this class was developed at MDRC, a nonprofit social policy research organization where I was employed for 16 years. Kristen Hunter also provided very helpful review of all these materials.\nThis is the first time I have taught this course, and I would not be surprised if there are problems. I very much welcome any and all feedback and questions at any time.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_ex.html",
    "href": "codeofethics_ex.html",
    "title": "Examples and best practices",
    "section": "",
    "text": "Examples and best practices\n\n\n\n\n Back to top"
  },
  {
    "objectID": "course_description.html",
    "href": "course_description.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. In this class, students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\nPrerequisites: Students should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_why.html",
    "href": "codeofethics_why.html",
    "title": "Why create a code of ethics before starting PA?",
    "section": "",
    "text": "Why create a code of ethics before starting PA?\n\n\nThis is a test.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "1 Papers to maybe work into class…",
    "section": "",
    "text": "References\n\n\n\n\n\n1 Papers to maybe work into class…\nThe Dangers of Risk Prediction in the Criminal Justice System\nAbstract: Courts across the United States are using computer software to predict whether a person will commit a crime, the results of which are incorporated into bail and sentencing decisions. It is imperative that such tools be accurate and fair, but critics have charged that the software can be racially biased, favoring white defendants over Black defendants. We evaluate the claim that computer software is more accurate and fairer than people tasked with making similar decisions. We also evaluate, and explain, the presence of racial bias in these predictive algorithms.\nAn Algorithm That Grants Freedom, or Takes It Away\n2020 NYT article summarizing predictive analytic applications and bias in multiple parts of the CJ system, as well as detecting fraud in the welfare system.\nPredicting Participation in Healthy Marriage and Responsible Fatherhood Programs (and nice technical appendices) from Mathematica/OPRE\n\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "1 Assignments and project",
    "section": "",
    "text": "1 Assignments and project\n\nFor a selected scenario, code of ethics.\nFor a selected scenario, prediction target: what, when, for whom; how will results be used?\nIdentify data set. Summarize prediction target, predictors. Create meta data. Fill in first part of PA toolset?\nData preparation, create predictors. Data model.\nComplete first full page of PA toolset - specify learners.\nCompare learners in terms of performance.\nCompare learners in terms of bias.\nTest link to assignments\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html",
    "href": "paforimprovingss_ethics.html",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "scoping_ex.html",
    "href": "scoping_ex.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "A motivating example\nLet’s begin with an example. This example is inspired by a project I was involved in, but I have made several modifications for illustration purposes. The project was in collaboration with a state agency overseeing the federal assistance program, Temporary Assistance for Needy Families (TANF). TANF’s primary goal is to empower families towards self-sufficiency by providing essential resources and services. Within this framework, state agencies administering the program extend a range of supportive services. These services are designed to assist families in overcoming obstacles to employment and self-reliance. Their approach revolves around tailoring assistance to the specific circumstances and needs of each family, ensuring a targeted response to diverse challenges and varying levels of need among households.\n\n\n\n\n\n\nThe agency came to the data science team with the following questions:\n\nWith a data-driven tool, could caseworkers better understand which clients were more at risk of not reaching key program outcomes? In particular, among those who were required to find employment, could they obtain reliable information about who was least likely to find employment? Note that the current practice for identifying clients who were most at-risk relied on caseworkers reviewing measures they thought were important (e.g. of previous employment history) and making a subjective decision based on their professional expertise. They wondered if predictive analytics would provide an improvement in terms of accuracy and fairness.\nCould we develop reliable risk scores that combine and weigh multiple risk factors into a single easy-to-understand number that can be incorporated into decision-making?\nCould trends in risk scores across the population of TANF clients provide new insights for program improvement and resource allocation?\n\n\n\nHow do we scope a project that helps them achieve their goals?\nNext, we will use this example to talk about project scoping. That is, we need to specify the following:\n\nHow would information about clients’ risks of adverse outcomes be used?\nWhat, exactly, are we trying to predict?\nFor whom would we be making predictions?\nWhen would we be making the predictions?\nWhen specifying what, when and for whom we are making predictions - what data are available? What do the data allow?\n\nAnswering these questions allows us to move forward in planning a potential predictive analytics project. That is what I mean by scoping - what potential project could there be? But importantly, the next step is to investigate that potential - to see if we can reliably, clearly and fairly make useful and actionable predictions. I refer to this next step as the “proof-of-concept,” which we will turn to in the next main section.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_learn.html",
    "href": "paforimprovingss_learn.html",
    "title": "How can PA be used to improve services?",
    "section": "",
    "text": "Predictive analytics has a wide variety of applications, from predicting financial markets to personalized medicine. The potential of predictive analytics in the services sector is to help government programs, nonprofit service providers or businesses identify those clients who could most benefit from targeted interventions. This targeting can facilitate effective service delivery at an efficient cost.\nWhy just focus on predictive analytics for improving services? Predictive analytics has vast applications. Different applications raise different issues with respect to project scoping, ethics and data and technical considerations. We focus on improving services because it allows us to build a framework that is driven on how results will be used. Also, my background is in using predictive analytics (as well as other data science and statistical methods) to strengthen social services, so it is the application to which I have given the most thought. But at the same time, many of the lessons we will learn will be helpful for other applications as well, and there will be flexibility in the data you use for your projects.\n\nSome examples of using predictive analytics to improve social services:\nDuring my time at MDRC, the nonprofit research organization from which the framework for these materials originated, my colleagues and I explored and applied predictive analytics in various ways:\n\nPredicting K-12 academic outcomes: School districts often utilize “early warning systems” (EWS) that rely on student-level data. These systems help identify students who might be at risk of not achieving important educational milestones, such as timely graduation or passing state exams. For instance, the probability of not graduating high school is often gauged using the ABC indicators, which stand for attendance, behavior, and course performance. This indicator system represents a simpler form of predictive analytics. However, as schools increasingly collect richer, data sets with frequent data updates of many student measures, including daily attendance, exam scores, and course marks, there has been opportunity to compute more accurate, frequent and nuanced predictions of student risk.\nProviding early warnings of not reaching milestones in an employment training program. This project focused on harnessing granular, longitudinal administrative data to build a system for ongoing, advanced analytics that support the continuous improvement process at the Center for Employment Opportunities (CEO). The goal was for these early warnings to be transmitted, practically in real time, to front-line case workers and leaders. The information would be part of standard dashboards and data protocols. CEO planned to train staff members to act on this information, and to work with MDRC to design, implement, and test new interventions based on insights provided by the predictive analytics results. Unfortunately, this project was interrupted by the COVID-19 pandemic.\nIdentifying TANF participants who were most likely to find employment. TANF is the Temporary Assistance to Need Families program that provides time-limited support for families’ basic needs. The federal government provides grants to states to run the TANF program. As part of MDRC’s TANF Data Collaborative Project, a team at the Virginia Department of Social Services (VDSS) sought to develop analytic tools to help TANF case workers customize education/employment-related services to increase the likelihood of participants’ labor market success after they leave the program. The team investigated whether demographic characteristics, household compositions, receipt of other public benefits, and past education/employment-related activities could predict success, and how to construct an unbiased predictive tool using such variables.(MDRC, 2023)\nIdentifying families at risk of disengaging from a home visiting program. Child First is a home visiting program that aims to promote high-quality relationships between caregivers and children in families experiencing challenges related to caregiver mental health and child behavior. Staff members provide intensive in-home clinical services to both the caregiver and child. They also connect families to additional services such as financial and housing support, health care, and treatment for disorders such as substance abuse. To accomplish its goals, Child First must ensure that families remain consistently engaged in program services over time. When families leave before being officially discharged from the program, they receive truncated interventions that are likely to be less effective at improving outcomes. Early disengagement is also expensive, given the large, fixed costs of enrolling new families into the program. Child First prioritizes collecting high-quality data to understand the population of families it serves. For example, the program collects information on a range of family characteristics assessed at intake, including sociodemographic information on both the caregiver and child, health-related information like insurance status and child DSM-5 diagnoses, as well as several risk measures and assessments such as adverse childhood experiences (ACEs) that capture experiences with violence, abuse or neglect, and household instability. The availability of these data provided researchers with a singular opportunity to explore whether predictive analytics could be a useful tool to summarize the large amounts of information the program collects and use it to help staff members identify families at particularly high risk of early disengagement, defined as being enrolled in the program for fewer than 90 days. That information could allow Child First staff members to triage families better at intake and provide more intensive support and services to those families at risk of early disengagement.(Xia, Htet, Porter, & McCormick, 2023)\nPretrial justice: Many jurisdictions across the United States are rethinking the “front end” of the criminal justice system — the pretrial period between an arrest and the disposition of a criminal case. Often these reforms focus on the initial decisions that judges and other court stakeholders make about whether to detain individuals in jail while they are awaiting trial, and on the use of money bail as a tool for ensuring that people will show up to court hearings. In most jurisdictions, the majority of people in jail at any point in time are awaiting trial, and many are there because they cannot afford to post bail. Jurisdictions are looking for fairer, more cost-effective approaches to the pretrial phase of the system. To assist jurisdictions in making better initial decisions, the Laura and John Arnold Foundation (now Arnold Ventures) developed the Public Safety Assessment (PSA), a tool that uses data on an individual’s history with the justice system and the current offense to predict the probability that the person will show up to hearings or be arrested for a new crime if released. The PSA aims to help judges make more informed, less subjective decisions about pretrial detention. It is currently used in nearly 40 jurisdictions across the nation.(Redcross & Henderson, 2019) (Golub, Redcross, & Valentine, 2019)\n\nLater, we will return to some of the above examples to discuss various issues, including ethics, project scoping or set-up, the trade-offs of different modeling approaches, and more. Here are some other examples of predictive analytics being used in the social service sector with the goal to strengthen programs’ services:\nChild welfare: An example of PA being used in child welfare is the Allegheny Family Screener Tool. First implemented in 2016, it was developed in a partnership between researchers from Auckland University of Technology and the Allegheny County Office of Children, Youth, and Families, a Pennsylvania child welfare system. The research team wanted to use predictive analytics to help inform and improve decisions made by staff when determining whether reports of possible child abuse and neglect should be marked for further investigation, rather than replace human decision making altogether. The tool summarizes vast amounts of information across multiple databases to provide a risk score to child welfare call screeners. The researchers worked closely with the child welfare agency and partner organizations to discuss implementation of the tool and results, and feedback from community meetings informed how the tool was developed. An independent evaluation found that it increased the staff’s ability to accurately screen reports and pursue investigations. Also, the tool did not increase the rate of children screened in for investigation. That is, using it resulted in a different pool of children being identified as needing child welfare intervention, but did not substantially increase the proportion of children investigated among all children referred for maltreatment. The model and its implementation have been updated over time so that its predictions reflect contemporary information on families currently being served.(Human Services, 2019)\nLead poisoning prevention: The Chicago Department of Public Health partnered with the Data Science for Social Good initiative at the University of Chicago to help find the homes that are most likely to still contain lead-based paint hazards. From their website: “By building statistical models that predict exposure based on evidence such as the age of a house, the history of children’s exposure at that address, and economic conditions of the neighborhood, CDPH and their partners can link high-risk children and pregnant women to inspection and lead-based paint mitigation funding before any harm is done.” This integrated and innovative system will ensure resources are used most efficiently, and ultimately will mean healthier Chicago children. This short video summarizes the project.\nCriminal justice: In addition to applications in the pretrial periods, predictive analytics is being applied to other parts of the criminal justice system. For example, predictive analytics has been used for “predictive policing” based on forecasting future crime at the community level, for guiding sentencing and probation, for detecting fraud, for assessing young people’s risk of becoming involved in crime and more. As you are surely aware, bias is an enormous concern and this topic alone could take up a whole courses. We will not have the time to delve is as deep as is warranted due to time constraints, but we will discuss this topic later, and I will provide some ideas for further reading. For now, this semi-recent New York Times article provides a brief overview of some applications and concerns about bias. As an optional, longer read, you can check out this Pro-Publica story about algorithmic bias in sentencing.\nHealth care: The adoption of electronic health records (EHRs) by most US health care systems for patient care has led to an explosion of predictive analytics in health care - with applications aimed at improving health outcomes, care coordination, and quality of care. Health care systems and insurance companies harness patient demographics, insurance claims data, and clinical characteristics in EHRs to create statistical models of future health care risks and resource utilization. There are also efforts to incorporate social and behavioral determinants of health (SBDH), which include measures of diet and physical activity as well as characteristics of patients’ neighborhoods, such as food access and transportation.\nThe above examples do not capture the breadth of growing applications of using predictive analytics to improve social services. To read about some additional examples, see the following short articles and blog posts. These are OPTIONAL.\n\nAnticipatory government: Preempting problems through predictive analytics\nCatalogue of predictive models in the humanitarian sector\nWhat is predictive analytics and what could it mean for local governments?\n\n\n\nSome examples of using predictive analytics to improve consumer services:\nI do not have first-hand experience in using or reviewing predictive analytics to improve consumer services, but of course there are many! Applications in social services have been inspired by applications in consumer services. Here are a few articles that provide some good summaries and discussion. Not all applications discussed focus on estimating binary outcomes, but many, such as predicting customer churn, product purchase or customer satisfaction can be examples of predicting binary outcomes.\n\nPredictive customer insight is the future\nHow predictive analytics can improve customer experience\n\nQuestions for discussion:\n\nWhat examples do you find interesting for using predictive analytics to improve services? Why?\nWhat promise do you see in the examples you read about or know about?\nWhat concerns do you have?\n\n\n\n\n\n\n Back to topReferences\n\nGolub, C., Redcross, C., & Valentine, E. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment effects of new jersey’s criminal justice reform.\n\n\nHuman Services, A. C. D. of. (2019). Impact evaluation summary of the allegheny family screening tool.\n\n\nMDRC. (2023). Virginia pilot: TANF data collaborative.\n\n\nRedcross, C., & Henderson, B. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment.\n\n\nXia, S., Htet, Z., Porter, K. E., & McCormick, M. (2023). Exploring the value of predictive analytics for strengthening home visiting: Evidence from child first."
  },
  {
    "objectID": "paforimprovingss_limitations.html",
    "href": "paforimprovingss_limitations.html",
    "title": "What are the limitations of PA for improving services?",
    "section": "",
    "text": "While PA has the potential to provide valuable insights for improving services, it also has many limitations that data scientists need to consider when scoping, implementing and communicating plans and results. The following provides a brief overview of some limitations of PA. We will return to these in more depth later.\n\nPredictions are only as good as the data on which they are built. If the dataset available for analysis is incomplete, inaccurate, or biased, it can lead to unreliable predictions.\nThere can be a trade-off between model performance and transparency and simplicity. Sometimes the best-performing models are complex machine learning algorithms and can therefore be difficult to explain and can be challenging to deploy and maintain. A lack of transparency can be especially problematic in certain domains with more ethical considerations.\nExternal factors may change: Predictive models rely on patterns in the past to make predictions about the future. But models may not account for unexpected events, external influences, or sudden changes in the environment that can substantially impact the outcome being predicted. As the time horizon extends further into the future, models may become unreliable. For this reason, predictive models need regular monitoring and may require frequent updating.\nPredictive analytics tells us nothing about causal relationships. Nor does it tell us that those with the lowest or highest probability of an outcome are the most likely to benefit from a new or revised service or intervention. This evaluation is determined by other analyses (e.g. rigorous evaluations), or is sometimes assumed based on the expertise of those using predictive analytics findings for decision-making.\nEthical Concerns: Predictive analytics can raise many ethical concerns. Ethics is a very important topic, which we turn to next and will return to throughout the course.\n\nMany of the lessons we learn in inferential statistics do not apply to predictive analytics. You can find a short discussion of some of the differences between predictive analytics and inferential statistics here. This is OPTIONAL.\nAnd for fun, Baba Brinkman covers some of these issues in his rap comparing and contrasting data science and statistics. (Click link below.) (Also OPTIONAL)\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_context.html",
    "href": "paforimprovingss_context.html",
    "title": "Is predictive analytics a good fit for your context?",
    "section": "",
    "text": "Before embarking on a PA project, it is valuable to assess whether it is the right approach at the right time for the setting. Here are a some key considerations to determine if PA is a good fit.\n\nWill PA provide new and actionable information?\n\nDo we anticipate that the PA findings will provide information that is more reliable, more frequent or easier to understand than current information available?\nHow will findings from PA be communicated and used, and by whom?\nHow frequently do PA findings need to be updated?\n\n\n\nAre there plans and buy-in for acting on insights that come out of PA?\n\nIs there sufficient leadership and commitment from the institution?\nIs there capacity for interpreting, communicating and and acting on PA results? How will findings be communicated?\nAre there plans for how program services will be changed or how new services will be introduced based on PA results?\nTo what extent does decision making and service provision currently rely on data-driven findings? Are data-driven findings in general, and PA findings more specifically, trusted and used? If not, are there plans for addressing practice and culture to support change?\nAre there structural or process factors that impede or foster the use predictive analytics (such as dedicated data teams) or the use of results (such as dashboard systems)?\n\n\n\nData quality and systems\n\nAre there sufficient data that are readily accessible?\nAre the data sufficiently documented and understood so that reliable measures (variables to be used as outcomes or predictors) can be created for PA?\nIs there sufficient capacity for investigating and deploying PA with existing analytic systems, software and staffing?\n\n\n\nPotential risks\n\nHas their been sufficient investigation into identifying potential risks?\nAre there systems or processes in place to make sure ethical issues are attended to throughout a PA project?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_optional_PAvsInfStat.html",
    "href": "paforimprovingss_optional_PAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "OptionalPAvsInfStat.html",
    "href": "OptionalPAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of all Gentoo penguins in the world? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches). To construct these estimates about our population, we use information from a sample we have collected, such as a set of 20 Gentoo penguins caught and measured by ecologists.\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "issuelog.html",
    "href": "issuelog.html",
    "title": "Issue log",
    "section": "",
    "text": "Formatting of references weird - how to capitalize where needed?\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "href": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "href": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders - to include as much as possible the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\n\nEthical Considerations in Predictive Analytics Projects\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices to follow:\n\nUnderstanding Regulatory Frameworks: The first step is to be well-versed in relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and ensures that ethical considerations are thoroughly explored from various perspectives.\nDevelop a Code of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nMinimizing Bias: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nOutcome Assessments: Conduct comprehensive outcome assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these practices, organizations can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "scoping_who.html",
    "href": "scoping_who.html",
    "title": "Who are we making predictions for?",
    "section": "",
    "text": "Our target population is specifically adult TANF recipients who are eligible and approved for the TANF department’s education and training program and who are required to work.\nOur data sets include a broader group of TANF applicants and participants so we need to be careful to exclude those who do not meet our criteria.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressEthical",
    "href": "paforimprovingss_ethics.html#addressEthical",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders. As much as possible, we should try to include the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices that may be helpful or necessary, depending on the project:\n\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and helps ensure that ethical considerations are thoroughly explored from various perspectives.\nCode of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nRegulatory Frameworks: Teams should be well-versed in relevant regulations and laws that may apply to their data and context, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nBias Minimization: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nProject Assessments: Conduct comprehensive PA project assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This training ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these kinds of practices (when they apply to the context), government agencies, organizations and companies can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "course_syllabus.html",
    "href": "course_syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_grading.html",
    "href": "course_grading.html",
    "title": "Assessment and grading",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_attendancenorms.html",
    "href": "course_attendancenorms.html",
    "title": "Class attendance and participation norms",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_gethelp.html",
    "href": "course_gethelp.html",
    "title": "Six ways to get help",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Course information",
    "section": "",
    "text": "This section of this course website provides course information.\n\n\n\n Back to top"
  },
  {
    "objectID": "course_syllabus.html#course-details",
    "href": "course_syllabus.html#course-details",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.html#course-description",
    "href": "course_syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nDescription:\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. Students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\n\n\nPrerequisites:\nStudents should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\nLearning objectives:\n\nGain the ability to critically assess the potential value and limitations of predictive analytics for improving services when applied to a particular context and data system.\nUnderstand how results from predictive analytics can provide actionable insights for improving services.\nUnderstand how to plan and implement all steps of a predictive analytics workflow using open-source code templates. The steps include planning the analysis to match goals, exploring and preparing data, specifying and training models, and assessing the validity and bias of predictive models – all while prioritizing best-practices for ethics, transparency and replication.\nGain a high-level understanding of sta(s(cal concepts for optmizing predictive analytics and have op(onal choices to dive more deeply.\nPresent predictive analytics results using an R notebook and clearly communicate interpretations, cautions and recommendations.\n\n\n\nAbout the Data Science Academy:\nWelcome to an NCSU Data Science Academy Course! In July 2021, the universitywide and interdisciplinary Data Science Academy was launched to meet the growing needs of data science research, education and expertise in North Carolina and beyond. At NC State, Data Science is for Everyone. Data Science Academy (DSA) Courses are designed to make sure that each student can pursue appropriate level challenges through opportunities to make choices and pursue projects of interest. Whether you have never thought about data science before or bring experience and expertise, we welcome you. Our goal is that after each DSA class you want to learn more! For more on the DSA course design. \n\n\nAbout DSA research:\nTo make sure that we are providing an appropriate collection of courses with a variety of challenge levels within each course, we will be collecting data to help us build a practice of continuous improvement. The purpose of the data is to evaluate the DSA and how well we are serving our students. We hope to be able to share what we learn with other universities and researchers - we will ask your permission at the beginning of the course to be able to share your anonymized data when we communicate about the work of the DSA."
  },
  {
    "objectID": "course_syllabus.html#resources",
    "href": "course_syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThere is no textbook for this class. All reading materials are provided within this website and can be accessed in the “Course Materials” tab at the top of the page. The course materials includes text developed by the instructor as well as links to helpful reading, which may be required or optional as noted."
  },
  {
    "objectID": "course_syllabus.html#grading",
    "href": "course_syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nFORTHCOMING"
  },
  {
    "objectID": "course_syllabus.html#course-schedule",
    "href": "course_syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule is linked here."
  },
  {
    "objectID": "course_syllabus.html#attendance-policy",
    "href": "course_syllabus.html#attendance-policy",
    "title": "Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nFor complete attendance and excused absence policies, please see http://policies.ncsu.edu/regulation/reg-02-20-03. For coronavirus related attendance guidance, see the “coronavirus information” section.\nAttendance Policy. Students are expected to attend class regularly. Since we only meet once a week, each class is vitally important to keep on track during the semester.\nAbsences Policy. Students should notify the instructor that they will be unable to attend class and schedule a time to meet with the instructor to review materials they missed during class time.\nMakeup Work Policy. Makeup work will be allowed if the student notifies the instructor at least one day before an assignment is due to arrange for a deadline extension. Notification given on the due date will be considered on a case-by-case basis."
  },
  {
    "objectID": "course_syllabus.html#academic-integrity",
    "href": "course_syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nStudents are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01.\nStudents are expected to produce work that they personally completed and in accordance with the instructions given for the assignment, project, test or quiz. Work copied from other students, completed by another individual, or directly taken from other sources without appropriate attribution will be in violation of academic integrity. Students are expected to follow the Code of Student Conduct (NCSU POL11.35.01) and Pack Pledge, and violations of academic integrity will be handled in accordance with the Student Discipline Procedures (NCSU REG 11.35.02).\nHonor Pledge. Your submission on any test or assignment indicates “I have neither given nor received unauthorized aid on this test or assignment.” Digital Course Components Students may be required to disclose personally identifiable information to other students in the course, via digital tools, such as email or web-postings, where relevant to the course. Examples include online discussions of class topics, and posting of student coursework. All students are expected to respect the privacy of each other by not sharing or using such information outside the course."
  },
  {
    "objectID": "course_syllabus.html#digital-course-components",
    "href": "course_syllabus.html#digital-course-components",
    "title": "Syllabus",
    "section": "Digital Course Components",
    "text": "Digital Course Components\nOur primary form of communication outside of online Zoom meetings will be through Moodle. Students are expected to monitor Moodle for announcements, assignments, and other information during the semester. Assignments. Assignments and final projects may be given and submitted through Moodle."
  },
  {
    "objectID": "course_syllabus.html#accommodations-for-disabilities",
    "href": "course_syllabus.html#accommodations-for-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Disabilities",
    "text": "Accommodations for Disabilities\nReasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with the Disability Resource Office at Holmes Hall, Suite 304, Campus Box 7509, 919-515-7653. For more information on NC State’s policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation (REG02.20.01). Non-Discrimination Policy NC State provides equal opportunity and affirmative action efforts, and prohibits all forms of unlawful discrimination, harassment, and retaliation (“Prohibited Conduct”) that are based upon a person’s race, color, religion, sex (including pregnancy), national origin, age (40 or older), disability, gender identity, genetic information, sexual orientation, or veteran status (individually and collectively, “Protected Status”). Additional information as to each Protected Status is included in NCSU REG 04.25.02 (Discrimination, Harassment and Retaliation Complaint Procedure). NC State’s policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or https://oied.ncsu.edu/divweb/. Any person who feels that he or she has been the subject of pro hibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148."
  },
  {
    "objectID": "course_syllabus.html#technology-requirements",
    "href": "course_syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThis course may require particular technologies to complete coursework. Specifically, students will need R and R Studio. To avoid challenges that may arise due to different versions of packages, the instructor is providing access to code at the this link. Students will need to create accounts with their email address in order to gain access.\nIf you need access to additional technological support, please contact the Libraries’ Technology Lending Service: https://www.lib.ncsu.edu/devices."
  },
  {
    "objectID": "course_syllabus.html#resources-1",
    "href": "course_syllabus.html#resources-1",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThese are difficult times, and academic and personal stress is a natural result. Everyone is encouraged to take care of themselves and their peers. If you need additional support, there are many resources on campus to help you: \n• Counseling Center https://counseling.dasa.ncsu.edu/ \n• Health Center https://healthypack.dasa.ncsu.edu/ \n• If you or someone you know are experiencing food, housing or financial insecurity, please see the Pack Essentials Program https://dasa.ncsu.edu/pack-essentials/ \nSupporting Fellow Students in Distress. As members of the NC State Wolfpack community, we each share a personal responsibility to express concern for one another and to ensure that this classroom and the campus as a whole remains a healthy and safe environment for learning. Occasionally, you may come across a fellow classmate whose personal behavior concerns or worries you, either for the classmate’s well-being or yours. When this is the case, I would encourage you to report this behavior to the NC State’s Students of Concern website: https://prevention.dasa.ncsu.edu/nc-state-cares. Although you can report anonymously, it is preferred that you share your contact information so they can follow-up with you personally. I may also make a referral to the CARES program if I notice you are having a difficult time. If you don’t know who to talk to or which steps to take, please don’t hesitate to come talk to me and I will help you the best I can. \nStudents are responsible for reviewing the NC State University Policies, Rules, and Regulations (PRRs) which pertain to their course rights and responsibilities, including those referenced both below and above in this syllabus:\nEqual Opportunity and Non-Discrimination Policy Statement: https://policies.ncsu.edu/policy/pol-04-25-05 with additional references at https://oied.ncsu.edu/divweb/policies/\nCode of Student Conduct: https://policies.ncsu.edu/policy/pol-11-35-01."
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "welcome.html#predictive-analytics-for-improving-services",
    "href": "welcome.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "index.html#predictive-analytics-for-improving-services",
    "href": "index.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "course_syllabus.html#syllabus",
    "href": "course_syllabus.html#syllabus",
    "title": "1 Course Description",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "scoping_what.html",
    "href": "scoping_what.html",
    "title": "What are we trying to predict?",
    "section": "",
    "text": "It is important to be very thoughtful and precise in defining and developing a measure of the outcome to be predicted. In our motivating example, we want to predict employment, but how exactly should we define and measure employment? When specifying an outcome, the following considerations are essential:\nIn our example, we will focus on predicting just one outcome measure, but multiple measures of employment that make sense for the goals of the department could be analyzed. For each outcome, the analytic procedures would need to be repeated.\nOur outcome: The selected precise outcome measure for our example is: whether a client holds a job covered by unemployment insurance (UI) for the first three quarters after exiting the department’s education and training program for TANF participants. The measure is coded as 1 when achieved and 0 otherwise.\nThis choice was made because of the following answers to the above questions:\nHowever, after defining this measure, the project decided to code this outcome in the negative – so that we were estimating probabilities of whether someone does not achieve the employment outcome measure defined above. Therefore, our predicted probabilities represent risk scores. (I initially framed it as predicting employment because it is much easier to describe that way.)\nAlso, when discussing this project with stakeholders, we want to emphasize sensitivity to clients’ experiences. While we focus on “success” definitions that inform our measures (subsequently turned into “risk” scores), we avoid implying “failure” when clients fall short of departmental employment goals. We acknowledge the multifaceted challenges clients encounter and the program’s mission to assist those facing elevated barriers."
  },
  {
    "objectID": "scoping_when.html",
    "href": "scoping_when.html",
    "title": "When are we making predictions?",
    "section": "",
    "text": "Deciding when to make predictions is not always straightforward, but is critical to developing a valid prediction model. \nThe key is to balance two important considerations:\n\nFirst, we want to run predictive models early enough for the findings to be actionable. In our case, we want our caseworkers to be able to help clients early enough that they can address challenges.\nBut running predictive models later may mean we have more data measures available to include in our models and therefore potentially have more accurate models.\n\n\n\nIn our TANF program example, through conversations with the program managers and caseworkers on our project team, we learned (a) that interventions should be introduced very early, so they wanted risk scores available soon after client intake and approval for the education and training program. But we also learned (b) that there can be substantial lag in the entry of most of data collected during the intake and approval processes for TANF clients.\nWe therefore decided to specify our prediction timepoint as one month after the approval process for the education and training program.\n\nThis allowed time for most measures collected up through approval to be entered into the system.\nThis means that our models can only include measures that would feasibly be entered before our timepoint.\nIf the model is deployed, we would need to be careful to wait to apply the model to new data only at the correct timepoint – not earlier. Otherwise, we would have a lot of missing data, which could lead to poor predictions. We will discuss this topic more later.\n\n\nIt is a common mistake to not give adequate thought to when predictions are being made and to which measures are available at that time. If we ignore this issue, we might generate our risk scores (a) too early, which means our models might use measures that would not yet be available in practice, or (b) too late, which means our models might use measures that are collected very close to the outcome.\nFirst, we discuss (a), generating risk scores too early in the process. If we are not careful about understanding not just the data, but the data collection process, then we might accidentally use data that would not realistically be available at our chosen time point. If we had not spoken in depth with the program managers, we would not have known that client intake data was not immediately available, but instead could take a few weeks to be input into the system. As discussed above, if we make this mistake, our models would look effective in our test phase, but would have poor performance in practice due to missing data.\nSecond, we discuss (b), generating risk scores too late in the process. This strategy could lead to excellent performance - great accuracy - but useless predictions. To illustrate this type of mistake, consider if our data includes a measure of whether a TANF client has submitted a job application at the end of a job preparation class. Including this measure in our model could improve its predictive performance. However, because this measure is collected towards the end of the education and training program, if we wait to generate risk scores after this measure is collected (setting our timepoint at the end of the education and training program), there is little time left to take action to help those at highest risk of not finding and sustaining employment.\nThink about this issue when you read or hear about the high accuracy of predictive models. If a company boasts that it predicts an outcome with very high accuracy, it is important to ask:\n\nWhen are predictions being made?\nIs it possible that measures are collected so close to the outcome that some predictors are actually proxies for the outcome?\nWhat trade-offs are being made in terms of predictive accuracy versus having time to act on predictive analytics results?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_stakeholders.html",
    "href": "scoping_stakeholders.html",
    "title": "Who are the stakeholders?",
    "section": "",
    "text": "forthcoming\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_what.html#footnotes",
    "href": "scoping_what.html#footnotes",
    "title": "What are we trying to predict?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs summarized by the Employment and Training Administration in the U.S. Department of Labor: These data are from the Unemployment Insurance Data Base (UIDB) as well as UI-related data from outside sources (e.g., Bureau of Labor Statistics data on employment and unemployment and U.S. Department of Treasury data on state UI trust fund activities). See this link for more details.↩︎"
  },
  {
    "objectID": "poc_compare.html",
    "href": "poc_compare.html",
    "title": "Adding and comparing learners",
    "section": "",
    "text": "Once we have specified our benchmark learner, we can specify additional learners for comparison. If simplicity and transparency are goals, we will want to incrementally introduce complexity. For example, for our high school graduation example, we might take the following approach:\nThis results in a total of nine learners because we have nine combinations of predictor sets and modelling approaches. Ultimately, we will then have nine predictive models that result from the nine learners.\nNote: Machine learning algorithms typically require specifications for how they are implemented. These are called “tuning parameters.” For example, the random forest algorithm has tuning parameters that include the number of decision trees in a forest, the maximum depth of the decision trees, and several more options. Different values of tuning parameters have different implications for the performance of the algorithm. While the analyst can specify these tuning parameters, it is typically best to use a data-driven approach. We will dive more into this later. However, the point to take away here is that we will “tune” each machine learning algorithm for each predictor set. So when we ultimately compare learners, we are comparing “tuned” versions of them."
  },
  {
    "objectID": "poc_learners.html",
    "href": "poc_learners.html",
    "title": "Defining learners",
    "section": "",
    "text": "Here we present a framework for conducting a PA proof-of-concept. The framework focuses on specifying and comparing “learners.”\nA learner is a combination of:\n\nA predictor set: a set of measures that are available at the prediction timepoint and that are potentially predictive of the outcome of interest.\nA modeling approach: a method for combining and weighting measures, e.g., regression or a particular machine learning algorithm.\n\n\n\n\n\n\nMachine learning algorithms automate model building using a series of steps that are driven by patterns in the data - rather than relying on functional forms specified by an analyst. Examples include decision trees, random forests, stepwise regression, and support vector machines, among many others.\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_importance.html",
    "href": "poc_importance.html",
    "title": "The importance of a proof-of-concept",
    "section": "",
    "text": "After we have scoped our predictive analytics project, the next step is to conduct a “proof-of-concept.” That is, before getting too invested in incorporating predictive analytics into systems of service improvement, we need to investigate the following:\n\nGiven the available data, how well can we predict our outcome of interest?\nWhen investigating this question, we will want to consider various metrics of model performance, which we will turn to in a later section. We will be concerned with how well our predictive models help us identify those who truly are at risk (or truly will realize the outcome of interest - this might not always be “risk” or conversely, “success” but I use this as a easier way to talk about it). We may also be concerned with the extent to which our model makes mistakes - either designating at-risk individuals as being not-at-risk, or not-at-risk individuals as being at-risk.\nGiven the available data, to what extent is there bias in our predictions?\nWhen investigating this question, we will want to consider various metrics of bias, which we will also turn to in a later section. Here we will be concerned with whether and by how much model performance varies for different groups.\nTo what extent are complex predictive models worthwhile? Do they improve upon a simpler model or decision-making process? Do they improve upon current practice?\nWhen investigating this question, our focus is on contrasting predictive capabilities and potential biases across various prediction approaches. We start by looking at the simplest approach, which doesn’t necessarily involve a statistical model at all. A simple decision-making process might involve taking a small number of measures and combining and weighting them in a straightforward manner based on prior knowledge. One version of this type of approach is choosing specific criteria based on these measures that put people into one predicted catgory or the other. For example, perhaps everyone who both has children and is below a certain income level are predicted to be high-risk, while all other people are considered to be low-risk. This type of approach is sometimes called a rules-based approach or a decision tree, among other terminology.\nMoving to more complex approaches, we could plug a small number of measures into a regression model to estimate their relationship with the outcome. Finally, the most complex approaches might involve adding more measures, potentially a large number, and/or using advanced, data-driven algorithms such as a support vector machine or random forest.\nThe crux then lies in weighing the pros and cons when comparing different models. Should a more complex approach yield superior results compared to a simpler one, it prompts a consideration: Are the improvements significant enough to offset potential drawbacks in terms of transparency and explainability? Additionally, do these improvements outweigh any potential challenges in implementing and sustaining a more intricate modeling process?\nDo stakeholders understand and trust the results from the “best” predictive model?\nWhile stakeholders should be engaged in the PA scoping process, the proof-of-concept provides another opportunity for valuable consultation. Sharing results used to investigate the above questions supports transparency and trust in the process. Stakeholders’ input and questions are essential to the goals of the proof-of-concept - to assess whether and how predictive analytics should be deployed, communicated and used to improve services.\n\n\n\nFor further consideration: Are there other questions you would want to investigate in a proof-of-concept?\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_benchmark.html",
    "href": "poc_benchmark.html",
    "title": "Specifying a benchmark learner",
    "section": "",
    "text": "If simplicity and transparency are goals for our predictive model (these goals may not apply in all contexts), then we want to specify a simple and transparent learner as a “benchmark” to which all other, increasingly complex learners are compared.\nA benchmark learner consists of a benchmark prediction set (with a small number of measures) - and a benchmark modeling approach (one that is simple to explain, such as a decision tree or regression model).\nThe benchmark learner may align with how a service provider is already making decisions. For example, the benchmark learner may include the same measures the service currently reviews in making a decision, even if they do not create a model. Or, the benchmark may focus on measures that the service provider thinks are most important based on their expertise or based on related research.\nRemember, any measures included in the benchmark prediction set must be available at the prediction timepoint and for the prediction population.\nLet’s consider an example: Imagine we are exploring how we might use predictive analytics to help a school system improve their “early warning system,” and we have scoped the project as follows:\n\nWhat are we aiming to predict? Whether a student will not graduate from high school on-time (within 4 years of enrolling in 9th grade).\nWhen are we aiming to make predictions? After students have completed 9th grade. The analytics would be done during summer, after all 9th grade information has been entered in data systems. Prediction results would be available to educators before the start of students’ 10th grade.\nFor whom are we aiming to make predictions? All students who enrolled in 9th grade. However, we would have to exclude students who transfer outside the district during high school since we cannot know their graduation status. (Note this can create a missing data problem that we will discuss later.)\n\nHow might we define a benchmark learner for this example?\n\nThe school district has been focusing on so-called ABC indicators of attendance, behavior, and course performance to identify which students are most at-risk of not graduating on time. Therefore, a good benchmark predictor set could include the same three measures of attendance, behavior and course performance the district has used. If the district used these three measures as “indicators” (e.g. deemed a student at-risk if they passed a threshold on 2 of the 3 measures), then we could replicate the same empirical rules as a benchmark modeling approach. Alternatively, we could combine the three measures with a logistic regression model.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_addcomplexity.html",
    "href": "poc_addcomplexity.html",
    "title": "Gradually adding complexity",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_split.html",
    "href": "data_split.html",
    "title": "Data workflow",
    "section": "",
    "text": "Here we will define the learner workflow for a PA proof-of-concept. This involves the following three steps:\n\n\n\n\nflowchart LR\n  A(Training) --&gt; B(Validating)\n  B --&gt; C(Testing)\n  \n  \n\n\n\n\n\n\n\n\nTraining: Uses data with known outcomes to apply a modeling approach (e.g., regression or a machine learning algorithm) in order to build (i.e., estimate, fit, train) a model of the relationship between predictors and the outcome of interest.\nValidating: uses new data with known outcomes (data not used for training) to apply all trained learners/models. The predictions from the trained learners/models are compared to the known outcomes so that metrics of model performance and fairness are computed. This validation can be repeated in multiple validation data sets. Looking across all results, the “best” model is selected. Criteria for a model being selected as “best” vary by context. To determine the “best” learner/model, the team will weigh different definitions and metrics of predictive performance and of fairness. We will take a deep dive into these metrics soon. The team may also weight the simplicity and transparency of the learner, as well as variability across multiple validations.\nTesting: uses new, set aside data with know outcomes (data not used for training or validation) to apply the “best” model. The predictions from the best model are compared to the known outcomes to report on predictive performance and fairness to stakeholders and to make a decision about whether the learner/model should be deployed for use.\n\nIn the following pages, we will go over how to implement these concepts.\n\n\n\n Back to top"
  },
  {
    "objectID": "data_splitconsiderations.html",
    "href": "data_splitconsiderations.html",
    "title": "Additional considerations",
    "section": "",
    "text": "How much data do you need?\nUltimately, the goal is to have a large enough training set to train the models effectively, and large enough validation and test sets to get reliable estimates of the models’ performance. The definition of “large enough” can vary based on:\n\nModel Complexity: More complex models (e.g., deep neural networks with many layers and parameters) typically require larger training datasets to avoid overfitting while simpler models (e.g., linear regression, decision trees) may require smaller training datasets.\nData Variability: If the data has a lot of variability or noise, a larger dataset may be needed to capture the underlying patterns and relationships.\nBalance between the classes of a binary outcome: This is a specific case of #2. If one class is much more common than the others, you may need a larger dataset to ensure that the model is exposed to enough examples of each class during training.\nAvailability of Data: Sometimes the amount of available data is a limiting factor. In such cases, it is important to make the best use of the available data and adjust the methods accordingly.\n\nUltimately, if the learner/model performs well in validation and generalizes well to the test data, the dataset is likely large enough. However, if the model overfits to the training data, one reason may be that the modeling approach was to0 complex for the size of the available data, but there may be other explanations as well - e.g. lack of data consistency over time as discussed a few pages back.\n\n\nWhat ratios should be designated for training, validation and testing?\nThere is no one-size-fits-all answer to this question. A common starting point is a 60% training, 20% validation, and 20% testing split. However, the size of your dataset may necessitate a different allocation. For instance, a very large dataset might allow for a 50% training, 25% validation, and 25% testing split. Conversely, a smaller dataset might require a larger proportion for training, such as a 70% training, 15% validation, and 15% testing split. As previously mentioned, we will be combining the training and validation data to conduct \\(v\\)-fold cross-validation.\n\n\nHow much do all these decisions matter?\n\nNavigating the maze of decisions required for training, validating, and testing can be both confusing and frustrating, especially when guidance is not clear. Here are some key takeaways:\n\nAlways set aside a test set: The most crucial practice is to reserve a test set until a final learner has been selected. This allows for a fair evaluation of your final predictive model’s performance on new, unseen data. Failing to adhere to this practice is very problematic.\n\nRelatedly, remember that metrics of model performance, which we will discuss shortly, are estimates and come with inherent uncertainty. In other words, if you measured model performance using different samples, your results would vary. This uncertainty is even more pronounced with a smaller testing sample size. Although it is often overlooked, a good practice is to estimate this uncertainty, for example, by calculating the standard error or confidence interval of the performance metric estimate.\n\nFocus on the ultimate goal: Throughout the entire process, keep the ultimate goal in mind: to find a model that generalizes well to new, unseen (and often, future) data so that we can provide insights for improving services. With this ultimate goal in mind, ensure that your test set is representative of the data you will use in deployment. Otherwise, your results may be overly optimistic, even if you adhered to the first point. Additionally, ensure that your training and validation data are as similar as possible to the data you will ultimately use for deployment.\nHow much does the set-up of the training and validation matter?: The specific details of how you set up tuning and validation are important for model selection but probably will not completely derail your project. The worst-case scenario is that you miss the opportunity to select the “best” model, but it is unlikely that you will choose the “wrong” model. Poor performance is more likely due to the limitations of your data and context rather than the choices made in setting up cross-validation. [KH: opinions here?]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_overfitting.html",
    "href": "data_overfitting.html",
    "title": "Generalizing to new data",
    "section": "",
    "text": "The objective of predictive analytics\nRecall that our objective, as shown in this simple illustration, is to use data with known outcomes to build a model that allows us to predict outcomes - or the likelihood of outcomes - in data with unknown outcomes. As a reminder, we are focusing on binary outcomes (those with just two values - yes or no), which is why we focus on predicting probabilities or likelihoods of outcomes.\n\n\n\nAvoiding “overfitting”\nHow do we make sure that our learner, or model, which is based on information in the data with known outcomes (data from the past), generalizes to new data? If our learner/model is too closely tailored to the data we use to develop it, it risks not only capturing meaningful statistical patterns but also incidental fluctuations or noise unique to that data. When a model inadvertently incorporates this noise, it excels at describing and fitting the original data. However, when confronted with new, unseen data, this same model tends to deliver inaccurate predictions. This is called “overfitting.” Overfitting refers to a model’s inability to generalize to new data - due to being constructed around specific noise rather than based on broader patterns.\nThe following simple plots display the concept of overfitting, as an overcorrection to “underfitting.” Each plot shows an attempt to capture the relationship between two measures. In the first plot, the estimated model, represented by the blue line, does not capture the trend evident in the data, shown by orange points. This is corrected in the second plot, in which the model appears to be a a good fit of the general trend. Then, in the the third plot, the model fits the data extremely well, but it overfits because it predicts almost every point - almost every random variation from the overall trend we care about.\n\nIn this first section on data for predictive analytics, we will learn how to develop, compare, and select learners/models so that we identify the one that does the “best” job in generalizing to new, unseen data. Note that “best” is in quotation marks because we have not yet discussed what how we define “best”. We will turn to that soon.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_validate.html",
    "href": "data_validate.html",
    "title": "Data for validating learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_train.html",
    "href": "data_train.html",
    "title": "Data for training learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_test.html",
    "href": "data_test.html",
    "title": "Designating data for testing",
    "section": "",
    "text": "Once we have identified the appropriate data for all steps (train, validate, test) of our PA proof-of-concept workflow, we want to work backwards. So first, we want to split off a subset that will be used for testing. Some notes about designating the testing data:\n\nThe test data will not be analyzed until we have completed learner training and validation and have selected our “best” model. We should set it aside and not touch it until we have made a final learner selection.\nIn the case that the goal of predictive analytics is to make predictions for *future* service recipients, then ideally, we want our test data set to include individuals (or other units) that are forward in time, compared to the data we will use for training and validation. This provides the best check of how well our “best” model generalizes not only to different data, but also to the future.\nIdeally, we would want data as close in time as possible to when we would deploy a predictive model. This might be tricky. For example, if our proof-of-concept is assessing possible deployment for predicting high school drop out risk of 10th graders in the 2024-2025 school year, then the best test data may be the most recent cohort of 10th graders for whom we know graduation status. This would be students who graduated in 2023 and were therefore 10th graders in the 2020-2021 academic year.\nIn some cases, we might be making predictions for a new location (e.g. for a new site, office, store). Then, we might designate testing data that is from a different location than data we will use for training and validation.\n\nUltimately, for an honest assessment of our selected, “best” model, we want to conduct testing in a test data set that best mimics how the predictive model would be deployed.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss.html",
    "href": "paforimprovingss.html",
    "title": "What is predictive analytics for improving services?",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_identify.html",
    "href": "data_identify.html",
    "title": "Identifying data",
    "section": "",
    "text": "We want to be thoughtful about the data that we will use for our proof-of-concept and that we will split for training, validating and testing. Here are some important considerations for identifying data:\n\nThe data should be representative of the population and context for the predictive analytics objective. Here we want to make sure we have data for units that are experiencing a similar context as what we expect for the units for whom we will ultimately deploy predictive analytics.\n\nRecall our earlier example that centered on predicting the risk of TANF (Temporary Assistance for Needy Families) training program participants not finding and sustaining employment. When determining which years of past data to incorporate into our learner workflow, it is essential to examine if and when the eligibility criteria for the training program underwent significant alterations. It may be prudent to restrict our data to a timeframe in which the eligibility criteria for the training program closely resemble those currently applicable to the TANF training program.\nIn our other example, in which the focus is on predicting the risk of 10th grade students not graduating on time, it is important to investigate the graduation guidelines. If the student cohorts used to train our models had more stringent or more lenient graduation standards compared to the cohorts to which we apply the models, the models might not be widely applicable in years beyond our training-validation-testing workflow.\n\nImportant measures should be consistently available. Here we want to assess whether we have sufficient consistency in the way measures are defined and entered into data systems. For example:\n\nHave the data integrations been maintained consistently over time? Specifically, if the data we intend to use is a combination of multiple underlying data sources, have these merges been conducted consistently? If not, crucial measures may occasionally be absent. Additionally, are these integrations/merges sustainable in the future? If certain data cannot be reliably obtained and integrated on an ongoing basis, they should be omitted from the predictive analytics proof-of-concept. It is imperative to consider not only historical data but also prospective scenarios. If particular data will be unavailable when implementing predictive analytics, then a proof-of-concept relying on that data will not be authentic.\nWas there a rollout of a new data system that had early implementation challenges? If so, we may want to eliminate data from those early years in our training-validation-testing workflow.\nIs survey data part of our data universe for predictive analytics? If so, do response rates vary substantially over time? Variation in response rates could (but not necessarily) mean that different populations are being captured with each survey. We would need to investigate this.\nWere there broad changes in how measures where defined or coded? We may want to limit the data we will use to train, validate and test models to a period in which the measures are consistent to the present (time time close to when any model would be deployed).\nNote that inconsistencies in some measures could have implications in measures we include in our prediction sets rather than implications for how we restrict our data for our proof-of-concept. That is, if some measures change over time, we may need to exclude just those measures from our models. If there is sufficient consistency across key predictors in data we may not need to restrict the data. We will return to measure consistency later, when we discuss data quality issues.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_trainvalidate.html",
    "href": "data_trainvalidate.html",
    "title": "Training and validating learners",
    "section": "",
    "text": "Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Tool Set templates you will use, but I will comment on some alternative variations.\nRecall that it is imperative to train (i.e. fit a model or build a machine learning algorithm) in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.\nWe can set aside a portion of the non-test data and designate that portion for validation. We can then use the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness. This is common and valid practice.\nWe can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to \\(v\\)-fold cross-validation, in which \\(v\\) is the number of folders or partitions - or times we repeating the training and validation. The following diagram illustrates 5-fold cross-validation:\n\n\n\n\n\nThis diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Tool Set does a combination of both. For a few of metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code. (And don’t worry - the PA Tool Set implements \\(v\\)-fold cross-validation for the user. You will not need to code the procedure; you will just specify how to the cross-validation should be implemented, following the guidance below.)\n\nGuidance for implementing \\(v\\)-fold cross-validation:\n\nNumber of folds: A recommended default for $v$ is 5. Higher values of $v$ (more folds) may be preferable if there is a large amount of data because the average performance across the folds will be more representative of the true performance (less biased), albeit with more variability in the estimates (larger variance). Additionally, more folds result in increased computation time, which can be significant. Alternatively (although also more computationally demanding), the entire $v$-fold cross-validation procedure can be repeated multiple times. In other words, we can obtain results from 5 folds, then resample to create 5 different folds and consequently acquire another set of results from another 5 folds of the same size. This approach may be advantageous if the sample size is small, as there could be substantial variability in the metrics across folds. (However, our current tool set code templates do not support this functionality.)\nHow to specify folds: Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of \\(v\\)-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.\nStratification: In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is “yes” or “1” for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few “yes” or “1” outcomes. To protect against this, we can stratify the cross-validation. This means we can (1st) randomly partition those observations for which the outcome = 1; and then (2nd) separately randomly partition those observations for which the outcome = 0; then (3rd) pair each partition with outcome =1 with a partition with outcome = 0 to get \\(v\\) folds with equal proportions of each value of the outcome. Note that the PA Tool Set templates will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.\n\n\n\nTuning with \\(v\\)-fold cross-validation:\n\nThe PA Tool Set will automatically optimize tuning parameters. It does this with \\(v\\)-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.\nThe comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the \\(v\\) folds, will be selected as the “tuned learner.” The user will then only view results of the tuned learner going forward.\n\nNote that using the same cross-validation procedure for both parameter tuning and learner validation and comparison is not technically, completely correct. Using the same data twice for both goals is sometimes called \"double-dipping.\" That is, using the same data again to validate the performance of the tuned learner, we may get overly optimistic performance estimates (overfitting) because the models have already “seen” that data during the tuning process. To correct for this, there are two approaches: (1) As described above, we can set aside a subset of data for validation. In this case, we first conduct parameter tuning using \\(v\\)-fold cross-validation with the training data, but then compare the tuned learners’ performance in the held-out validation data. Alternative, (2) we can use a nested \\(v\\)-fold cross-validation. In nested cross-validation, we have an outer loop of cross-validation for model validation, and an inner loop of cross-validation for parameter tuning. This allows you to use the same data for both tuning and validation, but in a way that prevents biased performance estimates. This approach may be preferable when there are not sufficient data to hold out a separate validation data set. Sorry, our tool set templates do not have these options at this time.\nBut…?\nKristen H: I’m confused here. Questions: (1) I guess I don’t totally see how we are double dipping. If we searched across v-fold cross-validation all combinations of possible tuning parameter values + algorithm + predset (and if our selection was based only on a metric such as AUC ROC) then wouldn’t we arrive at the same selection for testing as we would if we first found the optimally tuned model for each predset? We are not refitting the models/learners with the same data, we are using the CV results from the best tuned model/learner within each ML - so is this double-dipping? I’m sure I ’m missing something here. (2) Assuming we are double-dipping, is the worry that the resulting overfitting is different for different learners? That is, does it mean we could select the “wrong” learner when comparing validation performance (comparing metrics based on the v-fold CV)? Or does it just mean that the performance is inflated for all of them, so we expect even more decline in performance in the testing data (we expect worse performance anyway, especially if test data is from a different time period).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "toolset_intro.html",
    "href": "toolset_intro.html",
    "title": "Introduction to the PA Toolset",
    "section": "",
    "text": "Here introduce high level overview of PA toolset\nInstructions for getting set up on server\nOverview of files - snapshot of directory\nShow how to enter in information about prediction objective and learners?\n\n\n\n Back to top"
  },
  {
    "objectID": "course_syllabus.Rmd.html",
    "href": "course_syllabus.Rmd.html",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-details",
    "href": "course_syllabus.Rmd.html#course-details",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-description",
    "href": "course_syllabus.Rmd.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nDescription:\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. Students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\n\n\nPrerequisites:\nStudents should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\nLearning objectives:\n\nGain the ability to critically assess the potential value and limitations of predictive analytics for improving services when applied to a particular context and data system.\nUnderstand how results from predictive analytics can provide actionable insights for improving services.\nUnderstand how to plan and implement all steps of a predictive analytics workflow using open-source code templates. The steps include planning the analysis to match goals, exploring and preparing data, specifying and training models, and assessing the validity and bias of predictive models – all while prioritizing best-practices for ethics, transparency and replication.\nGain a high-level understanding of sta(s(cal concepts for optmizing predictive analytics and have op(onal choices to dive more deeply.\nPresent predictive analytics results using an R notebook and clearly communicate interpretations, cautions and recommendations.\n\n\n\nAbout the Data Science Academy:\nWelcome to an NCSU Data Science Academy Course! In July 2021, the universitywide and interdisciplinary Data Science Academy was launched to meet the growing needs of data science research, education and expertise in North Carolina and beyond. At NC State, Data Science is for Everyone. Data Science Academy (DSA) Courses are designed to make sure that each student can pursue appropriate level challenges through opportunities to make choices and pursue projects of interest. Whether you have never thought about data science before or bring experience and expertise, we welcome you. Our goal is that after each DSA class you want to learn more! For more on the DSA course design. \n\n\nAbout DSA research:\nTo make sure that we are providing an appropriate collection of courses with a variety of challenge levels within each course, we will be collecting data to help us build a practice of continuous improvement. The purpose of the data is to evaluate the DSA and how well we are serving our students. We hope to be able to share what we learn with other universities and researchers - we will ask your permission at the beginning of the course to be able to share your anonymized data when we communicate about the work of the DSA."
  },
  {
    "objectID": "course_syllabus.Rmd.html#resources",
    "href": "course_syllabus.Rmd.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThere is no textbook for this class. All reading materials are provided within this website and can be accessed in the “Course Materials” tab at the top of the page. The course materials includes text developed by the instructor as well as links to helpful reading, which may be required or optional as noted."
  },
  {
    "objectID": "course_syllabus.Rmd.html#grading",
    "href": "course_syllabus.Rmd.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nFORTHCOMING"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-schedule",
    "href": "course_syllabus.Rmd.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule is linked here."
  },
  {
    "objectID": "course_syllabus.Rmd.html#attendance-policy",
    "href": "course_syllabus.Rmd.html#attendance-policy",
    "title": "Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nFor complete attendance and excused absence policies, please see http://policies.ncsu.edu/regulation/reg-02-20-03. For coronavirus related attendance guidance, see the “coronavirus information” section.\nAttendance Policy. Students are expected to attend class regularly. Since we only meet once a week, each class is vitally important to keep on track during the semester.\nAbsences Policy. Students should notify the instructor that they will be unable to attend class and schedule a time to meet with the instructor to review materials they missed during class time.\nMakeup Work Policy. Makeup work will be allowed if the student notifies the instructor at least one day before an assignment is due to arrange for a deadline extension. Notification given on the due date will be considered on a case-by-case basis."
  },
  {
    "objectID": "course_syllabus.Rmd.html#academic-integrity",
    "href": "course_syllabus.Rmd.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nStudents are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01.\nStudents are expected to produce work that they personally completed and in accordance with the instructions given for the assignment, project, test or quiz. Work copied from other students, completed by another individual, or directly taken from other sources without appropriate attribution will be in violation of academic integrity. Students are expected to follow the Code of Student Conduct (NCSU POL11.35.01) and Pack Pledge, and violations of academic integrity will be handled in accordance with the Student Discipline Procedures (NCSU REG 11.35.02).\nHonor Pledge. Your submission on any test or assignment indicates “I have neither given nor received unauthorized aid on this test or assignment.” Digital Course Components Students may be required to disclose personally identifiable information to other students in the course, via digital tools, such as email or web-postings, where relevant to the course. Examples include online discussions of class topics, and posting of student coursework. All students are expected to respect the privacy of each other by not sharing or using such information outside the course."
  },
  {
    "objectID": "course_syllabus.Rmd.html#digital-course-components",
    "href": "course_syllabus.Rmd.html#digital-course-components",
    "title": "Syllabus",
    "section": "Digital Course Components",
    "text": "Digital Course Components\nOur primary form of communication outside of online Zoom meetings will be through Moodle. Students are expected to monitor Moodle for announcements, assignments, and other information during the semester. Assignments. Assignments and final projects may be given and submitted through Moodle."
  },
  {
    "objectID": "course_syllabus.Rmd.html#accommodations-for-disabilities",
    "href": "course_syllabus.Rmd.html#accommodations-for-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Disabilities",
    "text": "Accommodations for Disabilities\nReasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with the Disability Resource Office at Holmes Hall, Suite 304, Campus Box 7509, 919-515-7653. For more information on NC State’s policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation (REG02.20.01). Non-Discrimination Policy NC State provides equal opportunity and affirmative action efforts, and prohibits all forms of unlawful discrimination, harassment, and retaliation (“Prohibited Conduct”) that are based upon a person’s race, color, religion, sex (including pregnancy), national origin, age (40 or older), disability, gender identity, genetic information, sexual orientation, or veteran status (individually and collectively, “Protected Status”). Additional information as to each Protected Status is included in NCSU REG 04.25.02 (Discrimination, Harassment and Retaliation Complaint Procedure). NC State’s policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or https://oied.ncsu.edu/divweb/. Any person who feels that he or she has been the subject of pro hibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148."
  },
  {
    "objectID": "course_syllabus.Rmd.html#technology-requirements",
    "href": "course_syllabus.Rmd.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThis course may require particular technologies to complete coursework. Specifically, students will need R and R Studio. To avoid challenges that may arise due to different versions of packages, the instructor is providing access to code at the this link. Students will need to create accounts with their email address in order to gain access.\nIf you need access to additional technological support, please contact the Libraries’ Technology Lending Service: https://www.lib.ncsu.edu/devices."
  },
  {
    "objectID": "course_syllabus.Rmd.html#resources-1",
    "href": "course_syllabus.Rmd.html#resources-1",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThese are difficult times, and academic and personal stress is a natural result. Everyone is encouraged to take care of themselves and their peers. If you need additional support, there are many resources on campus to help you: \n• Counseling Center https://counseling.dasa.ncsu.edu/ \n• Health Center https://healthypack.dasa.ncsu.edu/ \n• If you or someone you know are experiencing food, housing or financial insecurity, please see the Pack Essentials Program https://dasa.ncsu.edu/pack-essentials/ \nSupporting Fellow Students in Distress. As members of the NC State Wolfpack community, we each share a personal responsibility to express concern for one another and to ensure that this classroom and the campus as a whole remains a healthy and safe environment for learning. Occasionally, you may come across a fellow classmate whose personal behavior concerns or worries you, either for the classmate’s well-being or yours. When this is the case, I would encourage you to report this behavior to the NC State’s Students of Concern website: https://prevention.dasa.ncsu.edu/nc-state-cares. Although you can report anonymously, it is preferred that you share your contact information so they can follow-up with you personally. I may also make a referral to the CARES program if I notice you are having a difficult time. If you don’t know who to talk to or which steps to take, please don’t hesitate to come talk to me and I will help you the best I can. \nStudents are responsible for reviewing the NC State University Policies, Rules, and Regulations (PRRs) which pertain to their course rights and responsibilities, including those referenced both below and above in this syllabus:\nEqual Opportunity and Non-Discrimination Policy Statement: https://policies.ncsu.edu/policy/pol-04-25-05 with additional references at https://oied.ncsu.edu/divweb/policies/\nCode of Student Conduct: https://policies.ncsu.edu/policy/pol-11-35-01."
  },
  {
    "objectID": "poc_compare.html#footnotes",
    "href": "poc_compare.html#footnotes",
    "title": "Adding and comparing learners",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDemographic information like race is excluded for ethics reasons. This decision will be discussed more in-depth when we discuss algorithmic bias.↩︎"
  }
]
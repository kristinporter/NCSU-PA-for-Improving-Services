[
  {
    "objectID": "performance_metrics_PLACEHOLDER.html",
    "href": "performance_metrics_PLACEHOLDER.html",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "The text below was pulled from 01_03 notebook.\n\n\nFirst, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP.\n\n\n\n\nAUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.\n\n\n\nAUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "href": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "First, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "href": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "href": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "title": "1 Understanding performance metrics",
    "section": "",
    "text": "AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#performance",
    "href": "performance_metrics_PLACEHOLDER.html#performance",
    "title": "1 Understanding performance metrics",
    "section": "Performance",
    "text": "Performance\nWe now compare the learners according to both ROC AUC and PR AUC.According to the mean values of these metrics, the best learners are:\n\nAUC_ROC: r best_roc_auc\nAUC_PR: r best_pr_auc\n\nIt is common for these metrics to show different learners as the best performer. We may want to consider the results holistically to select the best learners. For example, consider the following toy result:\n\n\n\nlearner\nAUC_ROC\nAUC_PR\nMSE\n\n\n\n\nlearner_1\n0.8\n0.7\n2.9\n\n\nlearner_2\n0.7\n0.6\n2.85\n\n\n\nIf we look at all the metrics, learner_2 is the best according to MSE. However, it is only marginally better on MSE, and performs substantially worse on both AUC metrics compared to learner_1. Thus, we would probably not consider learner_2 to be one of our best performing learners.\nAfter understanding broad learner performance, it is often helpful to select a subset of the learners to examine in more detail. For example, we may select only the best learners according to our performance metrics above. Alternatively, we may want to select learners that are more interpretable, even if they are not the best performers. For example, if a simple regression model performs almost as well as a more complicated algorithm, we may still want to select the regression model even if it is not the absolute best performer on our metrics."
  },
  {
    "objectID": "scoping_usefindings.html",
    "href": "scoping_usefindings.html",
    "title": "How will findings be used?",
    "section": "",
    "text": "The social services department had an established practice of offering additional support to clients requiring extra assistance. This encompassed:\n\nInitiating check-in calls with clients,\nFacilitating supplementary casework sessions, and\nArranging extra workshops to enhance job search and skills.\n\nGiven the department’s constraints in terms of staff and financial resources, their aim was to effectively direct extra services to those in greatest need. They recognized that their current method of prioritizing such clients could be enhanced by distilling a wide range of potentially predictive information into a single “risk score.”\nA few things to note:\n\nThey did not want, nor did the project team recommend, for their decision-making process to become completely dependent on the risk scores.\nInstead, the risk scores could potentially provide a valuable piece of information (if we could validate good predictive performance and minimal bias).\nThe department planned to train caseworkers in how to interpret and incorporate the risk score into their decision-making and prioritizing.\nThe department also wanted to look at how risk scores varied across and within populations served by offices throughout the state. This analysis could help them understand which offices were more in need of support for reaching out to clients and for planning workshops.\n\nThis information about how the department plans to use the information can guide how we scope the predictive analytics project.\nRemember, we also want to frequently check in with our ethical considerations. What ethical considerations do we want to be careful about as we proceed?\n\n\n\n Back to top"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "This is assignment 1",
    "section": "",
    "text": "This is assignment 1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_define.html",
    "href": "paforimprovingss_define.html",
    "title": "What is predictive analytics (PA)?",
    "section": "",
    "text": "Predictive analytics (PA) (or predictive modeling) is the use of historical data to forecast future outcomes.\n\n\n\n\n\n\n\nThe complexity of PA may vary. For example, PA may rely on relatively simple approaches, such as models with a small number of measures (i.e. predictors, variables). Alternatively, in settings with large amounts of data, they may involve complex machine-learning algorithms.\nPA may make predictions of binary outcomes, which take just two values such as yes or no. For example, we can predict whether students will graduate from high school on time or not. With binary outcomes, predictive analytics can also be used to estimate the probability of the outcome occurring. For example, we can predict the probability that a student will graduate from high school. This probability is between 0 and 1.\nPA may also forecast continuous outcomes. For example, we can predict the grade point average (GPA) students will achieve.\nTo narrow the scope of the class, we will focus only on binary outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "1 Introduction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_old.html",
    "href": "index_old.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nInsert overview of these training materials here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Course overview",
    "section": "Syllabus",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#classroom-expectations-and-norms",
    "href": "index.html#classroom-expectations-and-norms",
    "title": "Course overview",
    "section": "Classroom expectations and norms",
    "text": "Classroom expectations and norms"
  },
  {
    "objectID": "index.html#six-ways-to-get-help",
    "href": "index.html#six-ways-to-get-help",
    "title": "Course overview",
    "section": "Six ways to get help",
    "text": "Six ways to get help"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Course overview",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Overview",
    "section": "",
    "text": "These materials are designed to help you understand the main topics we’ll cover in class. It’s best to read them before each class, so we can use our time to review important points and discuss any questions. I’ll let you know what to read before each class.\n\nHow to use these materials\nThe class is organized into the sections listed in the left panel of this page. Clicking the arrow to the right of each main section title will show the sub-sections. Generally, we will cover one or two main sections per class. Some sections are currently empty, but will be added as the class progresses.\nThe materials consist of text, links (to articles, blog posts, videos, etc.), and snippets of the code we will use for analyses. Please note:\n\nYou should use the hyperlinks in these materials unless otherwise noted. Most are short.\nIf hyperlinks are optional, this will be clearly stated. They provide an opportunity to do a deeper dive into a topic if you are interested.\nIn addition, you will see citations throughout. There is no expectation for you to read the cited papers unless you want to.\n\n\n\nAcknowledgements\nThese course materials were put together by me, Kristin Porter. All errors and omissions are mine. But I have also had lots of help - many sections draw on and even copy valuable text written by Zarni Htet at MDRC and Kristen Hunter at the University of South Wales, Sydney. The framework presented in this class was developed at MDRC, a nonprofit social policy research organization where I was employed for 16 years. Kristen Hunter also provided very helpful review of all these materials.\nThis is the first time I have taught this course, and I would not be surprised if there are problems. I very much welcome any and all feedback and questions at any time.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_ex.html",
    "href": "codeofethics_ex.html",
    "title": "Examples and best practices",
    "section": "",
    "text": "Examples and best practices\n\n\n\n\n Back to top"
  },
  {
    "objectID": "course_description.html",
    "href": "course_description.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. In this class, students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\nPrerequisites: Students should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_why.html",
    "href": "codeofethics_why.html",
    "title": "Why create a code of ethics before starting PA?",
    "section": "",
    "text": "Why create a code of ethics before starting PA?\n\n\nThis is a test.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "1 Papers to maybe work into class…",
    "section": "",
    "text": "References\n\n\n\n\n\n1 Papers to maybe work into class…\nThe Dangers of Risk Prediction in the Criminal Justice System\nAbstract: Courts across the United States are using computer software to predict whether a person will commit a crime, the results of which are incorporated into bail and sentencing decisions. It is imperative that such tools be accurate and fair, but critics have charged that the software can be racially biased, favoring white defendants over Black defendants. We evaluate the claim that computer software is more accurate and fairer than people tasked with making similar decisions. We also evaluate, and explain, the presence of racial bias in these predictive algorithms.\nAn Algorithm That Grants Freedom, or Takes It Away\n2020 NYT article summarizing predictive analytic applications and bias in multiple parts of the CJ system, as well as detecting fraud in the welfare system.\nPredicting Participation in Healthy Marriage and Responsible Fatherhood Programs (and nice technical appendices) from Mathematica/OPRE\n\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "1 Assignments and project",
    "section": "",
    "text": "1 Assignments and project\n\nFor a selected scenario, code of ethics.\nFor a selected scenario, prediction target: what, when, for whom; how will results be used?\nIdentify data set. Summarize prediction target, predictors. Create meta data. Fill in first part of PA toolset?\nData preparation, create predictors. Data model.\nComplete first full page of PA toolset - specify learners.\nCompare learners in terms of performance.\nCompare learners in terms of bias.\nTest link to assignments\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html",
    "href": "paforimprovingss_ethics.html",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "scoping_ex.html",
    "href": "scoping_ex.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "A motivating example\nLet’s begin with an example. This example is inspired by a project I was involved in, but I have made several modifications for illustration purposes. The project was in collaboration with a state agency overseeing the federal assistance program, Temporary Assistance for Needy Families (TANF). TANF’s primary goal is to empower families towards self-sufficiency by providing essential resources and services. Within this framework, state agencies administering the program extend a range of supportive services. These services are designed to assist families in overcoming obstacles to employment and self-reliance. Their approach revolves around tailoring assistance to the specific circumstances and needs of each family, ensuring a targeted response to diverse challenges and varying levels of need among households.\n\n\n\n\n\n\nThe agency came to the data science team with the following questions:\n\nWith a data-driven tool, could caseworkers better understand which clients were more at risk of not reaching key program outcomes? In particular, among those who were required to find employment, could they obtain reliable information about who was least likely to find employment? Note that the current practice for identifying clients who were most at-risk relied on caseworkers reviewing measures they thought were important (e.g. of previous employment history) and making a subjective decision based on their professional expertise. They wondered if predictive analytics would provide an improvement in terms of accuracy and fairness.\nCould we develop reliable risk scores that combine and weigh multiple risk factors into a single easy-to-understand number that can be incorporated into decision-making?\nCould trends in risk scores across the population of TANF clients provide new insights for program improvement and resource allocation?\n\n\n\nHow do we scope a project that helps them achieve their goals?\nNext, we will use this example to talk about project scoping. That is, we need to specify the following:\n\nHow would information about clients’ risks of adverse outcomes be used?\nWhat, exactly, are we trying to predict?\nFor whom would we be making predictions?\nWhen would we be making the predictions?\nWhen specifying what, when and for whom we are making predictions - what data are available? What do the data allow?\n\nAnswering these questions allows us to move forward in planning a potential predictive analytics project. That is what I mean by scoping - what potential project could there be? But importantly, the next step is to investigate that potential - to see if we can reliably, clearly and fairly make useful and actionable predictions. I refer to this next step as the “proof-of-concept,” which we will turn to in the next main section.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_learn.html",
    "href": "paforimprovingss_learn.html",
    "title": "How can PA be used to improve services?",
    "section": "",
    "text": "Predictive analytics has a wide variety of applications, from predicting financial markets to personalized medicine. The potential of predictive analytics in the services sector is to help government programs, nonprofit service providers or businesses identify those clients who could most benefit from targeted interventions. This targeting can facilitate effective service delivery at an efficient cost.\nWhy just focus on predictive analytics for improving services? Predictive analytics has vast applications. Different applications raise different issues with respect to project scoping, ethics and data and technical considerations. We focus on improving services because it allows us to build a framework that is driven on how results will be used. Also, my background is in using predictive analytics (as well as other data science and statistical methods) to strengthen social services, so it is the application to which I have given the most thought. But at the same time, many of the lessons we will learn will be helpful for other applications as well, and there will be flexibility in the data you use for your projects.\n\nSome examples of using predictive analytics to improve social services:\nDuring my time at MDRC, the nonprofit research organization from which the framework for these materials originated, my colleagues and I explored and applied predictive analytics in various ways:\n\nPredicting K-12 academic outcomes: School districts often utilize “early warning systems” (EWS) that rely on student-level data. These systems help identify students who might be at risk of not achieving important educational milestones, such as timely graduation or passing state exams. For instance, the probability of not graduating high school is often gauged using the ABC indicators, which stand for attendance, behavior, and course performance. This indicator system represents a simpler form of predictive analytics. However, as schools increasingly collect richer, data sets with frequent data updates of many student measures, including daily attendance, exam scores, and course marks, there has been opportunity to compute more accurate, frequent and nuanced predictions of student risk.\nProviding early warnings of not reaching milestones in an employment training program. This project focused on harnessing granular, longitudinal administrative data to build a system for ongoing, advanced analytics that support the continuous improvement process at the Center for Employment Opportunities (CEO). The goal was for these early warnings to be transmitted, practically in real time, to front-line case workers and leaders. The information would be part of standard dashboards and data protocols. CEO planned to train staff members to act on this information, and to work with MDRC to design, implement, and test new interventions based on insights provided by the predictive analytics results. Unfortunately, this project was interrupted by the COVID-19 pandemic.\nIdentifying TANF participants who were most likely to find employment. TANF is the Temporary Assistance to Need Families program that provides time-limited support for families’ basic needs. The federal government provides grants to states to run the TANF program. As part of MDRC’s TANF Data Collaborative Project, a team at the Virginia Department of Social Services (VDSS) sought to develop analytic tools to help TANF case workers customize education/employment-related services to increase the likelihood of participants’ labor market success after they leave the program. The team investigated whether demographic characteristics, household compositions, receipt of other public benefits, and past education/employment-related activities could predict success, and how to construct an unbiased predictive tool using such variables.(MDRC, 2023)\nIdentifying families at risk of disengaging from a home visiting program. Child First is a home visiting program that aims to promote high-quality relationships between caregivers and children in families experiencing challenges related to caregiver mental health and child behavior. Staff members provide intensive in-home clinical services to both the caregiver and child. They also connect families to additional services such as financial and housing support, health care, and treatment for disorders such as substance abuse. To accomplish its goals, Child First must ensure that families remain consistently engaged in program services over time. When families leave before being officially discharged from the program, they receive truncated interventions that are likely to be less effective at improving outcomes. Early disengagement is also expensive, given the large, fixed costs of enrolling new families into the program. Child First prioritizes collecting high-quality data to understand the population of families it serves. For example, the program collects information on a range of family characteristics assessed at intake, including sociodemographic information on both the caregiver and child, health-related information like insurance status and child DSM-5 diagnoses, as well as several risk measures and assessments such as adverse childhood experiences (ACEs) that capture experiences with violence, abuse or neglect, and household instability. The availability of these data provided researchers with a singular opportunity to explore whether predictive analytics could be a useful tool to summarize the large amounts of information the program collects and use it to help staff members identify families at particularly high risk of early disengagement, defined as being enrolled in the program for fewer than 90 days. That information could allow Child First staff members to triage families better at intake and provide more intensive support and services to those families at risk of early disengagement.(Xia, Htet, Porter, & McCormick, 2023)\nPretrial justice: Many jurisdictions across the United States are rethinking the “front end” of the criminal justice system — the pretrial period between an arrest and the disposition of a criminal case. Often these reforms focus on the initial decisions that judges and other court stakeholders make about whether to detain individuals in jail while they are awaiting trial, and on the use of money bail as a tool for ensuring that people will show up to court hearings. In most jurisdictions, the majority of people in jail at any point in time are awaiting trial, and many are there because they cannot afford to post bail. Jurisdictions are looking for fairer, more cost-effective approaches to the pretrial phase of the system. To assist jurisdictions in making better initial decisions, the Laura and John Arnold Foundation (now Arnold Ventures) developed the Public Safety Assessment (PSA), a tool that uses data on an individual’s history with the justice system and the current offense to predict the probability that the person will show up to hearings or be arrested for a new crime if released. The PSA aims to help judges make more informed, less subjective decisions about pretrial detention. It is currently used in nearly 40 jurisdictions across the nation.(Redcross & Henderson, 2019) (Golub, Redcross, & Valentine, 2019)\n\nLater, we will return to some of the above examples to discuss various issues, including ethics, project scoping or set-up, the trade-offs of different modeling approaches, and more. Here are some other examples of predictive analytics being used in the social service sector with the goal to strengthen programs’ services:\nChild welfare: An example of PA being used in child welfare is the Allegheny Family Screener Tool. First implemented in 2016, it was developed in a partnership between researchers from Auckland University of Technology and the Allegheny County Office of Children, Youth, and Families, a Pennsylvania child welfare system. The research team wanted to use predictive analytics to help inform and improve decisions made by staff when determining whether reports of possible child abuse and neglect should be marked for further investigation, rather than replace human decision making altogether. The tool summarizes vast amounts of information across multiple databases to provide a risk score to child welfare call screeners. The researchers worked closely with the child welfare agency and partner organizations to discuss implementation of the tool and results, and feedback from community meetings informed how the tool was developed. An independent evaluation found that it increased the staff’s ability to accurately screen reports and pursue investigations. Also, the tool did not increase the rate of children screened in for investigation. That is, using it resulted in a different pool of children being identified as needing child welfare intervention, but did not substantially increase the proportion of children investigated among all children referred for maltreatment. The model and its implementation have been updated over time so that its predictions reflect contemporary information on families currently being served.(Human Services, 2019)\nLead poisoning prevention: The Chicago Department of Public Health partnered with the Data Science for Social Good initiative at the University of Chicago to help find the homes that are most likely to still contain lead-based paint hazards. From their website: “By building statistical models that predict exposure based on evidence such as the age of a house, the history of children’s exposure at that address, and economic conditions of the neighborhood, CDPH and their partners can link high-risk children and pregnant women to inspection and lead-based paint mitigation funding before any harm is done.” This integrated and innovative system will ensure resources are used most efficiently, and ultimately will mean healthier Chicago children. This short video summarizes the project.\nCriminal justice: In addition to applications in the pretrial periods, predictive analytics is being applied to other parts of the criminal justice system. For example, predictive analytics has been used for “predictive policing” based on forecasting future crime at the community level, for guiding sentencing and probation, for detecting fraud, for assessing young people’s risk of becoming involved in crime and more. As you are surely aware, bias is an enormous concern and this topic alone could take up a whole courses. We will not have the time to delve is as deep as is warranted due to time constraints, but we will discuss this topic later, and I will provide some ideas for further reading. For now, this semi-recent New York Times article provides a brief overview of some applications and concerns about bias. As an optional, longer read, you can check out this Pro-Publica story about algorithmic bias in sentencing.\nHealth care: The adoption of electronic health records (EHRs) by most US health care systems for patient care has led to an explosion of predictive analytics in health care - with applications aimed at improving health outcomes, care coordination, and quality of care. Health care systems and insurance companies harness patient demographics, insurance claims data, and clinical characteristics in EHRs to create statistical models of future health care risks and resource utilization. There are also efforts to incorporate social and behavioral determinants of health (SBDH), which include measures of diet and physical activity as well as characteristics of patients’ neighborhoods, such as food access and transportation.\nThe above examples do not capture the breadth of growing applications of using predictive analytics to improve social services. To read about some additional examples, see the following short articles and blog posts. These are OPTIONAL.\n\nAnticipatory government: Preempting problems through predictive analytics\nCatalogue of predictive models in the humanitarian sector\nWhat is predictive analytics and what could it mean for local governments?\n\n\n\nSome examples of using predictive analytics to improve consumer services:\nI do not have first-hand experience in using or reviewing predictive analytics to improve consumer services, but of course there are many! Applications in social services have been inspired by applications in consumer services. Here are a few articles that provide some good summaries and discussion. Not all applications discussed focus on estimating binary outcomes, but many, such as predicting customer churn, product purchase or customer satisfaction can be examples of predicting binary outcomes.\n\nPredictive customer insight is the future\nHow predictive analytics can improve customer experience\n\nQuestions for discussion:\n\nWhat examples do you find interesting for using predictive analytics to improve services? Why?\nWhat promise do you see in the examples you read about or know about?\nWhat concerns do you have?\n\n\n\n\n\n\n Back to topReferences\n\nGolub, C., Redcross, C., & Valentine, E. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment effects of new jersey’s criminal justice reform.\n\n\nHuman Services, A. C. D. of. (2019). Impact evaluation summary of the allegheny family screening tool.\n\n\nMDRC. (2023). Virginia pilot: TANF data collaborative.\n\n\nRedcross, C., & Henderson, B. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment.\n\n\nXia, S., Htet, Z., Porter, K. E., & McCormick, M. (2023). Exploring the value of predictive analytics for strengthening home visiting: Evidence from child first."
  },
  {
    "objectID": "paforimprovingss_limitations.html",
    "href": "paforimprovingss_limitations.html",
    "title": "What are the limitations of PA for improving services?",
    "section": "",
    "text": "While PA has the potential to provide valuable insights for improving services, it also has many limitations that data scientists need to consider when scoping, implementing and communicating plans and results. The following provides a brief overview of some limitations of PA. We will return to these in more depth later.\n\nPredictions are only as good as the data on which they are built. If the dataset available for analysis is incomplete, inaccurate, or biased, it can lead to unreliable predictions.\nThere can be a trade-off between model performance and transparency and simplicity. Sometimes the best-performing models are complex machine learning algorithms and can therefore be difficult to explain and can be challenging to deploy and maintain. A lack of transparency can be especially problematic in certain domains with more ethical considerations.\nExternal factors may change: Predictive models rely on patterns in the past to make predictions about the future. But models may not account for unexpected events, external influences, or sudden changes in the environment that can substantially impact the outcome being predicted. As the time horizon extends further into the future, models may become unreliable. For this reason, predictive models need regular monitoring and may require frequent updating.\nPredictive analytics tells us nothing about causal relationships. Nor does it tell us that those with the lowest or highest probability of an outcome are the most likely to benefit from a new or revised service or intervention. This evaluation is determined by other analyses (e.g. rigorous evaluations), or is sometimes assumed based on the expertise of those using predictive analytics findings for decision-making.\nEthical Concerns: Predictive analytics can raise many ethical concerns. Ethics is a very important topic, which we turn to next and will return to throughout the course.\n\nMany of the lessons we learn in inferential statistics do not apply to predictive analytics. You can find a short discussion of some of the differences between predictive analytics and inferential statistics here. This is OPTIONAL.\nAnd for fun, Baba Brinkman covers some of these issues in his rap comparing and contrasting data science and statistics. (Click link below.) (Also OPTIONAL)\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_context.html",
    "href": "paforimprovingss_context.html",
    "title": "Is predictive analytics a good fit for your context?",
    "section": "",
    "text": "Before embarking on a PA project, it is valuable to assess whether it is the right approach at the right time for the setting. Here are a some key considerations to determine if PA is a good fit.\n\nWill PA provide new and actionable information?\n\nDo we anticipate that the PA findings will provide information that is more reliable, more frequent or easier to understand than current information available?\nHow will findings from PA be communicated and used, and by whom?\nHow frequently do PA findings need to be updated?\n\n\n\nAre there plans and buy-in for acting on insights that come out of PA?\n\nIs there sufficient leadership and commitment from the institution?\nIs there capacity for interpreting, communicating and and acting on PA results? How will findings be communicated?\nAre there plans for how program services will be changed or how new services will be introduced based on PA results?\nTo what extent does decision making and service provision currently rely on data-driven findings? Are data-driven findings in general, and PA findings more specifically, trusted and used? If not, are there plans for addressing practice and culture to support change?\nAre there structural or process factors that impede or foster the use predictive analytics (such as dedicated data teams) or the use of results (such as dashboard systems)?\n\n\n\nData quality and systems\n\nAre there sufficient data that are readily accessible?\nAre the data sufficiently documented and understood so that reliable measures (variables to be used as outcomes or predictors) can be created for PA?\nIs there sufficient capacity for investigating and deploying PA with existing analytic systems, software and staffing?\n\n\n\nPotential risks\n\nHas their been sufficient investigation into identifying potential risks?\nAre there systems or processes in place to make sure ethical issues are attended to throughout a PA project?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_optional_PAvsInfStat.html",
    "href": "paforimprovingss_optional_PAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "OptionalPAvsInfStat.html",
    "href": "OptionalPAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of all Gentoo penguins in the world? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches). To construct these estimates about our population, we use information from a sample we have collected, such as a set of 20 Gentoo penguins caught and measured by ecologists.\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "issuelog.html",
    "href": "issuelog.html",
    "title": "Issue log",
    "section": "",
    "text": "Formatting of references weird - how to capitalize where needed?\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "href": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "href": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders - to include as much as possible the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\n\nEthical Considerations in Predictive Analytics Projects\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices to follow:\n\nUnderstanding Regulatory Frameworks: The first step is to be well-versed in relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and ensures that ethical considerations are thoroughly explored from various perspectives.\nDevelop a Code of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nMinimizing Bias: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nOutcome Assessments: Conduct comprehensive outcome assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these practices, organizations can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "scoping_who.html",
    "href": "scoping_who.html",
    "title": "Who are we making predictions for?",
    "section": "",
    "text": "Our target population is specifically adult TANF recipients who are eligible and approved for the TANF department’s education and training program and who are required to work.\nOur data sets include a broader group of TANF applicants and participants so we need to be careful to exclude those who do not meet our criteria.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressEthical",
    "href": "paforimprovingss_ethics.html#addressEthical",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders. As much as possible, we should try to include the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices that may be helpful or necessary, depending on the project:\n\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and helps ensure that ethical considerations are thoroughly explored from various perspectives.\nCode of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nRegulatory Frameworks: Teams should be well-versed in relevant regulations and laws that may apply to their data and context, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nBias Minimization: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nProject Assessments: Conduct comprehensive PA project assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This training ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these kinds of practices (when they apply to the context), government agencies, organizations and companies can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "course_syllabus.html",
    "href": "course_syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_grading.html",
    "href": "course_grading.html",
    "title": "Assessment and grading",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_attendancenorms.html",
    "href": "course_attendancenorms.html",
    "title": "Class attendance and participation norms",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_gethelp.html",
    "href": "course_gethelp.html",
    "title": "Six ways to get help",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Course information",
    "section": "",
    "text": "This section of this course website provides course information.\n\n\n\n Back to top"
  },
  {
    "objectID": "course_syllabus.html#course-details",
    "href": "course_syllabus.html#course-details",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.html#course-description",
    "href": "course_syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nDescription:\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. Students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\n\n\nPrerequisites:\nStudents should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\nLearning objectives:\n\nGain the ability to critically assess the potential value and limitations of predictive analytics for improving services when applied to a particular context and data system.\nUnderstand how results from predictive analytics can provide actionable insights for improving services.\nUnderstand how to plan and implement all steps of a predictive analytics workflow using open-source code templates. The steps include planning the analysis to match goals, exploring and preparing data, specifying and training models, and assessing the validity and bias of predictive models – all while prioritizing best-practices for ethics, transparency and replication.\nGain a high-level understanding of sta(s(cal concepts for optmizing predictive analytics and have op(onal choices to dive more deeply.\nPresent predictive analytics results using an R notebook and clearly communicate interpretations, cautions and recommendations.\n\n\n\nAbout the Data Science Academy:\nWelcome to an NCSU Data Science Academy Course! In July 2021, the universitywide and interdisciplinary Data Science Academy was launched to meet the growing needs of data science research, education and expertise in North Carolina and beyond. At NC State, Data Science is for Everyone. Data Science Academy (DSA) Courses are designed to make sure that each student can pursue appropriate level challenges through opportunities to make choices and pursue projects of interest. Whether you have never thought about data science before or bring experience and expertise, we welcome you. Our goal is that after each DSA class you want to learn more! For more on the DSA course design. \n\n\nAbout DSA research:\nTo make sure that we are providing an appropriate collection of courses with a variety of challenge levels within each course, we will be collecting data to help us build a practice of continuous improvement. The purpose of the data is to evaluate the DSA and how well we are serving our students. We hope to be able to share what we learn with other universities and researchers - we will ask your permission at the beginning of the course to be able to share your anonymized data when we communicate about the work of the DSA."
  },
  {
    "objectID": "course_syllabus.html#resources",
    "href": "course_syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThere is no textbook for this class. All reading materials are provided within this website and can be accessed in the “Course Materials” tab at the top of the page. The course materials includes text developed by the instructor as well as links to helpful reading, which may be required or optional as noted."
  },
  {
    "objectID": "course_syllabus.html#grading",
    "href": "course_syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nFORTHCOMING"
  },
  {
    "objectID": "course_syllabus.html#course-schedule",
    "href": "course_syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule is linked here."
  },
  {
    "objectID": "course_syllabus.html#attendance-policy",
    "href": "course_syllabus.html#attendance-policy",
    "title": "Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nFor complete attendance and excused absence policies, please see http://policies.ncsu.edu/regulation/reg-02-20-03. For coronavirus related attendance guidance, see the “coronavirus information” section.\nAttendance Policy. Students are expected to attend class regularly. Since we only meet once a week, each class is vitally important to keep on track during the semester.\nAbsences Policy. Students should notify the instructor that they will be unable to attend class and schedule a time to meet with the instructor to review materials they missed during class time.\nMakeup Work Policy. Makeup work will be allowed if the student notifies the instructor at least one day before an assignment is due to arrange for a deadline extension. Notification given on the due date will be considered on a case-by-case basis."
  },
  {
    "objectID": "course_syllabus.html#academic-integrity",
    "href": "course_syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nStudents are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01.\nStudents are expected to produce work that they personally completed and in accordance with the instructions given for the assignment, project, test or quiz. Work copied from other students, completed by another individual, or directly taken from other sources without appropriate attribution will be in violation of academic integrity. Students are expected to follow the Code of Student Conduct (NCSU POL11.35.01) and Pack Pledge, and violations of academic integrity will be handled in accordance with the Student Discipline Procedures (NCSU REG 11.35.02).\nHonor Pledge. Your submission on any test or assignment indicates “I have neither given nor received unauthorized aid on this test or assignment.” Digital Course Components Students may be required to disclose personally identifiable information to other students in the course, via digital tools, such as email or web-postings, where relevant to the course. Examples include online discussions of class topics, and posting of student coursework. All students are expected to respect the privacy of each other by not sharing or using such information outside the course."
  },
  {
    "objectID": "course_syllabus.html#digital-course-components",
    "href": "course_syllabus.html#digital-course-components",
    "title": "Syllabus",
    "section": "Digital Course Components",
    "text": "Digital Course Components\nOur primary form of communication outside of online Zoom meetings will be through Moodle. Students are expected to monitor Moodle for announcements, assignments, and other information during the semester. Assignments. Assignments and final projects may be given and submitted through Moodle."
  },
  {
    "objectID": "course_syllabus.html#accommodations-for-disabilities",
    "href": "course_syllabus.html#accommodations-for-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Disabilities",
    "text": "Accommodations for Disabilities\nReasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with the Disability Resource Office at Holmes Hall, Suite 304, Campus Box 7509, 919-515-7653. For more information on NC State’s policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation (REG02.20.01). Non-Discrimination Policy NC State provides equal opportunity and affirmative action efforts, and prohibits all forms of unlawful discrimination, harassment, and retaliation (“Prohibited Conduct”) that are based upon a person’s race, color, religion, sex (including pregnancy), national origin, age (40 or older), disability, gender identity, genetic information, sexual orientation, or veteran status (individually and collectively, “Protected Status”). Additional information as to each Protected Status is included in NCSU REG 04.25.02 (Discrimination, Harassment and Retaliation Complaint Procedure). NC State’s policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or https://oied.ncsu.edu/divweb/. Any person who feels that he or she has been the subject of pro hibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148."
  },
  {
    "objectID": "course_syllabus.html#technology-requirements",
    "href": "course_syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThis course may require particular technologies to complete coursework. Specifically, students will need R and R Studio. To avoid challenges that may arise due to different versions of packages, the instructor is providing access to code at the this link. Students will need to create accounts with their email address in order to gain access.\nIf you need access to additional technological support, please contact the Libraries’ Technology Lending Service: https://www.lib.ncsu.edu/devices."
  },
  {
    "objectID": "course_syllabus.html#resources-1",
    "href": "course_syllabus.html#resources-1",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThese are difficult times, and academic and personal stress is a natural result. Everyone is encouraged to take care of themselves and their peers. If you need additional support, there are many resources on campus to help you: \n• Counseling Center https://counseling.dasa.ncsu.edu/ \n• Health Center https://healthypack.dasa.ncsu.edu/ \n• If you or someone you know are experiencing food, housing or financial insecurity, please see the Pack Essentials Program https://dasa.ncsu.edu/pack-essentials/ \nSupporting Fellow Students in Distress. As members of the NC State Wolfpack community, we each share a personal responsibility to express concern for one another and to ensure that this classroom and the campus as a whole remains a healthy and safe environment for learning. Occasionally, you may come across a fellow classmate whose personal behavior concerns or worries you, either for the classmate’s well-being or yours. When this is the case, I would encourage you to report this behavior to the NC State’s Students of Concern website: https://prevention.dasa.ncsu.edu/nc-state-cares. Although you can report anonymously, it is preferred that you share your contact information so they can follow-up with you personally. I may also make a referral to the CARES program if I notice you are having a difficult time. If you don’t know who to talk to or which steps to take, please don’t hesitate to come talk to me and I will help you the best I can. \nStudents are responsible for reviewing the NC State University Policies, Rules, and Regulations (PRRs) which pertain to their course rights and responsibilities, including those referenced both below and above in this syllabus:\nEqual Opportunity and Non-Discrimination Policy Statement: https://policies.ncsu.edu/policy/pol-04-25-05 with additional references at https://oied.ncsu.edu/divweb/policies/\nCode of Student Conduct: https://policies.ncsu.edu/policy/pol-11-35-01."
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "welcome.html#predictive-analytics-for-improving-services",
    "href": "welcome.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "index.html#predictive-analytics-for-improving-services",
    "href": "index.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "course_syllabus.html#syllabus",
    "href": "course_syllabus.html#syllabus",
    "title": "1 Course Description",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "scoping_what.html",
    "href": "scoping_what.html",
    "title": "What are we trying to predict?",
    "section": "",
    "text": "It is important to be very thoughtful and precise in defining and developing a measure of the outcome to be predicted. In our motivating example, we want to predict employment, but how exactly should we define and measure employment? When specifying an outcome, the following considerations are essential:\nIn our example, we will focus on predicting just one outcome measure, but multiple measures of employment that make sense for the goals of the department could be analyzed. For each outcome, the analytic procedures would need to be repeated.\nOur outcome: The selected precise outcome measure for our example is: whether a client holds a job covered by unemployment insurance (UI) for the first three quarters after exiting the department’s education and training program for TANF participants. The measure is coded as 1 when achieved and 0 otherwise.\nThis choice was made because of the following answers to the above questions:\nHowever, after defining this measure, the project decided to code this outcome in the negative – so that we were estimating probabilities of whether someone does not achieve the employment outcome measure defined above. Therefore, our predicted probabilities represent risk scores. (I initially framed it as predicting employment because it is much easier to describe that way.)\nAlso, when discussing this project with stakeholders, we want to emphasize sensitivity to clients’ experiences. While we focus on “success” definitions that inform our measures (subsequently turned into “risk” scores), we avoid implying “failure” when clients fall short of departmental employment goals. We acknowledge the multifaceted challenges clients encounter and the program’s mission to assist those facing elevated barriers."
  },
  {
    "objectID": "scoping_when.html",
    "href": "scoping_when.html",
    "title": "When are we making predictions?",
    "section": "",
    "text": "Deciding when to make predictions is not always straightforward, but is critical to developing a valid prediction model. \nThe key is to balance two important considerations:\n\nFirst, we want to run predictive models early enough for the findings to be actionable. In our case, we want our caseworkers to be able to help clients early enough that they can address challenges.\nBut running predictive models later may mean we have more data measures available to include in our models and therefore potentially have more accurate models.\n\n\n\nIn our TANF program example, through conversations with the program managers and caseworkers on our project team, we learned (a) that interventions should be introduced very early, so they wanted risk scores available soon after client intake and approval for the education and training program. But we also learned (b) that there can be substantial lag in the entry of most of data collected during the intake and approval processes for TANF clients.\nWe therefore decided to specify our prediction timepoint as one month after the approval process for the education and training program.\n\nThis allowed time for most measures collected up through approval to be entered into the system.\nThis means that our models can only include measures that would feasibly be entered before our timepoint.\nIf the model is deployed, we would need to be careful to wait to apply the model to new data only at the correct timepoint – not earlier. Otherwise, we would have a lot of missing data, which could lead to poor predictions. We will discuss this topic more later.\n\n\nIt is a common mistake to not give adequate thought to when predictions are being made and to which measures are available at that time. If we ignore this issue, we might generate our risk scores (a) too early, which means our models might use measures that would not yet be available in practice, or (b) too late, which means our models might use measures that are collected very close to the outcome.\nFirst, we discuss (a), generating risk scores too early in the process. If we are not careful about understanding not just the data, but the data collection process, then we might accidentally use data that would not realistically be available at our chosen time point. If we had not spoken in depth with the program managers, we would not have known that client intake data was not immediately available, but instead could take a few weeks to be input into the system. As discussed above, if we make this mistake, our models would look effective in our test phase, but would have poor performance in practice due to missing data.\nSecond, we discuss (b), generating risk scores too late in the process. This strategy could lead to excellent performance - great accuracy - but useless predictions. To illustrate this type of mistake, consider if our data includes a measure of whether a TANF client has submitted a job application at the end of a job preparation class. Including this measure in our model could improve its predictive performance. However, because this measure is collected towards the end of the education and training program, if we wait to generate risk scores after this measure is collected (setting our timepoint at the end of the education and training program), there is little time left to take action to help those at highest risk of not finding and sustaining employment.\nThink about this issue when you read or hear about the high accuracy of predictive models. If a company boasts that it predicts an outcome with very high accuracy, it is important to ask:\n\nWhen are predictions being made?\nIs it possible that measures are collected so close to the outcome that some predictors are actually proxies for the outcome?\nWhat trade-offs are being made in terms of predictive accuracy versus having time to act on predictive analytics results?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_stakeholders.html",
    "href": "scoping_stakeholders.html",
    "title": "Who are the stakeholders?",
    "section": "",
    "text": "forthcoming\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_what.html#footnotes",
    "href": "scoping_what.html#footnotes",
    "title": "What are we trying to predict?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs summarized by the Employment and Training Administration in the U.S. Department of Labor: These data are from the Unemployment Insurance Data Base (UIDB) as well as UI-related data from outside sources (e.g., Bureau of Labor Statistics data on employment and unemployment and U.S. Department of Treasury data on state UI trust fund activities). See this link for more details.↩︎"
  },
  {
    "objectID": "poc_compare.html",
    "href": "poc_compare.html",
    "title": "Adding and comparing learners",
    "section": "",
    "text": "Once we have specified our benchmark learner, we can specify additional learners for comparison. If simplicity and transparency are goals, we will want to incrementally introduce complexity. For example, for our high school graduation example, we might take the following approach:\nThis results in a total of nine learners because we have nine combinations of predictor sets and modelling approaches. Ultimately, we will then have nine predictive models that result from the nine learners.\nNote: Machine learning algorithms typically require specifications for how they are implemented. These are called “tuning parameters.” For example, the random forest algorithm has tuning parameters that include the number of decision trees in a forest, the maximum depth of the decision trees, and several more options. Different values of tuning parameters have different implications for the performance of the algorithm. While the analyst can specify these tuning parameters, it is typically best to use a data-driven approach. We will dive more into this later. However, the point to take away here is that we will “tune” each machine learning algorithm for each predictor set. So when we ultimately compare learners, we are comparing “tuned” versions of them."
  },
  {
    "objectID": "poc_learners.html",
    "href": "poc_learners.html",
    "title": "Defining learners",
    "section": "",
    "text": "Here we present a framework for conducting a PA proof-of-concept. The framework focuses on specifying and comparing “learners.”\nA learner is a combination of:\n\nA predictor set: a set of measures that are available at the prediction timepoint and that are potentially predictive of the outcome of interest.\nA modeling approach: a method for combining and weighting measures, e.g., regression or a particular machine learning algorithm.\n\n\n\n\n\n\nMachine learning algorithms automate model building using a series of steps that are driven by patterns in the data - rather than relying on functional forms specified by an analyst. Examples include decision trees, random forests, stepwise regression, and support vector machines, among many others.\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_importance.html",
    "href": "poc_importance.html",
    "title": "The importance of a proof-of-concept",
    "section": "",
    "text": "After we have scoped our predictive analytics project, the next step is to conduct a “proof-of-concept.” That is, before getting too invested in incorporating predictive analytics into systems of service improvement, we need to investigate the following:\n\nGiven the available data, how well can we predict our outcome of interest?\nWhen investigating this question, we will want to consider various metrics of model performance, which we will turn to in a later section. We will be concerned with the reliablity of our results. How “accurate” are they? I put “accurate” in quotations because I am referring to a general assessment of model performance, rather than the precise statistical definition of accuracy, which we will discuss soon, along with many other metrics of predictive performance.\nGiven the available data, to what extent is there bias in our predictions?\nWhen investigating this question, we will want to consider various metrics of bias, which we will also turn to in a later section. Here we will be concerned with whether and by how much model performance varies for different groups. Later, we will discuss that there are multiple definitions of bias and therefore multiple ways to measure it.\nTo what extent are complex predictive models worthwhile? Do they improve upon a simpler model or decision-making process? Do they improve upon current practice?\nWhen investigating this question, our focus is on contrasting predictive capabilities and biases across various prediction approaches. We start by looking at the simplest approach, which doesn’t necessarily involve a statistical model at all. A simple decision-making process might involve taking a small number of measures and combining and weighting them in a straightforward manner based on prior knowledge. For one version of this approach, we may set criteria that assign units into one predicted catgory or the other. For example, in the high school graduation example, perhaps any student with attendane less than 90% and with less than 20 credits is predicted to be high-risk for not graduating on time (we predict they won’t). This type of approach is sometimes called a rules-based approach or a decision tree, among other terminology.\nMoving to more complex approaches, we could plug a small number of measures into a regression model to estimate their relationship with the outcome. Finally, the most complex approaches might involve adding more measures, potentially a large number, and/or using advanced, data-driven algorithms - i.e., machine learning algorithms.\nThe crux then lies in weighing the pros and cons when comparing different models. Should a more complex approach yield superior results compared to a simpler one, it prompts a consideration: Are the improvements significant enough to offset potential drawbacks in terms of transparency and explainability? Additionally, do these improvements outweigh any potential challenges in implementing and sustaining a more intricate modeling process?\nDo stakeholders understand and trust the results from the “best” predictive model?\nWhile stakeholders should be engaged in the PA scoping process, the proof-of-concept provides another opportunity for valuable consultation. Sharing results used to investigate the above questions supports transparency and trust in the process. Stakeholders’ input and questions are essential to the goals of the proof-of-concept - to assess whether and how predictive analytics should be deployed, communicated and used to improve services.\n\n\n\nFor further consideration: Are there other questions you would want to investigate in a proof-of-concept?\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_benchmark.html",
    "href": "poc_benchmark.html",
    "title": "Specifying a benchmark learner",
    "section": "",
    "text": "If simplicity and transparency are goals for our predictive model (these goals may not apply in all contexts), then we want to specify a simple and transparent learner as a “benchmark” to which all other, increasingly complex learners are compared.\nA benchmark learner consists of a benchmark prediction set (with a small number of measures) - and a benchmark modeling approach (one that is simple to explain, such as a decision tree or regression model).\nThe benchmark learner may align with how a service provider is already making decisions. For example, the benchmark learner may include the same measures the service currently reviews in making a decision, even if they do not create a model. Or, the benchmark may focus on measures that the service provider thinks are most important based on their expertise or based on related research.\nRemember, any measures included in the benchmark prediction set must be available at the prediction timepoint and for the prediction population.\nLet’s consider an example: Imagine we are exploring how we might use predictive analytics to help a school system improve their “early warning system,” and we have scoped the project as follows:\n\nWhat are we aiming to predict? Whether a student will not graduate from high school on-time (within 4 years of enrolling in 9th grade).\nWhen are we aiming to make predictions? After students have completed 9th grade. The analytics would be done during summer, after all 9th grade information has been entered in data systems. Prediction results would be available to educators before the start of students’ 10th grade.\nFor whom are we aiming to make predictions? All students who enrolled in 9th grade. However, we would have to exclude students who transfer outside the district during high school since we cannot know their graduation status. (Note this can create a missing data problem that we will discuss later.)\n\nHow might we define a benchmark learner for this example?\n\nThe school district has been focusing on so-called ABC indicators of attendance, behavior, and course performance to identify which students are most at-risk of not graduating on time. Therefore, a good benchmark predictor set could include the same three measures of attendance, behavior and course performance the district has used. If the district used these three measures as “indicators” (e.g. deemed a student at-risk if they passed a threshold on 2 of the 3 measures), then we could replicate the same empirical rules as a benchmark modeling approach. Alternatively, we could combine the three measures with a logistic regression model.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "poc_addcomplexity.html",
    "href": "poc_addcomplexity.html",
    "title": "Gradually adding complexity",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_split.html",
    "href": "data_split.html",
    "title": "Data workflow",
    "section": "",
    "text": "Here we will define the learner workflow for a PA proof-of-concept. This involves the following three steps:\n\n\n\n\nflowchart LR\n  A(Training) --&gt; B(Validating)\n  B --&gt; C(Testing)\n  \n  \n\n\n\n\n\n\n\n\nTraining: Uses data with known outcomes to apply a modeling approach (e.g., regression or a machine learning algorithm) in order to build (i.e., estimate, fit, train) a model of the relationship between predictors and the outcome of interest.\nValidating: uses new data with known outcomes (data not used for training) to apply all trained learners/models. The predictions from the trained learners/models are compared to the known outcomes so that metrics of model performance and fairness are computed. This validation can be repeated in multiple validation data sets. Looking across all results, the “best” model is selected. Criteria for a model being selected as “best” vary by context. To determine the “best” learner/model, the team will weigh different definitions and metrics of predictive performance and of fairness. We will take a deep dive into these metrics soon. The team may also weight the simplicity and transparency of the learner, as well as variability across multiple validations.\nTesting: uses new, set aside data with known outcomes (data not used for training or validation) to apply the “best” model. The predictions from the best model are compared to the known outcomes to report on predictive performance and fairness to stakeholders and to make a decision about whether the learner/model should be deployed for use.\n\nIn the following pages, we will go over how to implement these concepts, but first we will discuss considerations for selecting data that would be used across the full workflow.\n\n\n\n Back to top"
  },
  {
    "objectID": "data_splitconsiderations.html",
    "href": "data_splitconsiderations.html",
    "title": "Additional considerations",
    "section": "",
    "text": "How much data do you need?\nUltimately, the goal is to have a large enough training set to train the models effectively, and large enough validation and test sets to get reliable estimates of the models’ performance. The definition of “large enough” can vary based on:\n\nModel Complexity: More complex models (e.g., deep neural networks with many layers and parameters) typically require larger training datasets to avoid overfitting while simpler models (e.g., linear regression, decision trees) may require smaller training datasets.\nData Variability: If the data has a lot of variability or noise, a larger dataset may be needed to capture the underlying patterns and relationships.\nBalance between the classes of a binary outcome: This is a specific case of #2. If one class is much more common than the others, you may need a larger dataset to ensure that the model is exposed to enough examples of each class during training.\nAvailability of Data: Sometimes the amount of available data is a limiting factor. In such cases, it is important to make the best use of the available data and adjust the methods accordingly.\n\nUltimately, if the learner/model performs well in validation and generalizes well to the test data, the dataset is likely large enough. However, if the model overfits to the training data, one reason may be that the modeling approach was too complex for the size of the available data. But there may be other explanations as well - e.g. lack of data consistency over time as discussed a few pages back.\n\n\nWhat ratios should be designated for training, validation and testing?\nThere is no one-size-fits-all answer to this question. A common starting point is a 60% training, 20% validation, and 20% testing split. However, the size of your dataset may necessitate a different allocation. For instance, a very large dataset might allow for a 50% training, 25% validation, and 25% testing split. Conversely, a smaller dataset might require a larger proportion for training, such as a 70% training, 15% validation, and 15% testing split. As previously mentioned, we will be combining the training and validation data to conduct \\(v\\)-fold cross-validation.\n\n\nHow much do all these decisions matter?\n\nNavigating the maze of decisions required for training, validating, and testing can be both confusing and frustrating, especially when guidance is not clear. Here are some key takeaways:\n\nAlways set aside a test set: The most crucial practice is to reserve a test set until a final learner has been selected. This allows for a fair evaluation of your final predictive model’s performance on new, unseen data. Failing to adhere to this practice is very problematic.\n\nRelatedly, remember that metrics of model performance, which we will discuss shortly, are estimates and come with inherent uncertainty. In other words, if you measured model performance using different samples, your results would vary. This uncertainty is even more pronounced with a smaller testing sample size. Although it is often overlooked, a good practice is to estimate this uncertainty, for example, by calculating the standard error or confidence interval of the performance metric estimate.\n\nFocus on the ultimate goal: Throughout the entire process, keep the ultimate goal in mind: to find a model that generalizes well to new, unseen (and often, future) data so that we can provide insights for improving services. With this ultimate goal in mind, ensure that your test set is representative of the data you will use in deployment. Otherwise, your results may be overly optimistic, even if you adhered to the first point above. Additionally, ensure that your training and validation data are as similar as possible to the data you will ultimately use for deployment.\nHow much does the set-up of the training and validation matter?: The specific details of how you set up tuning and validation are important for model selection but probably will not completely derail your project. The worst-case scenario is that you miss the opportunity to select the “best” model, but it is unlikely that you will choose the “wrong” model. Poor performance is more likely due to the limitations of your data and context rather than the choices made in setting up cross-validation.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_overfitting.html",
    "href": "data_overfitting.html",
    "title": "Generalizing to new data",
    "section": "",
    "text": "The objective of predictive analytics\nRecall that our objective, as shown in this simple illustration, is to use data with known outcomes to build a model that allows us to predict outcomes - or the likelihood of outcomes - in data with unknown outcomes. As a reminder, we are focusing on binary outcomes (those with just two values - yes or no), which is why we focus on predicting probabilities or likelihoods of outcomes.\n\n\n\nAvoiding “overfitting”\nHow do we make sure that our learner, or model, which is based on information in the data with known outcomes (data from the past), generalizes to new data? If our learner/model is too closely tailored to the data we use to develop it, it risks capturing incidental fluctuations or noise unique to that data in addition to meaningful statistical patterns. When a model inadvertently incorporates this noise, it excels at describing and fitting the original data. However, when confronted with new, unseen data, this same model tends to deliver inaccurate predictions. This is called “overfitting.” Overfitting refers to a model’s inability to generalize to new data - due to being constructed around specific noise rather than based on broader patterns.\nThe following simple plots display the concept of overfitting, as an overcorrection to “underfitting.” Each plot shows an attempt to capture the relationship between two measures. In the first plot, the estimated model, represented by the blue line, does not capture the trend evident in the data, shown by orange points. This is corrected in the second plot, in which the model appears to be a a good fit of the general trend. Then, in the the third plot, the model fits the data extremely well, but it overfits because it predicts almost every point - almost every random variation from the overall trend we care about.\n\nIn this first section on data for predictive analytics, we will learn how to develop, compare, and select learners/models so that we identify the one that does the “best” job in generalizing to new, unseen data. Note that “best” is in quotation marks because we have not yet discussed what how we define “best”. We will turn to that soon.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_validate.html",
    "href": "data_validate.html",
    "title": "Data for validating learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_train.html",
    "href": "data_train.html",
    "title": "Data for training learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_test.html",
    "href": "data_test.html",
    "title": "Designating data for testing",
    "section": "",
    "text": "Once we have identified the appropriate data for all steps (train, validate, test) of our PA proof-of-concept workflow, we want to work backwards. So first, we want to split off a subset that will be used for testing. Some notes about designating the testing data:\n\nThe test data will not be analyzed until we have completed learner training and validation and have selected our “best” model. We should set it aside and not touch it until we have made a final learner selection.\nIn the case that the goal of predictive analytics is to make predictions for *future* service recipients, then ideally, we want our test data set to include individuals (or other units) that are forward in time, compared to the data we will use for training and validation. This provides the best check of how well our “best” model generalizes not only to different data, but also to the future.\nIdeally, we would want data as close in time as possible to when we would deploy a predictive model. This might be tricky. For example, if our proof-of-concept is assessing possible deployment for predicting high school drop out risk of 10th graders in the 2024-2025 school year, then the best test data may be the most recent cohort of 10th graders for whom we know graduation status. This would be students who graduated in 2023 and were therefore 10th graders in the 2020-2021 academic year.\nIn some cases, we might be making predictions for a new location (e.g. for a new site, office, store). Then, we might designate testing data that is from a different location than data we will use for training and validation.\n\nUltimately, for an honest assessment of our selected, “best” model, we want to conduct testing in a test data set that best mimics how the predictive model would be deployed.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss.html",
    "href": "paforimprovingss.html",
    "title": "What is predictive analytics for improving services?",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_identify.html",
    "href": "data_identify.html",
    "title": "Identifying data",
    "section": "",
    "text": "We want to be thoughtful about the data that we will use for our proof-of-concept and that we will split for training, validating and testing. Here are some important considerations for identifying data:\n\nThe data should be representative of the population and context for the predictive analytics objective. Here we want to make sure we have data for units that are experiencing a similar context as what we expect for the units for whom we will ultimately deploy predictive analytics.\n\nRecall our earlier example that centered on predicting TANF training program participants’ risks of not finding and sustaining employment. When determining which years of past data to incorporate into our learner workflow, it is essential to examine if and when the eligibility criteria for the training program underwent significant alterations. It may be prudent to restrict our data to a timeframe in which the eligibility criteria for the training program closely resemble those currently applicable to the TANF training program.\nIn our other example, in which the focus is on predicting the risk of 10th grade students not graduating on time, it is important to investigate the graduation guidelines. If the student cohorts used to train our models had more stringent or more lenient graduation standards compared to the cohorts to which we apply the models, the models might not be widely applicable in years beyond our training-validation-testing workflow.\n\nImportant measures should be consistently available. Here we want to assess whether we have sufficient consistency in the way measures are defined and entered into data systems. For example:\n\nHave the data integrations been maintained consistently over time? Specifically, if the data we intend to use is a combination of multiple underlying data sources, have these merges been conducted consistently? If not, crucial measures may occasionally be absent. Additionally, are these integrations/merges sustainable in the future? If certain data cannot be reliably obtained and integrated on an ongoing basis, they should be omitted from the predictive analytics proof-of-concept. It is imperative to consider not only historical data but also prospective scenarios. If particular data will be unavailable when implementing predictive analytics, then a proof-of-concept relying on that data will not be authentic.\nWas there a rollout of a new data system that had early implementation challenges? If so, we may want to eliminate data from those early years in our training-validation-testing workflow.\nIs survey data part of our data universe for predictive analytics? If so, do response rates vary substantially over time? Variation in response rates could (but not necessarily) mean that different populations are being captured with each survey. We would need to investigate this.\nWere there broad changes in how measures where defined or coded? We may want to limit the data we will use to train, validate and test models to a period in which the measures are consistent to the present (time close to when any model would be deployed).\nNote that inconsistencies in some measures could have implications in measures we include in our prediction sets rather than implications for how we restrict our data for our proof-of-concept. That is, if some measures change over time, we may need to exclude just those measures from our models. If there is sufficient consistency across key predictors in data we may not need to restrict the data. We will return to measure consistency later, when we discuss data quality issues.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data_trainvalidate.html",
    "href": "data_trainvalidate.html",
    "title": "Training and validating learners",
    "section": "",
    "text": "Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Tool Set templates you will use, but I will comment on some alternative variations.\nRecall that it is imperative to train (i.e. fit a model or build a machine learning algorithm) in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.\nWe can set aside a portion of the non-test data and designate that portion for validation. We can then use the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness. This is common and valid practice.\nWe can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to \\(v\\)-fold cross-validation, in which \\(v\\) is the number of folders or partitions - or times we repeating the training and validation. The following diagram illustrates 5-fold cross-validation:\n\n\n\n\n\nThis diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Tool Set does a combination of both. For a few metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code. (And don’t worry - the PA Tool Set implements \\(v\\)-fold cross-validation for the user. You will not need to code the procedure; you will just specify how to the cross-validation should be implemented, following the guidance below.)\n\nGuidance for implementing \\(v\\)-fold cross-validation:\n\nNumber of folds: A recommended default for \\(v\\) is 5. Higher values of \\(v\\) (more folds) may be preferable if there is a large amount of data because the average performance across the folds will be more representative of the true performance (less biased), albeit with more variability in the estimates (larger variance). Additionally, more folds result in increased computation time, which can be significant. Alternatively (although also more computationally demanding), the entire \\(v\\)-fold cross-validation procedure can be repeated multiple times. In other words, we can obtain results from 5 folds, then resample to create 5 different folds and consequently acquire another set of results from another 5 folds of the same size. This approach may be advantageous if the sample size is small, as there could be substantial variability in the metrics across folds. (However, our current tool set code templates do not support this functionality.)\nHow to specify folds: Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of \\(v\\)-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.\nStratification: In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is “yes” or “1” for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few “yes” or “1” outcomes. To protect against this, we can stratify the cross-validation. This means we can (1st) randomly partition those observations for which the outcome = 1; and then (2nd) separately randomly partition those observations for which the outcome = 0; then (3rd) pair each partition with outcome =1 with a partition with outcome = 0 to get \\(v\\) folds with equal proportions of each value of the outcome. Note that the PA Tool Set templates will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.\n\n\n\nTuning with \\(v\\)-fold cross-validation:\n\nThe PA Tool Set will automatically optimize tuning parameters. It does this with \\(v\\)-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.\nThe comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the \\(v\\) folds, will be selected as the “tuned learner.” The user will then only view results of the tuned learner going forward.\n\nNote that using the same cross-validation procedure for both parameter tuning and learner validation and comparison is not technically, completely correct. Using the same data twice for both goals is sometimes called “double-dipping.” That is, using the same data again to validate the performance of the tuned learner, we may get overly optimistic performance estimates (overfitting) because the models have already “seen” that data during the tuning process. To correct for this, there are two approaches: (1) As described above, we can set aside a subset of data for validation. In this case, we first conduct parameter tuning using \\(v\\)-fold cross-validation with the training data, but then compare the tuned learners’ performance in the held-out validation data. Alternatively, (2) we can use a nested \\(v\\)-fold cross-validation. In nested cross-validation, we have an outer loop of cross-validation for model validation, and an inner loop of cross-validation for parameter tuning. This allows you to use the same data for both tuning and validation, but in a way that prevents biased performance estimates. This approach may be preferable when there are not sufficient data to hold out a separate validation data set. Sorry, our tool set templates do not have these options at this time.\nBut although our performance in the validation may be inflated, it should be inflated for all learners, still allowing us to make valid comparisons and to select the “best” learner. Remember, it is the testing that provides the best assessment of how well our models actually perform in new, unseen data.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "toolset_intro.html",
    "href": "toolset_intro.html",
    "title": "Introduction to the PA Toolset",
    "section": "",
    "text": "Here introduce high level overview of PA toolset\nInstructions for getting set up on server\nOverview of files - snapshot of directory\nShow how to enter in information about prediction objective and learners?\n\n\n\n Back to top"
  },
  {
    "objectID": "course_syllabus.Rmd.html",
    "href": "course_syllabus.Rmd.html",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-details",
    "href": "course_syllabus.Rmd.html#course-details",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-description",
    "href": "course_syllabus.Rmd.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nDescription:\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. Students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\n\n\nPrerequisites:\nStudents should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\nLearning objectives:\n\nGain the ability to critically assess the potential value and limitations of predictive analytics for improving services when applied to a particular context and data system.\nUnderstand how results from predictive analytics can provide actionable insights for improving services.\nUnderstand how to plan and implement all steps of a predictive analytics workflow using open-source code templates. The steps include planning the analysis to match goals, exploring and preparing data, specifying and training models, and assessing the validity and bias of predictive models – all while prioritizing best-practices for ethics, transparency and replication.\nGain a high-level understanding of sta(s(cal concepts for optmizing predictive analytics and have op(onal choices to dive more deeply.\nPresent predictive analytics results using an R notebook and clearly communicate interpretations, cautions and recommendations.\n\n\n\nAbout the Data Science Academy:\nWelcome to an NCSU Data Science Academy Course! In July 2021, the universitywide and interdisciplinary Data Science Academy was launched to meet the growing needs of data science research, education and expertise in North Carolina and beyond. At NC State, Data Science is for Everyone. Data Science Academy (DSA) Courses are designed to make sure that each student can pursue appropriate level challenges through opportunities to make choices and pursue projects of interest. Whether you have never thought about data science before or bring experience and expertise, we welcome you. Our goal is that after each DSA class you want to learn more! For more on the DSA course design. \n\n\nAbout DSA research:\nTo make sure that we are providing an appropriate collection of courses with a variety of challenge levels within each course, we will be collecting data to help us build a practice of continuous improvement. The purpose of the data is to evaluate the DSA and how well we are serving our students. We hope to be able to share what we learn with other universities and researchers - we will ask your permission at the beginning of the course to be able to share your anonymized data when we communicate about the work of the DSA."
  },
  {
    "objectID": "course_syllabus.Rmd.html#resources",
    "href": "course_syllabus.Rmd.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThere is no textbook for this class. All reading materials are provided within this website and can be accessed in the “Course Materials” tab at the top of the page. The course materials includes text developed by the instructor as well as links to helpful reading, which may be required or optional as noted."
  },
  {
    "objectID": "course_syllabus.Rmd.html#grading",
    "href": "course_syllabus.Rmd.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nFORTHCOMING"
  },
  {
    "objectID": "course_syllabus.Rmd.html#course-schedule",
    "href": "course_syllabus.Rmd.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule is linked here."
  },
  {
    "objectID": "course_syllabus.Rmd.html#attendance-policy",
    "href": "course_syllabus.Rmd.html#attendance-policy",
    "title": "Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nFor complete attendance and excused absence policies, please see http://policies.ncsu.edu/regulation/reg-02-20-03. For coronavirus related attendance guidance, see the “coronavirus information” section.\nAttendance Policy. Students are expected to attend class regularly. Since we only meet once a week, each class is vitally important to keep on track during the semester.\nAbsences Policy. Students should notify the instructor that they will be unable to attend class and schedule a time to meet with the instructor to review materials they missed during class time.\nMakeup Work Policy. Makeup work will be allowed if the student notifies the instructor at least one day before an assignment is due to arrange for a deadline extension. Notification given on the due date will be considered on a case-by-case basis."
  },
  {
    "objectID": "course_syllabus.Rmd.html#academic-integrity",
    "href": "course_syllabus.Rmd.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nStudents are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01.\nStudents are expected to produce work that they personally completed and in accordance with the instructions given for the assignment, project, test or quiz. Work copied from other students, completed by another individual, or directly taken from other sources without appropriate attribution will be in violation of academic integrity. Students are expected to follow the Code of Student Conduct (NCSU POL11.35.01) and Pack Pledge, and violations of academic integrity will be handled in accordance with the Student Discipline Procedures (NCSU REG 11.35.02).\nHonor Pledge. Your submission on any test or assignment indicates “I have neither given nor received unauthorized aid on this test or assignment.” Digital Course Components Students may be required to disclose personally identifiable information to other students in the course, via digital tools, such as email or web-postings, where relevant to the course. Examples include online discussions of class topics, and posting of student coursework. All students are expected to respect the privacy of each other by not sharing or using such information outside the course."
  },
  {
    "objectID": "course_syllabus.Rmd.html#digital-course-components",
    "href": "course_syllabus.Rmd.html#digital-course-components",
    "title": "Syllabus",
    "section": "Digital Course Components",
    "text": "Digital Course Components\nOur primary form of communication outside of online Zoom meetings will be through Moodle. Students are expected to monitor Moodle for announcements, assignments, and other information during the semester. Assignments. Assignments and final projects may be given and submitted through Moodle."
  },
  {
    "objectID": "course_syllabus.Rmd.html#accommodations-for-disabilities",
    "href": "course_syllabus.Rmd.html#accommodations-for-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Disabilities",
    "text": "Accommodations for Disabilities\nReasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with the Disability Resource Office at Holmes Hall, Suite 304, Campus Box 7509, 919-515-7653. For more information on NC State’s policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation (REG02.20.01). Non-Discrimination Policy NC State provides equal opportunity and affirmative action efforts, and prohibits all forms of unlawful discrimination, harassment, and retaliation (“Prohibited Conduct”) that are based upon a person’s race, color, religion, sex (including pregnancy), national origin, age (40 or older), disability, gender identity, genetic information, sexual orientation, or veteran status (individually and collectively, “Protected Status”). Additional information as to each Protected Status is included in NCSU REG 04.25.02 (Discrimination, Harassment and Retaliation Complaint Procedure). NC State’s policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or https://oied.ncsu.edu/divweb/. Any person who feels that he or she has been the subject of pro hibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148."
  },
  {
    "objectID": "course_syllabus.Rmd.html#technology-requirements",
    "href": "course_syllabus.Rmd.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThis course may require particular technologies to complete coursework. Specifically, students will need R and R Studio. To avoid challenges that may arise due to different versions of packages, the instructor is providing access to code at the this link. Students will need to create accounts with their email address in order to gain access.\nIf you need access to additional technological support, please contact the Libraries’ Technology Lending Service: https://www.lib.ncsu.edu/devices."
  },
  {
    "objectID": "course_syllabus.Rmd.html#resources-1",
    "href": "course_syllabus.Rmd.html#resources-1",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThese are difficult times, and academic and personal stress is a natural result. Everyone is encouraged to take care of themselves and their peers. If you need additional support, there are many resources on campus to help you: \n• Counseling Center https://counseling.dasa.ncsu.edu/ \n• Health Center https://healthypack.dasa.ncsu.edu/ \n• If you or someone you know are experiencing food, housing or financial insecurity, please see the Pack Essentials Program https://dasa.ncsu.edu/pack-essentials/ \nSupporting Fellow Students in Distress. As members of the NC State Wolfpack community, we each share a personal responsibility to express concern for one another and to ensure that this classroom and the campus as a whole remains a healthy and safe environment for learning. Occasionally, you may come across a fellow classmate whose personal behavior concerns or worries you, either for the classmate’s well-being or yours. When this is the case, I would encourage you to report this behavior to the NC State’s Students of Concern website: https://prevention.dasa.ncsu.edu/nc-state-cares. Although you can report anonymously, it is preferred that you share your contact information so they can follow-up with you personally. I may also make a referral to the CARES program if I notice you are having a difficult time. If you don’t know who to talk to or which steps to take, please don’t hesitate to come talk to me and I will help you the best I can. \nStudents are responsible for reviewing the NC State University Policies, Rules, and Regulations (PRRs) which pertain to their course rights and responsibilities, including those referenced both below and above in this syllabus:\nEqual Opportunity and Non-Discrimination Policy Statement: https://policies.ncsu.edu/policy/pol-04-25-05 with additional references at https://oied.ncsu.edu/divweb/policies/\nCode of Student Conduct: https://policies.ncsu.edu/policy/pol-11-35-01."
  },
  {
    "objectID": "poc_compare.html#footnotes",
    "href": "poc_compare.html#footnotes",
    "title": "Adding and comparing learners",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDemographic information like race is excluded for ethics reasons. This decision will be discussed more in-depth when we discuss algorithmic bias.↩︎"
  },
  {
    "objectID": "data2_overview.html",
    "href": "data2_overview.html",
    "title": "Data preparation",
    "section": "",
    "text": "This section covers best practices and considerations for preparing data for predictive analytics. The data we use for predictive analytics can often be messy...\n\nThere may be missing data values that can create confusion and errors if not handled thoughtfully;\nThere may be mistakes or nonsensical values that create noise and obfuscate underlying patterns;\nThere may be valuable information to be extracted from raw measures that may not provide clear empirical value (e.g. text data); and\nThere may be differences in data variables over time, which can threaten the performance of our models.\n\nNote that for predictive analytics, data preparation may be an iterative process. In data with a large number of predictors, it may make sense to focus first on a core set of variables that are hypothesized to be strong predictors - those specified in a benchmark predictor set or incrementally expanded predictor sets. If validation of learners with these predictor sets indicate strong performance, then the data science team may move on to model selection and testing. However, if learner performance in the validation step is lower than desired or anticipated, then the data science team can circle back and work to extract more predictive value from the raw data.\n\n\n\n Back to top"
  },
  {
    "objectID": "data2_whymissing.html",
    "href": "data2_whymissing.html",
    "title": "Overview of missing data",
    "section": "",
    "text": "It is rare for data to be complete for all observations and all variables. Missing data is a challenge for just about any data analysis. In R, missing data often looks like what we see in the following example. Here, missing information is indicated by NA. Missing information may be indicated in other ways too (e.g. a specific numeric value outside a variable’s range, or ” “). These cases would need to be translated to R’s convention of using NA during data cleaning.\nIn the toy data set below, we see a missing value for the 3rd student’s math score and corresponding proficiency level. We also see a missing value for whether the 5th student graduated on time.\n\n\n  ID School MathScore MathProficiency GradOnTime\n1  1      A        40               1          1\n2  2      A        60               2          0\n3  3      B        NA              NA          1\n4  4      B        80               3          0\n5  5      B        60               2         NA\n\n\nAcross many types of data, information may be missing for a variety reasons, including:\n\nNon-Response: When individuals respond to a questionnaire, fill out an application, etc., some may choose not to answer certain questions due to the sensitivity of the topic, lack of knowledge, or other reasons. This can lead to missing values.\nData Entry Errors: Mistakes made during data entry can result in missing values. Data entry errors - due to typos, misinterpretation of handwriting, or glitches - can also lead to nonsensical information, which may need to be converted to missing.\nTechnical Issues: Technical problems such as software crashes, network failures, or sensor malfunctions can lead to missing data points, particularly in real-time data collection scenarios.\nPrivacy and Confidentiality Concerns: In some cases, data might be intentionally omitted or masked to protect individuals’ privacy or sensitive information. For instance, certain personal identifiers might be removed from the dataset.\nSkip Patterns: In surveys or questionnaires, skip patterns are used where respondents are directed to skip certain questions based on their previous responses. This can lead to missing values for respondents who don’t meet the criteria for certain questions.\nData Collection Costs: Collecting certain types of data can be expensive and time-consuming. As a result, researchers might prioritize certain data points over others, leading to missing values in less prioritized areas.\nData Migration or Integration: When merging data from different sources or migrating data to a new system, compatibility issues might result in missing values or discrepancies.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data2_handlemissing.html",
    "href": "data2_handlemissing.html",
    "title": "Handling missing data - predictors",
    "section": "",
    "text": "Here we focus on how to handle missingness in variables that are potential predictors for our learners. Some implementations of machine learning algorithms have a built-in way to handle missing data. Others either will not run if the model is fed in data containing missingness or will drop observations with any missing values. We will address missing values before using any modeling functions in R so that the approach will not vary across modeling approaches. Addressing missing data consistently allows us to make a fair comparison of differnt models’ performances.\nThere are many different methods for managing missing data. They typically involve “filling in” - or “imputing” - missing data points with plausible values. Imputation can be a valuable approach to consider, and it is discussed below. But imputation should not be the primary strategy. Instead, we should recognize that missingness may be informative. Therefore, it is valuable to capture the missingness in our modeling.\n\nCapturing information in missingness\nThe idea here is pretty simple. Data may be missing for a reason - or for multiple reasons, which may be related to the outcome we are trying to predict, even if in a small way.\nFor example, when predicting whether a student will graduate on time, their standardized math score and the resulting proficiency level are potential predictors. Here the math score is on a scale from 0 to 100, and a proficiency value of 1 indicates the student scored “below proficient”; a value of 2 indicates the student scored as “proficient;” and a value of 3 indicates the student scored “above proficient.” Students who have missing values for these variables may not have taken the test. Perhaps they were absent on the day of the test and the day of the make-up. It is possible some of these students tend to be absent more often than the average student. Therefore, missingness on a math test may tell us something about a student’s level of absenteeism. Or, perhaps some students are exempt from the test because they are on a different math track.\nWe have no way of knowing the reasons students are missing data for variables with information about the math standardized test. But…\n\nIt is quite possible that those with missing data are different than those entered data; therefore imputation based on entered data would likely be biased. And…\nStudents with missing data may be less or more likely to graduate on time. Therefore, we want to allow for missingness to be predictive of the outcome of interest.\n\nTo capture the missingness information in our modeling, we can take the following approaches\n\nFor categorical variables, we add a category for the missingness.\n\nNote if our raw variable (before accounting for missingness) is an “ordered” categorical variable, for which the categories have a clear and meaningful order or scale, then adding a category for missingness means that our newly formatted variable is an “unordered” categorical variable. In R, a factor (a class of an R object or of a column in a data frame) is used to represent categorical variables. (By default a factor treats values as unordered categorical variables, although factors can also be ordered.)\nEquivalently, we can create a set of dummies (binary variables with values 0 or 1 indicating whether an observation belongs to each category or not). Note, when we create a set of dummies, we leave one category out, as a reference category. For some modeling approaches, including all possible categories can create unstable estimation. So for example, we might create a set of three dummies indicating (1) proficient, (2) above proficient, and (3) missing proficiency information (leaving the 4th category of below proficiency out).\n\n\nBelow, we see how this looks in a revised R data frame:\n\n\n  ID School MathScore Math.Proficient Math.AboveProficient Math.Missing\n1  1      A        40               0                    0            0\n2  2      A        60               1                    1            0\n3  3      B        NA               0                    0            1\n4  4      B        80               0                    1            0\n5  5      B        60               1                    0            0\n  GradOnTime\n1          1\n2          0\n3          1\n4          0\n5         NA\n\n\n-   If we have a small category, we will want to combine it with another category. For example, if there are just a few missing values, we might create a factor variable where one category is \"proficient\"; another is \"above proficient\"; and the other is \"either below proficient or missing proficiency information\" - or we create two dummies (leaving one category out).\n\n-   If missingness is a very large category, this may indicate a problem with the reliability of the variable. Variables with large amounts of missing data should perhaps be eliminated from analyses - but this is an issue to discuss with those who have expertise with the data systems and data entry processess. \n\nFor continuous variables, we can turn create a new, transformed categorical variable. That is, if our data only contained the math test score, to address missingness, we can turn it into a variable that captures different levels and missingness as above. Converting continuous variables to categorical variables may be desirable for extracting predictive information as well. For example, whether a student scored at a proficient level or not may be more predictive than their raw score.\nFor continuous variables, we can also do a combination of imputation and creating a dummy to capture missingness. This method, “dummy variable adjustment” (Cohen and Cohen, 1985) is illustrated below. Here we impute the missing math scores with the mean of the nonmissing math scores, and we create a dummy variable that is 1 for observations with inputations and 0 otherwise.\n\n\n  ID School MathScore Impute.MathScore Miss.MathScore GradOnTime\n1  1      A        40               40              0          1\n2  2      A        60               60              0          0\n3  3      B        NA               60              1          1\n4  4      B        80               80              0          0\n5  5      B        60               60              0         NA\n\n\nWhen we use this “dummy variable adjsutment,” we include both the new variable with imputed values and the new dummy in our modeling. Note that in a regression, the choice of the value used for imputation does not affect the coefficient of Miss.MathScore or Impute.MathScore. The coefficient for Impute.MathScore can be regarded as an estimate of the effect of the math score among the subgroup of those observations that have complete math score data. The only aspect of the model that depends on the choice of the imputation value is the coefficient on the missing dummies. When we impute with the mean (or the median or mode) of non-missing values, the coefficient of a missingness dummy can be interpreted as the predicted value of the outcome for individuals with missing data on X minus the predicted value of Y for individuals at the mean of X, controlling for other covariates in the model.\nThere has been a considerable amount of criticism focused on this approach in the literature. Jones (1996) and Allison (2002) show that, generally in studies using observational data, this approach leads to biased estimates of the coefficients in the regression model.9 Despite the uniform criticism of the method in the literature, however, we believe this approach still warrants consideration in the special case of random assignment evaluations.\n\nHere is some R code to illustrate. Here, we have a variable for math test level. Level 1 is not proficient, while Level 2 is proficient. Two of the 5 students do not have any informtion entered in the data.\n\n\n  math.test math.test.levels math.test.proficient math.test.missing\n1         1                1                    0                 0\n2         2                2                    1                 0\n3        NA                0                    0                 1\n4         1                1                    0                 0\n5         2                2                    1                 0\n\n\n\n\nImputation\nIf it is missing not at random (MNAR), we recommend using an imputation approach along with creating a missingness indicator to capture informative missingness. In the companion predictive analytics tool, we have made the default assumption of missing not at random.\nFor example, a simple version of imputation is to replace missing values of continuous variables with mean or median of the non-missing values, or to replace missing values of categorical variables with the mode. While this practice solves the operational difficulty of missing data, it can potentially create problems. On the last page, we reviewed a list of of reasons why data may be missing. Many of reasons suggest that units of observations (e.g. individuals) with missing data may be different from units without missing data. People who are resistant to reporting their wage may be more likely to have a low wage. Data collection at one site of a business may be more reliable at entering data than another site.\nTherefore, if we replace missing values with the mean of nonmissing values, this mean is not a good estimate because it was computed for a different population. When estimating statistical parameters, imputation can introduce bias - that is, it can lead to inflated or undervalued estimates. When fitting a predictive model, this practice can lead to diminished performance because the relationships between predictors (e.g. age and wage) and an outcome (e.g., job retention) can be faulty.\nSome analysts focus on doing a really good job at finding plausible values for their imputation. This is a very active area in statistical research. For example, the imputed values may be estimated by modeling - using information throughout the dataset to come up with more plausible values because they are similar to observations with a similar profile across numerous measures (variables). With an approach called “multiple imputation,” the modeling for the missing values is repeated multiple times, resulting in multiple complete data sets. This repeated modeling accounts for both sampling uncertainty as well as modeling (specification) uncertainty of imputed values. (E.g., Gelman and Hill (2007)). The multiple complete datasets can be combined in different ways or results from repeated analyses of the multiple datasets can be combined. Multiple imputation methods reduce but do not eliminate bias in filled-in values.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data2_missingpredictors.html",
    "href": "data2_missingpredictors.html",
    "title": "Handling missing data - predictors",
    "section": "",
    "text": "Here we focus on how to handle missingness in variables that are potential predictors of our outcome of interest. Some implementations of machine learning algorithms have a built-in way to handle missing data. Others either will not run if fed data with missing values or will drop observations with any missing values. We will address missing values before using any modeling functions in R so that the approach will not vary across modeling approaches. Addressing missing data consistently allows us to make a fair comparison of differnt models’ performances.\nThere are many different methods for managing missing data. They typically involve “filling in” - or “imputing” - missing data points with plausible values. Imputation can be a valuable approach to consider, and it is discussed below. But I would argue that imputation should not be the primary strategy for predictive analytics, at least in settings in which careful data preparation is being done.1 Instead, we should recognize that missingness may be informative. Therefore, it is valuable to capture the missingness in our modeling."
  },
  {
    "objectID": "data2_extract.html",
    "href": "data2_extract.html",
    "title": "Extracting predictive value",
    "section": "",
    "text": "Here we discuss data preparation steps focused on extracting helpful information from data in order to enhance the performance of our predictive models. This set of steps is often referred to as “feature engineering” (as predictors are often referred to “features.”) The data preparation process can be guided by EDA in some cases, but it also involves domain knowledge, intuition, and experimentation to represent data in a manner that makes it more suitable or informative - often leading to improved model accuracy and generalization. After addressing missingness according to the guidance on the previous pages, data preparation, or feature engineering, involves the following:"
  },
  {
    "objectID": "data2_datamodel.html",
    "href": "data2_datamodel.html",
    "title": "Data model for code templates",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data2_missingoutcomes.html",
    "href": "data2_missingoutcomes.html",
    "title": "Missing data - outcomes",
    "section": "",
    "text": "Outcome values could be missing for multiple reasons:\n\nThe outcome has not been sufficiently defined. For example, imagine the case in which a company wants to predict whether a customer signs up for an advertised service. The binary outcome for the outcome may be equal to 1 if a customer signs up, and 0 if they decline to sign up. But what about those who haven’t indicated “yes” (1) or “no” (0) yet. The may have a missing value. It is critical here to define a window of time for observing the outcome. That is, we need to define by when the sign-up occurs (e.g. within 1 month). Then the missing values would take the value of 0, indicating that “no” the customer did not sign up for the services within 1 month of advertising it.\nAll the same reasons predictors can be missing. If an outcome is missing due to “random” typos, technical glitches or messiness, then this is probably not a problem. The corresponding unit/observation would have to be excluded from modeling. However, if an outcome variable’s missingness is nonnegligible and due to nonrandom reasons (e.g. some individuals were hesitant to report the information for the outcome variable), then this could introduce a big problem for the analytics. If the outcome is observed for a subgroup that differs from the overall population in your data in some meaningful way, then your modeling will only apply to that subgroup. And, if you can only identify that subgroup by nonmissing outcome data, then you will be applying that model to make predictions for a broader population, introducing unreliable results when the model is deployed with data in which the outcomes are unknown.\nThe outcome cannot be observed for a subgroup. This another version of #2 but occurs somewhat differently. Consider the following example: Across the country, release and detention decisions for defendants in the pretrial period — that is, the period after arrest while a criminal case is being adjudicated — are increasingly guided by risk assessments, which rely on data to estimate defendants’ risk of failing to appear for a court date or of being charged with new criminal activity if released pending trial. In this setting, predictive models are generally fit to only those defendants not detained while awaiting trial. Those detained, potentially because they were unable to make bail, are not included as there is no outcome related to new criminal activity or failure to appear (this is referred to as “censorship”). This censorship can cause multiple problems. First, if the people who are detained are systematically different from the people who are not detained, the final models may not generalize: the models may not accurately predict risk for those people who were detained. Additionally, if detention patterns differ by racial group, bias may be introduced by fitting models using different subsets of the racial groups. One of the approaches to address the censoring problem when validating pretrial risk assessments is to impute missing outcomes for detained or partly detained defendants. This imputation could include race and other characteristics not used for the final risk model. (As imputation is only used to build the model, these defendant characteristics will not, in the end, be used as risk factors by the final predictive model and thus the overall risk assessment.) As long as the imputation captures any differential relationships between detention and subgroups, the subsequent model-fitting process will not be as vulnerable to biases from censoring.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "data2_missingpredictors.html#footnotes",
    "href": "data2_missingpredictors.html#footnotes",
    "title": "Handling missing data - predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn some settings, that rely on frequent streams of big data for example or that have very minimal missing values, data preparation may need to be fully automated and imputation may be preferred.↩︎"
  },
  {
    "objectID": "data2_extract.html#footnotes",
    "href": "data2_extract.html#footnotes",
    "title": "Extracting predictive value",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMean centering supports numeric stability for machine learning algorithms that involve matrix inversion or optimization by scaling down the numbers. Algorithms that rely on gradient descent can converge faster when data is centered and normalized because it shapes the cost function into a more bowl-like form, making it easier to reach the global minimum. Some algorithms assume that the data is centered around zero. For instance, LASSO and Ridge regression add penalty terms based on the magnitude of the coefficients. If the predictors are not centered, the penalty might be applied in a biased manner because it would be influenced by the scale and mean of the predictors. Also, many machine learning algorithms, such as Support Vector Machines, k-means clustering, and deep learning networks, work better or converge faster when data variables are standardized to z-scores. Mean centering and standardizing might not always be necessary or beneficial. It largely depends on the context, the nature of the data, and the specific algorithm being employed. For example, in tree-based algorithms like Decision Trees or Random Forests, mean centering usually doesn’t offer any advantage because these algorithms are scale-invariant.↩︎"
  },
  {
    "objectID": "data2_EDA.html",
    "href": "data2_EDA.html",
    "title": "Exploratory data analysies",
    "section": "",
    "text": "Exploratory data analysis (EDA) is valuable for gaining an understanding of your data before diving into predictive analytics. EDA is not technically a required step in a predictive analytics work flow. In some contexts, it may be possible to achieve excellent model performance with raw data measures and without understanding the underlying data very well. However, EDA is typically recommended and often, extremely valuable for…\nData Quality Assessment: EDA uncovers data quality issues like missing values, outliers, or implausible values, which might be the result of data entry errors. Addressing these quality concerns is imperative as they can drastically affect model accuracy. Plots of variables’ distributions and summary statistics (e.g., mean, median, standard deviation, range, quartiles, interquartile range) aid in this assessment.\nInformational Value of Variables: Variables with little variation often add negligible value to predictive models. For instance, a binary predictor that’s consistent across 99% of observations offers limited insight. Similarly, a continuous variable clustering around a singular value isn’t very informative. Again plots and statistics that summarize variables’ distributions are helpful.\nIdentification of Potential Predictors:Correlations between independent variables and the outcome of interest can hint at important predictors. However, keep in mind that variables not strongly correlated with the outcome can still be valuable in models, especially when they interact with other variables or aid in capturing complex, non-linear relationships. It is difficult or impossible to visualize interaction terms (i.e. when predictors have a different relationship with the outcome when considered together than when considered apart).\nRedundancy Detection: Tools like correlation matrices or heatmaps elucidate inter-variable correlations, highlighting redundancy. When faced with overlapping predictors, it’s beneficial to opt for the most explanatory variable, especially when aiming for model simplicity and interpretability.\nPotential Bias: EDA is instrumental in detecting biases. Visual tools, such as histograms, display group distributions. An underrepresentation of a specific group, compared to the target population, can introduce model bias. Furthermore, a disproportionate frequency of missing values or outliers for certain groups is another bias indicator. Moreover, It’s crucial to evaluate the relationship between multiple variables and protected attributes (e.g., race, gender) or other important equity attributes (e.g., income level, geographical location). Variables strongly correlated with these sensitive attributes can inadvertently become proxies, potentially leading to biased decisions. For instance, using a neighborhood-based variable that correlates closely with race might unintentionally result in decisions influenced by racial attributes. EDA is an important first step for being attentive to bias, but assessing bias of model results is also essential. We will turn to this soon.\n\n\n\n Back to top"
  },
  {
    "objectID": "data2_changesovertime.html",
    "href": "data2_changesovertime.html",
    "title": "Changes in data over time",
    "section": "",
    "text": "Keep in mind that data quality, missingness and distributions can change over time. If the changes are substantial, this can interfere with model performance over time. For example, if a predictor has more or less missingness in later years compared to missingness in earlier years, this could suggest that there some new dynamic at play - e.g., a change in how the variable is understood by those providing or entering data, a change in clients’ willingness to provide information, or a change in data integration processes. Therefore, the missingness has different meaning over time and consequently perhaps different predictive value. Exploration of trends over time could guide data scientists decisions about whether to exclude some variables from their predictor sets.\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_lasso.html",
    "href": "methods_lasso.html",
    "title": "Lasso",
    "section": "",
    "text": "Lasso, which stands for “Least Absolute Shrinkage and Selection Operator,” is a “penalized” form of regression. This means that it introduces a penalty to the regression model to shrink some of the regression coefficients towards zero.\nTo illustrate, imagine you’re assembling a team for a game and each player’s skill contributes to your team’s overall performance. However, for each player you add, you need to pay a cost (like a registration fee). The Lasso penalty is akin to this cost. If a player (or a predictor in our regression context) doesn’t contribute much value, you might opt to leave them out to avoid the fee. The stronger the penalty, the more selective you’d be about who you add to your team.\nIn the context of Lasso regression, this penalty pushes less important predictors’ coefficients towards zero, effectively excluding them from the model.\nIn Lasso regression, the primary tuning parameter is \\(\\lambda\\) (lambda), which controls the strength of the penalization.\n\nWhen \\(\\lambda=0\\), no penalty is applied, and the resulting model will include all predictors.\nAs \\(\\lambda\\) increases, the penalization effect becomes stronger, and more coefficients are shrunk towards zero. For a sufficiently large value of \\(\\lambda\\), all coefficients may become exactly zero.\nThe value of \\(\\lambda\\) can be selected with cross-validation with the training data.\n\nAdvantages of Lasso\n\nFeature selection: One of the key advantages of Lasso regression is its ability to perform feature or variable selection by shrinking the coefficients of the least important predictors to zero. This feature of the algorithm can be very helpful when you have a large number of predictors, and you suspect that many of them are irrelevant or redundant.\nDealing with lots or predictors: In situations where the number of predictors (\\(p\\)) is close to or exceeds the number of observations \\(n\\), classical logistic regression might overfit or might not even run at all. Lasso can be a solution in these high-dimensional settings.\nMulticollinearity: When predictor variables are highly correlated, standard logistic regression’s estimates can be very unstable. Lasso helps to alleviate “multicollinearity” (high correlation) issues by penalizing certain coefficients and pushing them towards zero.\nInterpretability: Because Lasso can zero out coefficients, the resulting model can be more interpretable than a model with many predictors. And Lasso is more interpretable than “black-box” machine learning algorithms.\nModel Performance: If there’s a concern about overfitting due to a large number of predictors, Lasso can provide a more generalized model that might perform better on out-of-sample data compared to a non-regularized logistic regression.\n\nDisdvantages of Lasso\n\nTrue model: If the true underlying model is very sparse (i.e., very few predictors truly matter), then lasso will perform very well. However, if a large number of predictors matter, lasso may not be stable.\nChallenged with highly correlated predictors: With highly correlated predictors, lasso arbitrarily chooses which to include in the model. This arbitrary selection may not matter for predictive performance but can interfere with explainability.\n\nImplementation of Lasso in R\nIn R, we implement lasso with glmnet() in the glmnet package. The code templates will do this for you.\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_rf.html",
    "href": "methods_rf.html",
    "title": "Decision trees and Random Forest",
    "section": "",
    "text": "Decision trees\nA decision tree is a simple machine learning algorithm consisting of a set of flowchart-like nodes representing decisions, which are based on a variable’s value, and branches representing the outcomes of those decisions, which lead to either a subsequent node/decision or a final prediction. We can think of a decision tree as a series of “questions” we ask the data. Based on the answers, the algorithm makes decisions or predictions.\nThe following illustrates a very simple decision tree for predicting whether bank customers will remain with their bank throughout the next year.\n\n\n\n\n\nThis tree illustrates, starting on the right side, that if a customer has a low balance and a loan with the bank, they’re likely considering leaving, possibly due to financial constraints. However, if they have a low balance but no loan, the deciding factor might be their interaction with customer service. Many calls could indicate dissatisfaction, making them likely to leave. On the left side of the tree, we see that customers with medium to high balances are further segmented based on account type. Those with basic accounts who have had many interactions with customer service are predicted to leave. However, those with premium accounts are predicted to stay, without it depending on customer service interactions.\nWith real data, the decisions made at the various nodes in a decision tree are based on a measure of “split quality.” The exact process depends on the algorithm used, but here’s a general outline of how decisions are made at the nodes using the popular CART (Classification And Regression Trees) approach:\nSelection of node variables: For each predictor, the algorithm evaluates its ability to distinguish between outcomes = 1 and outcomes = 0. This distinction is usually measured using impurity scores, such as Gini impurity or entropy. The predictor variable that provides the best separation is selected as the node variable.\nDetermining the splits: For each potential predictor variable, the algorithm assesses every possible threshold or category as a potential split. If it’s a categorical predictor (e.g., low balance: yes or no), it directly checks the separation created by this category. If it’s a continuous predictor (e.g., account balance amount), it might consider every unique value or use binning techniques to determine the best split point. The threshold or category that gives the most distinct separation (least impurity) is chosen as the split.\nIterative process: After choosing the best variable and its split, the data is divided accordingly, resulting in two child nodes. Then the process is recursively applied to each child node until a stopping criterion is reached - like a maximum tree depth or a minimum number of samples in a node.\nPruning (for some DT algorithms): After the tree is “fully grown,” it can be pruned to remove branches that have little importance or add little predictive power. This step is done to simplify the model and improve its performance on unseen data.\nIn essence, the decisions made at each node are data-driven, aiming to improve the prediction accuracy (or reduce error) of the tree. The tree-building process aims to find the splits that most effectively segregate the data in terms of the target outcome.\n\n\nRandom Forest\nA Random Forest considers many different possible decision trees. A bootstrap sample (a random sample of the units (rows), with replacement) is taken from the data, and a decision tree is constructed. This bootstrap process is repeated many times, resulting in many different possible decision trees.\nWhen growing each tree, instead of considering all features for splitting at a node, only a random subset of the predictors is considered. This procedure introduces variability and helps “de-correlate the trees,” making the model more robust.\nTo aggregate results across all the trees in the forest, the algorithm takes an “ensemble” approach. For regression (including for predicting probabilities of a binary outcome), the average prediction of all the individual trees is the final prediction. For classification, the class (i.e., yes or no, 0 or 1) that gets the most votes from all the individual trees is the final prediction (majority voting).\nThe combination of diverse trees reduces the variance (overfitting) without increasing the bias, leading to a more accurate and robust model compared to a single decision tree.\nTuning parameters for Random Forest: There are many tuning parameters for the Random Forest algorithm. While not a comprehensive list, some of the key tuning parameters include:\n\nNumber of trees (\\(B\\)): chosen to be sufficiently large so that results are stable. \\(B\\)=500 is the default in the code templates.\nNumber of predictors to consider at each node (\\(m\\)). One rule of thumb is \\(m=\\sqrt{p}\\) where \\(p\\) is the total number of predictors.\nMinimum node size. This is the minimum number of observations that can be in a terminal node. For the code templates the default is 10.\nMaximum tree depth. This limits tree complexity. The code templates do not constrain tree depth for random forests.\n\nAdvantages of Random Forest\n\nPerformance: Because Random Forest takes an ensemble approach - by aggregating across multiple trees (i.e., “bagging”) - the algorithm reduces variance and buffers against overfitting. Because a Random Forest algorithm tends to be much less prone to overfitting than individual decision trees, it is recommended over using individual decision trees.\nFlexibility: The Random Forest algorithm works well for classification and regression. It also works well with categorical and continuous predictors. Also, it does not require transforming predictor values (e.g. mean centering or scaling). Moreover, it does not require the data analyst to specify a functional form of the relationship between predictors and the outcome and therefore excels over regression methods that assume linear relationships.\nDetects interactions: The Random Forest algorithm inherently captures interactions between variables without the data scientist needing to explicitly specify them. Moreover, the Random Forest algorithm can capture not only 2-way interactions, but also 3-way or higher interactions, as well as non-linear interactions between variables, which might be missed or hard to model in traditional regression techniques.\nHandles large data sets: The algorithm can handle large datasets efficiently. The sampling technique ensures that it can scale well with the increase in data.\n\nDisadvantages of Random Forest\n\nComputationally Intensive: While Random Forest handdle large datasets, it can be very time-consuming. Computation time depends on the number of trees.\nInterpretability: Individual decision trees, with their hierarchical structure, can be visualized and interpreted easily. However, Random Forest, being an ensemble of multiple trees, lacks this level of interpretability. “Variable importance” methods, which we will turn to later, can give some insights, however.\nTuning Required: While the Random Forest algorithm can perform reasonably well with default parameters, to get the most out of the algorithm, hyperparameter tuning is often required, which can be time-consuming.\n\nImplementing Random Forest in R\nImplementation of the Random Forest algorithm, which was developed by Leo Breiman and Adele Cutler, is done with randomForest package.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_logistic.html",
    "href": "methods_logistic.html",
    "title": "Logistic regression",
    "section": "",
    "text": "For some starting context: With multiple linear regression, we predict a continuous outcome based on multiple predictors. The model for multiple linear regression looks like this:\n\\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\epsilon\\)\nwhere\nThe coefficients \\(\\beta_0\\), \\(\\beta_1\\),…,\\(\\beta_k\\) tell us the strength of the relationship between the predictors and the outcome.1\nLogistic regression can be used to predict the probability of a particular category of a binary outcome.\nIf we use a linear regression model to predict probabilities, we can get predicted values that are outside the [0,1] range, which doesn’t make sense for probabilities. To ensure predictions stay in the [0,1] range, logistic regression uses the logistic function:\n\\(P(Y=1) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k}}\\)\nwhere\nThis equation can also be written as follows:\n\\(\\log\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k\\)\nHere, the log odds (or the logarithm of the odds) of the outcome being 1 (versus 0) is expressed as a linear combination of predictor variables. As we plug in values for the predictors, the linear combination gives us the log odds, which can then be transformed to get the actual probabilities (\\(P(Y=1)\\) ).2\nUnlike many machine learning algorithms, when a data scientist uses logistic regression, they are specifying the specific functional form of the relationship between the predictors and the outcome. Specifically, with logistic regression, we are specifying that the log odds is a linear function of the predictors.\nAdvantages of Logistic Regression\nDisdvantages of Logistic Regression\nImplementation of Logistic Regression in R\nIn R, we implement logistic regression with glm() in Base R, setting family=binomial(link=\"logit\"). The code templates will do this for you."
  },
  {
    "objectID": "toolset_overview.html",
    "href": "toolset_overview.html",
    "title": "Overview of PA templates",
    "section": "",
    "text": "This section provides an orientation to the code templates for implementing the PA proof-of-concept framework. These templates were developed by colleagues at MDRC and edited by me. Here are some notes about these templates:\n\nThey can be accessed at: https://patools.sprouthub.io. You will log in with the provided username and password. This will take you to your personal work space in R Studio, where you can upload data, edit R notebooks, run analyses and download results.\nThe templates are organized into three primary R notebooks:\n\n01_Learner_Specification.Rmd\n02_Learner_Training_and_Validation.Rmd\n03_Learner_Testing.Rmd\n\n\nIf you are new to the concept of R notebooks, these are dynamic documents with a .Rmd extension that offer a seamless way to blend both analytics and narrative. R notebooks contain:\n\nText Content:\n\nThe text elements are written using “markdown language,” a lightweight markup language with plain-text-formatting syntax. It makes the document easily readable in almost any text editor. However, if you’re not well-acquainted with markdown, R Studio’s “visual” mode can come in handy. This interface simplifies the editing experience, rendering a user-friendly environment akin to conventional word processors, minimizing the need to juggle through markdown syntaxes.\n\nCode Chunks:\n\nR notebooks are compartmentalized into distinct sections called “chunks,” each capable of executing R code independently. This modularity facilitates iterative development and debugging, as you can focus on and run one chunk at a time. When you’re ready to compile the entire notebook, the “knit” function transforms the .Rmd file into a cohesive document in formats like PDF, HTML, or DOCX. Our templates are preset to render PDFs for uniformity and ease of sharing.\nOne of the advantageous features of R notebooks is the flexibility in displaying code. You can choose to showcase the code alongside its output, hide the code to present only the results, or even conceal both to keep the report neat, depending on your audience’s preference and the context of the presentation.\n\n\nIn essence, R notebooks epitomize the integration of data analysis and reporting. You can write, execute, and test R code in isolated chunks, ensuring each segment is accurate and efficient before proceeding. Concurrently, the narrative, including explanations, interpretations, and visualizations, can be weaved in between these chunks. This interleaving of code and commentary fosters a comprehensive, reproducible, and easily interpretable analysis, culminating in a well-organized, professional report.\nThe R notebook templates listed above are designed to be “user-friendly” in that they do not include lengthy chunks of complicated code. Rather, they call other R scripts that contain multiple functions and work in the background.\n\n\n\n Back to top"
  },
  {
    "objectID": "toolset_datamodel.html",
    "href": "toolset_datamodel.html",
    "title": "Data model for the code templates",
    "section": "",
    "text": "Data model for training, validation and testing data\nThe code templates rely on data sets that follow a particular “data model,” or format. The following provides some required specifications for the data model:\n\nEach row corresponds to the unit of analysis. For example:\n\nIf you are predicting students’ probabilities of graduating on time, each row corresponds to a unique student.\nIf you are predicting a community’s probability of being impacted by devistating flooding, each row corresponds to a community.\nIf you are predicting a customer’s probability of signing up for an offer, then each row corresponds to a customer by promotion (i.e. if the customer is marketed multiple promotions, then they may have a row for each promotion). In this case, the unit is not customer, it is “customer/promotion.”\n\nEach unit of analysis appears in only one row. There should be no repeated rows for the same unit. That is, if there are variables with repeated measures over time, the data should be arranged in “wide format” rather than “long format.” For example, imagine your unit of analysis is indiviual, but your data has multiple records for each in, each corresponding to a job - with variables for wage and length of time at the job. In this case, the data need to be rearranged so that each record has variables such as “wage1,” “wage2,” etc. and “lengthjob1,” “lengthjob2,” etc.\nThe columns should contain:\n\nA unique identification number or code (ID). This can be of classnumeric or character.\nOutcome(s) of interest. This should be a binary variable with class factor. The variable should take values of 0 or 1. There should be no missing values.\nPre-processed potential predictors. These should be either numeric or factor variables. There should be no missing values. That is, this data set does not include raw variables with missingness but rather new versions of the variables that address missingness according to the guidance [here.](data2_missingpredictors.qmd)\nVariables to be used for assessing equity. For example, you would include variables such as race, gender, or any other variables for which you want to compare model performance or estimate measures of bias. These measures should all be categorical and have class factor. Missingness of these variables should be handled with the same approach as used for predictors.\nVariables needed for stratification. For example, if cross-validation will be stratified by a location variable, make sure that variable is included. It should be categorical and have class factor. Missingness should be handled with the same approach as used for predictors.\n\nAcross the different data sets used for training, validating and testing, the columns should be identical.\n\nHere is a mini example of a training data file that aligns with the data model:\n\n\n  ID SchoolA SchoolB GradOnTime Math.Proficient Math.AboveProficient\n1  1       1       0          1               0                    0\n2  2       1       0          0               1                    1\n3  3       0       1          1               0                    0\n4  4       0       1          0               0                    1\n5  5       0       1          1               1                    0\n  Math.Missing Chronic.Absent Race\n1            0              0    1\n2            0              0    2\n3            1              0    3\n4            0              0    2\n5            0              1    1\n\n\n\n\nData model for meta data\nHaving a meta data file is also recommended. A meta data file is a machine readable codebook, which summarizes your data. A meta data file for the training/validation data is valuable for (1) documenting and describing the data being fed into predictive analytics steps and (2) in some cases, selecting variables based on information in the meta data rather than having to type them all in. A meta data file for the testing data is valuable for comparing the variable distributions to those in the training data. Here are specifications for the meta data files:\n\nEach row corresponds to variable.\nColumns may include but are not limited to the following:\n\nVariable type. E.g., “ID”, “outcome”, “predictor”, “protectedAttribute”, etc.\nSummary statistics. E.g., min, max, mean, median, percentiles (e.g. 5, 25, 75, 95)\nData source. If your data results from integrating multiple data sources, you might want to include a column that indicates the source of each variable.\nTime point. If variables in your data are entered or integrated into your data at different time points, you can indicate that here. This provides a nice check to make sure variables are available for your prediction time point. If you are repeating predictive analytics at multiple time points, this will column may be essential.\nLabels. You may want to include a column that provides a short description of each variable.\nOther information. You may want to include other information that is helpful to document or that will help you specify predictor sets. For example, perhaps your data includes a set of variables that all come from three different various assessments. Perhaps you want to add variables from one of the assessments to a predictor set and then add variables from the other two assessments to a larger predictor set to see the predictive value they add. Rather than typing the assessment variables into your R notebook, you could use the meta data file to grab all the variables that correspond to each kind of assessment.\n\n\nHere is a mini example of a meta data file that aligns with the data model. I have included just a few example summary statistics to keep the example short.\n\n\n               varName      varType min max median mean  source timepoint\n1                   ID           ID   1   5      3  0.4 studRec      &lt;NA&gt;\n2              SchoolA    predictor   0   1      0  0.4 studRec     S1end\n3              SchoolB    predictor   0   1      1  0.6 studRec     S1end\n4           GradOnTime      outcome   0   1      1  0.6 studRec     S1end\n5      Math.Proficient    predictor   0   1      0  0.4 studRec     S1end\n6 Math.AboveProficient    predictor   0   1      0  0.4 studRec     S1end\n7         Math.Missing    predictor   0   1      0  0.2 studRec     S1end\n8       Chronic.Absent    predictor   0   1      0  0.2 studRec     S2end\n9                 Race equityAttrib   1   3      2   NA studRec   S1start\n                                             label\n1                                               ID\n2                                 Attends School A\n3                                 Attends School B\n4                                Graduated on time\n5 Proficient but not above on fall state math test\n6               Proficient on fall state math test\n7            Missing score on fall state math test\n8           Whether chronically absent across year\n9            Self-reported race on enrollment form\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_naivebayes.html",
    "href": "methods_naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is based on Bayes theorem from probability theory. It’s called “naive” because it makes a naive assumption that each predictor in your data is independent of the others, given the outcome. While this assumption is typically false, in practice, it still allows the algorithm to work fairly well.\nThe Naive Bayes algorithm tries to determine the likelihood of the predictors given both classes of the binary outcome (yes and no). It then uses these likelihoods and some prior knowledge about the general frequency of the outcome (an initial estimate based on the data, before considering the predictors) to make a prediction.\nSo first, the algorithm checks the frequency of your predictors given the outcome. For instance, when predicting on-time high school graduation, it may ask: How many students were chronically absent among students who graduated on time? And: How many students were chronically absent among students who did not graduate on time?1\nFor continuous predictors (e.g. attendace rate), one approach is to “bin” or discretize the variable’s values into categories, but this approach can be limiting. Therefore, the Naive Bayes algorithm relies on “kernels,” which provide a way to estimate likelihoods without discretizing the data.2\nThe next step is based on Bayes Theorem, which is:\n\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(Y=y)P(X=x|Y=y)}{P(X=x)}\n\\end{equation}\\]\nTherefore, Naive Bayes algorithm combines the prior with the likelihoods to produce a “posterior” probability for both outcomes: the probability of yes and no given the predictors in your predictor set. The outcome with the higher posterior probability is the algorithm’s prediction if classification is the goal. Otherwise, the computed posterior probability can be directly used as the predicted probability of the outcome.\nAdvantages of Naive Bayes\nAdvantages of Naive Bayes\nImplementing of Naive Bayes in R\nThe Naive Bayes algorithm can be implemented using the naiveBayes() function in the e1071 package."
  },
  {
    "objectID": "methods_xgboost.html",
    "href": "methods_xgboost.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "Gradient boosting algorithms, like Random Forest, are built on decision trees. However, gradient boosting takes a different approach for constructing trees than the Random Forest algorithm.\nThe basic idea behind gradient boosting is to build trees sequentially rather than independently. Basically, each tree is grown to correct the errors of its predecessor. First, a simple model is used to predict the target variable. The residuals (differences between the predicted values and the true values) are then computed. For binary outcomes, we actually have “pseudo-residuals”, which are the differences between the observed outcome and the predicted probability of the positive class (i.e. the predicted probability that the outcome = 1). The next tree tries to predict the error made by the previous model. The predictions from this new tree are scaled by a factor (learning rate) and added to the existing model’s predictions. This process is like taking a step in the direction that minimizes prediction error, hence “gradient” boosting.\nThese steps of are repeated multiple times. Each new tree is fit to the residuals of the current combined ensemble of previous trees. As trees are added, the model becomes a weighted sum of all the trees. To prevent overfitting, gradient boosting introduces “regularization.” As we saw in Lasso, regularization is a technique used to add some form of penalty to the model, which discourages it from fitting too closely to the noise in the training data (overfitting). One common form of regularization is “shrinkage”, where each tree’s contribution is reduced by multiplying it with a small learning rate. Gradient boosting requires careful tuning of parameters such as tree depth, learning rate, and the number of trees.\nXGBoost (Extreme Gradient Boosting):\nThe code templates you will use, use a particular gradient boosting algorithm called XGBoost. Here are its distinctive features:\n\nRegularization: Unlike the basic gradient boosting, XGBoost includes L1 and L2 regularization terms in its cost function to buffer against overfitting. L1 regularization adds a penalty proportional to the absolute value (magnitude) of the model coefficients. L2 regularization adds a penalty proportional to the square of the model coefficients.\nEfficiency: XGBoost is designed to be highly efficient. It can utilize the power of parallel processing to build trees, making it faster than many other implementations of gradient boosting.\nEarly stopping: Instead of growing a tree to its maximum depth and then pruning, XGBoost uses “max_depth” to grow the tree and stops splitting when it no longer adds significant value. XGBoost can also halt the training process if the model’s performance on a validation set doesn’t improve after a set number of rounds, preventing potential overfitting.\n\nAdvantages of XGBoost\n\nSpeed: The XGBoost algorithm is optimized to be relatively fast for a data-driven algorithm.\n\nDisadvantages of XGBoost\n\nTuning parameter sensitivity: XGBoost typically requires careful tuning of its tuning parameters to achieve the best results.\nMemory consumption: XGBoost can be memory-intensive, especially when handling large datasets.\nHandling categorical features: While the Random Forest algorithm can directly handle categorical variables, XGBoost requires them to be transformed into a numerical format - creating multiple dummies, as discussed in “Data for PA: Part 2,” (which is sometimes called “one-hot coding”).\n\nImplementation of XGBoost in R\nThis method is implemented with the [XGBoost]{https://cran.r-project.org/web/packages/xgboost/xgboost.pdf} package in R. The code templates will do this for you.\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_temp.html",
    "href": "methods_temp.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Support Vector Machines (SVMs) are a set of machine learning methods that are primarily used for classification, though they can also be used for regression. SVM tries to find the optimal decision boundary (or “hyperplane”) that best divides a dataset into classes.\n“Support Vectors” refers to the data points that lie closest to the decision boundary and are the most difficult to classify. They essentially define the position of the decision boundary. In fact, even if you were to remove all the other data points and only keep the support vectors, the position of the optimal hyperplane would not change.\nSVM algorithms aims to maximize the margin around the decision boundary. This margin is defined as the distance between the decision boundary and the nearest support vector from either class. A larger margin implies better generalization ability and a lower chance of overfitting.\nDecision boundaries may be linear (separated by a straight line in 2D, a plan in 3D or a “hyperplan” in higher dimensions) or nonlinear, which gets even more complicated. When decision boundaries are nonlinear, the algorithms map the data into a higher-dimensional space (the space with polynomial and interaction terms), where it becomes linearly separable. This is achieved through the use of “kernels.” Kernels are functions that transform the data into the required form. There are various types of kernel functions such as linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel is important and can influence the performance of the SVM. The PA code templates support two nonlinear kernels: polynomial and RBF.\nTuning of SVM involves a regularization parameter, which determines the trade-off between maximizing the margin (the width of decision boundary) and minimizing the classification error. A smaller value regularization value creates a wider margin, which might mis-classify more training data points, while larger value aims for a tighter fit to the training data. Tuning also includes degree of the polynomial terms (quadratic, cubic, etc).\nTraditionally, SVM is designed for classification, but SVM can be used to obtain predicted probabilities for binary outcomes. SVM returns “decision values,” which are distances from the separating hyperplane. These can be translated into probabilities, by fitting a logistic to the decision values.\nAdvantages of SVM\nEffective with a large number of predictors: SVM tends to perform well in high-dimensional spaces.\nEfficient: SVM tends to be memory efficient and fast in smaller data sets.\nFlexible: With the option for different kernel functions, SVM is flexible to different functional forms and can incorporate interaction effects.\nDisdvantages of SVM\nSensitive: SVM can be sensitive to choices of kernels and to values of tuning parameters.\nNeeds extra step for predicted probabilities: SVM does not directly provide probability estimates for classification, although R implementations will do this step for you.\nComputationally intensive with large data sets: With large data files, SVM can take a lot of time to run.\nLacking transparency: SVM can be hard to interpret and explain, especially when using nonlinear kernels. The transformation carried out by these kernels doesn’t have an intuitive meaning in the original feature space.\nImplementing of SVM in R\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_guidance.html",
    "href": "methods_guidance.html",
    "title": "Guidance for using ML",
    "section": "",
    "text": "Now that we have a basic understanding of some prevalent machine learning algorithms, you are probably wondering how to decide which ones to use when. Here is an attempt to provide some guidance:1\nNote that standardizing/normalizing will never hurt model performance compared to not doing so, but it may make model interpretation more challenging."
  },
  {
    "objectID": "methods_svm.html",
    "href": "methods_svm.html",
    "title": "Support vector machines",
    "section": "",
    "text": "Support Vector Machines (SVMs) are a set of machine learning methods that are primarily used for classification, though they can also be used for regression.\nSVM tries to find the optimal decision boundary (or “hyperplane”) that best divides a dataset into classes. “Support Vectors” refers to the data points that lie closest to the decision boundary and are the most difficult to classify. They essentially define the position of the decision boundary. In fact, even if you were to remove all the other data points and only keep the support vectors, the position of the optimal hyperplane would not change.\nSVM algorithms aims to maximize the margin around the decision boundary. This margin is defined as the distance between the decision boundary and the nearest support vector from either class. A larger margin implies better generalization ability and a lower chance of overfitting.\nDecision boundaries may be linear (separated by a straight line in 2D, a plan in 3D or a “hyperplane” in higher dimensions) or nonlinear, which gets even more complicated. When decision boundaries are nonlinear, the algorithms map the data into a higher-dimensional space (the space with polynomial and interaction terms), where it becomes linearly separable. This step is achieved through the use of “kernels.” Kernels are functions that transform the data into the required form. There are various types of kernel functions such as linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel is important and can influence the performance of the SVM. The PA code templates support two nonlinear kernels: polynomial and RBF.\nTuning of SVM involves a regularization parameter, which determines the trade-off between maximizing the margin (the width of decision boundary) and minimizing the classification error. A smaller regularization value creates a wider margin, which might misclassify more training data points (but might generalize better to unseen data), while larger value aims for a tighter fit to the training data. Tuning parameters also include degree of the polynomial terms (quadratic, cubic, etc).\nTraditionally, SVM is designed for classification, but SVM can be used to obtain predicted probabilities for binary outcomes. SVM returns “decision values,” which are distances from the separating hyperplane. These can be translated into probabilities, by fitting a logistic to the decision values.\nAdvantages of SVM\n\nEffective with a large number of predictors: SVM tends to perform well in high-dimensional spaces.\nEfficient: SVM tends to be memory efficient and fast in smaller data sets.\nFlexible: With the option for different kernel functions, SVM is flexible to different functional forms and can incorporate interaction effects.\n\nDisdvantages of SVM\n\nSensitive: SVM can be sensitive to choices of kernels and to values of tuning parameters.\nNeeds extra step for predicted probabilities: SVM does not directly provide probability estimates for classification, although R implementations will do this step for you.\nComputationally intensive with large data sets: With large data files, SVM can take a lot of time to run.\nLacking transparency: SVM can be hard to interpret and explain, especially when using nonlinear kernels. The transformation carried out by these kernels doesn’t have an intuitive meaning in the original feature space.\n\nImplementing of SVM in R\nSVM can be implemented with the svm() function in the e1071 package.\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_overview.html",
    "href": "methods_overview.html",
    "title": "Overview",
    "section": "",
    "text": "In this section, we focus on modeling methods. We begin with a short discussion about how logistic regression can be used to predict binary outcomes. Then, we take a quick tour through some prevalent machine learning (ML) algorithms.\n\n\n\n\n\n\nDefining machine learning\nMachine learning refers to models that use iterative algorithms — that is, a series of steps that continuously adjust for better predictive performance — rather than relying on functional forms (specifications of the independent variables and the types of relationships between the variables and the outcome) specified by an analyst.\nNote that machine learning is divided into two primary categories. These categories are distinguished by the type of data they handle and their objectives.\n\nSupervised Learning: Here, algorithms are trained using labeled data. Labeled data means every unit’s set of data points includes a corresponding outcome. The primary objective is to train a model that will then make informed predictions on previously unseen data. This is our task in this class. With supervised learning, there are two different tasks:\n\nClassification: This approach is employed when the outcome is categorical. The prediction might be a ‘yes’ or ‘no’, ‘0’ or ‘1’. Though we will focus on binary classification, classification can also include multi-category outcomes, such as predicting what product a user will buy next or what movie a user might like.\nRegression: This approach is used for continuous outcomes (not the focus of this course). However, for binary outcomes, which is our primary concern, regression can estimate the probability of one outcome over the other (e.g., the probability of the outcome being ‘1’).\n\nUnsupervised learning: Here, algorithms work with unlabeled data (data without an output or outcome), aiming to find hidden structures or relationships within the data. A classic application is clustering, where data points are grouped based on inherent similarities, like customer segmentation. Unsupervised techniques can complement supervised ones, for instance, in dimensionality reduction by clustering raw predictors. However, topics related to unsupervised learning are beyond the scope of this course.\n\nTo understand the difference between supervised and unsupervised learning, consider an example of classifying images that contain either cats or dogs. In supervised learning, the algorithm is given the series of images and whether the image contains a cat or a dog. A model is trained to predict whether future images contain a cat or dog. In unsupervised learning, the algorithm is given the series of images, but is given no information about what is contained in the picture. The algorithm clusters the images in two groups, with the goal of producing one cluster of cats and one cluster of dogs, but the two groups are not characterized or labelled.\n\n\nUnderstanding machine learning algorithms\nAs you will read in the pages that follow, machine learning algorithms can vary substantially in their underlying approaches. At the same time, some have very similar underlying approaches but vary in their details. Machine learning algorithms also vary in how easy they are to understand and explain - both in terms of their empirical approach and in terms of the models they produce. This lack of transparency is why many machine learning algorithms are described as “black box” modeling methods.\nThe pages that follow give a concise overviews of select ML algorithms designed for predictive tasks (supervised learning). I don’t go into a lot of details, but instead try to provide the basic intuitions for how they work. If certain aspects seem complex, I’d advise concentrating on the listed advantages and disadvantages for each method.\nNote that the code templates implement all of the machine learning algorithms for you. This means you won’t need to grapple with specific R packages or functions that have been developed for specific algorithms. However, at the end of each algorithm’s summary, I provide a corresponding R package and function, in case you are curious about diving deeper. All of the machine learning algorithms I mention can also be implemented with tidymodels, and there are many tutorials available online.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "methods_logistic.html#footnotes",
    "href": "methods_logistic.html#footnotes",
    "title": "Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo estimate the coefficient values, we minimize the sum of squared errors across all observations.↩︎\nWith logistic regression, we use maximum likelihood estimation to find the best-fitting coefficients.↩︎"
  },
  {
    "objectID": "methods_naivebayes.html#footnotes",
    "href": "methods_naivebayes.html#footnotes",
    "title": "Naive Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote than when certain categories have zero counts, a Laplace correction is used. It involves adding 1 to both the numerator and the denominator to avoid ending up with unrealistic zero conditional probabilities.↩︎\nEssentially, a kernel is a function that “smears” or “spreads out” the observed data points over the continuous space, allowing for a smooth estimate of the likelihood of observing a particular value. That is, instead of relying on a histogram-style blocky representation of a predictor’s distribution, the data is represented using a smooth curve. Commonly used kernels are the Gaussian kernel, Epanechnikov kernel, and the Rectangular kernel, among others.↩︎"
  },
  {
    "objectID": "methods_guidance.html#footnotes",
    "href": "methods_guidance.html#footnotes",
    "title": "Guidance for using ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA note about combining the goals of predictive analytics and variable importance: Variable importance refers to the objective of measuring the predictive value (i.e. importance) of a variable in predicting the outcome - that is, the strength of the relationship between predictors and the outcome. Variable importance involves many different considerations than predictive analytics. There are many ways to define and measure variable importance, and there are many different methods for estimating variable importance. Some methods are agnostic to the ML algorithm used for prediction. Others are directly related. We will spend a little time talking about variable importance towards the end of the course. For now, I want to point out that sometimes data scientists select a particular machine learning algorithm because it allows them to simultaneously satisfy both goals of predictive analytics and variable importance. More on this later.↩︎"
  },
  {
    "objectID": "methods_nn.html",
    "href": "methods_nn.html",
    "title": "Neural networks",
    "section": "",
    "text": "A neural network is a popular machine learning algorithm. The approach was motivated by theories about how the brain works. Neural networks are highly simplified models of the brain. Even though they are a simplification of how the brain works, they can still be quite complicated.\nImagine you’re trying to predict a student will graduate on time, using various predictors that include measures of attendance and academic performance. The neural network is built on multiple layers, with each layer having multiple nodes.\n\nInput Layer: This layer holds initial clues about the outcome. The nodes in this layer are the individual predictors.\nHidden Layers: Next, the neural network starts to process this information, combining the individual predictors in various ways to derive deeper insights. The nodes in this layer are functions of nodes in previous layers.\nOutput Layer: The output layer produces the final answer based on all the clues (nodes) and their processing. For binary outcomes like whether a student will graduate on time, the output layer often has a single node that outputs a value (typically between 0 and 1) representing the probability of the outcome being 1.\n\nAt each layer, the neural network makes adjustments to the importance of individual and combined predictors by weighting them. The weights are adjusted during training to minimize the difference between the network’s predictions and the actual outcomes.\nThe power of neural networks comes from their ability to identify and learn complex relationships and patterns in the data. Networks with more layers and nodes are more complex and are termed “deep neural networks,” leading to the field known as deep learning.\nKey tuning parameters for neural networks include the number of hidden layers, the number of nodes in each hidden layer, regularization techniques (if applied), and the number of times during the training step the algorithm iterates through the entire dataset (known as “epochs”) to refine the model parameters.\nAdvantages of neural networks\n\nGood for modeling complex relationships: Neural networks, especially deep ones, can capture complex non-linear relationships.\nCapture interactions: The method also excels at capturing interactions between features without manual creation of interaction terms.\nHigh performing in a wide variety of settings.: Neural networks, particularly deep learning models, have seen significant attention in recent years due to their performance in a wide range of complex tasks.\n\nDisadvantages of neural networks\n\nData hungry: Neural networks often require a large amount of data to generalize well. On smaller datasets, they can easily overfit. I’m not sure if they are a good fit for most applications you all are choosing for projects.\nComputationally intensive: Training can be slow, requiring specialized hardware like GPUs for faster computation.\nNot transparent: The method is hard to explain and the resulting model is hard to decipher.\nSensitive to hyperparameter turning: The values of tuning parameters can greatly affect performance and require tuning.\n\nImplementing neural networks in R\nYou can use the neuralnet package to run neural networks in R. The code templates also allow for neural networks.\n\n\n\n Back to top"
  },
  {
    "objectID": "toolset_firstlook.html",
    "href": "toolset_firstlook.html",
    "title": "Orientation to server with code templates",
    "section": "",
    "text": "When you first log in to https://patools.sprouthub.io, you should see R Studio. It should look like this:\n\n\n\n\n\nNotice the lower right quadrant in the above screenshot is showing “files.” Zooming in, we see:\n\n\n\n\n\nClick on the CDIPAtools folder. Then you should see the following:\n\n\n\n\n\nClick on CDIPATools.Rproj. This will open the project. It will ask you if you want to save your workspace. You never need to save your workspace. Click “no.” You should still have the same view of the files. Next, open the Notebooks folder.\nThis is where you will find all of the notebook templates. For each of the three notebooks, you will see the .Rmd and .pdf versions (the markdown script and “knit” or compiled document.) You will also see an .Rmd file called “Directions_for_Setup_All_Notebooks.Rmd,” which provides information about the “setup” sections across all the other analytic notebooks. You have the option to reference this if you are curious.\n\n\n\n\n\nWe will come back to this folder, but first, let’s look at a couple of other folders. If you go pack to the main CDIPAtools folder, you can open the data-r-objects folder. You will see two subfolders: inputs and outputs. You will upload your data files to the inputs folder. That is, it is to this folders that you will upload your training data, testing data, training metadata and testing metadata. Remember, this is your own workspace, so you will not see other students’ data files here. However, you will see files that I have created for illustration. You can just leave these alone and add your own files. The notebooks will write files to the output folder, but you probably will never need to access them. The notebooks will access and summarize the output files for you.\n\n\n\n\n\nFinally, if you again go back to the CDIPAtools folder, you can then click on the R folder. You do not need to access any of the scripts in this folder. However, if you are curious and would like to look “under the hood,” you can explore these scripts. The R notebook templates are calling these scripts, which hold functions for implementing the predictive analytics workflow. You will see the notebooks loading these scripts in the setup chunks.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "toolset_tidymodels.html",
    "href": "toolset_tidymodels.html",
    "title": "Harnessing the tidymodels package",
    "section": "",
    "text": "For implementing much of the predictive analytics workflow, we make use of the tidymodels package.\n\n\n\n\n\nThis all happens in the background. That is, you will not see calls to tidymodels because that is happening in the scripts within in the R folder. If you are curious to learn more about tidymodels and how to use related functions directly, there are many tutorials available online.\nI will note that tidymodels often takes a different approach to predictive analytics than what I am teaching in this course. For example, while I am recommending doing data preparation as a separate, initial step, tidymodels has built in recipes for doing some data preparation as part of model training procedures. We ignore those features that do not apply to our framework. We instead harness the strengths of tidymodels and make it work for our framework. A key strength of tidymodels is that it standardizes output across a wide range of machine learning packages.\n\n\n\n Back to top"
  },
  {
    "objectID": "perform_output.html",
    "href": "perform_output.html",
    "title": "Learner training & validation with code templates",
    "section": "",
    "text": "After making all specifications for your PA proof-of-concept in 01_Learner_Specification.Rmd, all the learners you defined are trained in 02_Learner_Training_and_Validation.Rmd. This happens in the code chunk replicated below. The user does not need to enter or edit any code within the chunk. Specifications that were entered in 01_Learner_Specification.Rmd are loaded in the chunk that precedes this one and are passed into the train_learners() function.\n# running model results all learners in learnerSpec (all specified in 01_Learner_Specification.Rmd)\nlearnersResults &lt;- train_learners(\n  learnerSpec = learnerSpec,\n  recipes = recipes,\n  mainRecipes = mainRecipes\n)\n\n # this returns cross-validated means and SE for AUC-PR and AUC-ROC\nmodelResults &lt;- learnersResults$modelResults\n # this is predicted probabilities, from when obs are in validation set\npredProbs    &lt;- learnersResults$predProbs \nAs commented in the above chunk, the code returns information about couple of performance metrics abbreviated as AUC-PR and AUC-ROC. AUC-PR is the area under the curve (AUC) for a precision recall (PR) curve. AUC-ROC is the area under the curve (AUC) for a receiver operating characteristic (ROC) curve. We will return to explanations of these in a bit.\nFirst, let’s focus on the other information that is returned in the above code chunk - the predicted probabilities. For each observation (unit or row) in our dataset used for training and valiation and for each learner, the code returns a predicted probability of the outcome.\nFor example, imagine we are predicting whether a student will drop out of high school. Then, for each student, we get the following information, displayed for just one student.\n\n\n      student_id              learnerName  .pred_yes dropout\n1              1           glm_predSet_bm 0.19666440       0\n8000           1           glm_predSet_ms 0.09658994       0\n15999          1         lasso_predSet_ms 0.08760455       0\n23998          1 random_forest_predSet_ms 0.17969438       0\n\n\nThe numbers to the left (1, 8000, 15999, 23998) are the row numbers in the data.frame predProbs that correspond to the student with student_id = ‘1’. By examining the entries for learnerName, we can see that we conducted training for four different learners. These four different learners were combinations of two predictor sets (“bm” and “ms”) and three modeling approaches (“glm”, “lasso” and “random_forest”). Note that the predSet suffix of ’_bm’ signifies the “benchmark” predictor set. The suffix of ’_ms’ was chosen by the user for a second predictor set; it is not following any particular convention. The column .pred_yes tells us the probability that the outcome is equal to ‘1’. (While not displayed, predProbs also contains pred_no, the probability that the outcome is equal to ‘0’.) Finally, we also see the actual, observed outcome (dropout) for this student. The predProbs dataframe also includes all the other measures in the training/validation data; they are just not displayed here.\nThe predicted probabilities listed for .pred_yes were estimated through the \\(v\\)-fold cross-validation procedure. Recall from the earlier section on cross-validation, that each observation takes a turn in a validation fold. After a learner was trained on the \\(v-1\\) other folds, the resulting model was applied to the validation fold, producing a predicted probability for each observation in the validation fold."
  },
  {
    "objectID": "perform_output.html#learner_training_and_validation",
    "href": "perform_output.html#learner_training_and_validation",
    "title": "Learner training & validation with code templates",
    "section": "02_Learner_Training_and_Validation",
    "text": "02_Learner_Training_and_Validation\nAfter making all specifications for your PA proof-of-concept in 01_Learner_Specification.Rmd, all the learners you defined are trained in 02_Learner_Training_and_Validation.Rmd. This training happens in the code chunk replicated below. The user does not need to enter or edit any code within the chunk. Specifications that were entered in 01_Learner_Specification.Rmd are loaded in the chunk that precedes this one and are passed into the train_learners() function.\n# running model results all learners in learnerSpec (all specified in 01_Learner_Specification.Rmd)\nlearnersResults &lt;- train_learners(\n  learnerSpec = learnerSpec,\n  recipes = recipes,\n  mainRecipes = mainRecipes\n)\n\n # this returns cross-validated means and SE for AUC-PR and AUC-ROC\nmodelResults &lt;- learnersResults$modelResults\n # this is predicted probabilities, from when obs are in validation set\npredProbs    &lt;- learnersResults$predProbs \nAs commented in the above chunk, the code returns information about a couple of performance metrics abbreviated as AUC-PR and AUC-ROC. AUC-PR is the area under the curve (AUC) for a precision recall (PR) curve. AUC-ROC is the area under the curve (AUC) for a receiver operating characteristic (ROC) curve. We will return to explanations of these in a bit.\nFirst, let’s focus on the other information that is returned in the above code chunk - the predicted probabilities. For each observation (unit or row) in our dataset used for training or validation, each learner produces a predicted probability of the outcome. The code returns these predicted probabilities for each observation for each learner.\nFor example, imagine we are predicting whether a student will drop out of high school. Then, for each student, we get the following information, displayed for just one student.\n\n\n      student_id              learnerName  .pred_yes dropout\n1              1           glm_predSet_bm 0.19666440       0\n8000           1           glm_predSet_ms 0.09658994       0\n15999          1         lasso_predSet_ms 0.08760455       0\n23998          1 random_forest_predSet_ms 0.17969438       0\n\n\nThe numbers to the left (1, 8000, 15999, 23998) are the row numbers in the data.frame predProbs that correspond to the student with student_id = ‘1’. By examining the entries for learnerName, we can see that we conducted training for four different learners. These four different learners were combinations of two predictor sets (“bm” and “ms”) and three modeling approaches (“glm”, “lasso” and “random_forest”). Note that the predSet suffix of ’_bm’ signifies the “benchmark” predictor set. The suffix of ’_ms’ was chosen by the user for a second predictor set; it is not following any particular convention. The column .pred_yes tells us the probability that the outcome is equal to ‘1’. (While not displayed, predProbs also contains pred_no, the probability that the outcome is equal to ‘0’.) Finally, we also see the actual, observed outcome (dropout) for this student. The predProbs dataframe also includes all the other measures in the training/validation data; they are just not displayed here.\nThe predicted probabilities listed for .pred_yes were estimated through the \\(v\\)-fold cross-validation procedure. Recall from the earlier section on cross-validation, that each observation takes a turn in a validation fold. After a learner was trained on the \\(v-1\\) other folds, the resulting model was applied to the validation fold, producing a predicted probability for each observation in the validation fold."
  },
  {
    "objectID": "perform_output.html#summarizing-the-predicted-probabilities",
    "href": "perform_output.html#summarizing-the-predicted-probabilities",
    "title": "Learner training & validation with code templates",
    "section": "Summarizing the predicted probabilities",
    "text": "Summarizing the predicted probabilities\nA helpful first step for understanding the validation results is to examine the distributions of the predicted probabilities. The following set of plots shows the distribution for each learner. The code chunk named “plotpredProbs” in 02_Learner_Training_and_Validation will create these plots for you.\nAs expected, because most students do not drop out, the predicted probabilities tend to cluster around low values. Also, note that some learners produce greater variability than others.\n\n\n\n\n\nNext, the notebook plots overlapping distributions of predicted probabilities for when the observed outcome = ‘1’ and when the observed outcome = ‘0’. This plot illustrates how well each learner is able to separate out the predicted probabilities for the different classifications."
  },
  {
    "objectID": "perform_confusionmatrix.html",
    "href": "perform_confusionmatrix.html",
    "title": "The confusion matrix and threshold-variant metrics",
    "section": "",
    "text": "Many of the most helpful and intuitive metrics of learner/model performance depend on converting predicted probabilities into predicted binary classifications (or ‘0’ or ‘1’ predictions of the outcome). To do this, we first need to specify a threshold. After selecting a threshold, predicted probabilities above the threshold indicate that the predicted outcome equals ‘1’ (or ‘yes’), and predicted probabilities below the threshold indicate that the predicted outcome equals ‘0’ (or ‘no’). For example, we may specify a threshold of 0.5. Then, turning back to our example on the previous page, any student with a predicted probability greater than or equal to 0.5 is considered “predicted to drop-out,” while any student with a predicted probability less than 0.5 is considered “not predicted to drop-out.”\nLater, we will discuss how to pick a threshold. For now, once we have picked a threshold, we can then compare each resulting predicted classification (i.e. predicted outcome) to the actual, true, observed classification (e.g. whether or not a student actually dropped out.) Sometimes We can summarize the comparisons in 2x2 matrix called a confusion matrix:\n\n\n\n\n\nWithin a confusion matrix, observations are categorized into one of the following cells:\n\nTrue positives (TP): the predicted classification (or “class”) is ‘1’ (‘yes’) and the true classification is ‘1’.\nFalse positives (FP): the predicted classification (or “class”) is ‘1’ (‘yes’) but the true classification is ‘0’.\nFalse negatives (FN): the predicted classification (or “class”) is ‘0’ (‘no’) but the true classification is ‘1’.\nTrue negatives (TN): the predicted classification (or “class”) is ‘0’ (‘no’) and the true classification is ‘0’.\n\nNote that for “true positives” and “false positives”, the predicted and observed classification match, while in “false positives” and “false negatives” we make a prediction that does not match the true classification.\nWe also calculate the following totals:\n\nObserved positives (OP): the number of observed (true) positives (the number of observations for which the outcome is observed to be ‘1’).\nObserved negatives (ON): the number of observed (true) negatives (the number of observations for which the outcome is observed to be ‘0’).\nPredicted positives (PP): the number of predicted positives (the number of observations for which the outcome is predicted to be ‘1’).\nPredicted negatives (PN): the number of predicted negatives (the number of observations for which the outcome is predicted to be ‘0’).\n\nFinally, we can also summarize these categories into many different rates. Below is a list of various rates that may be helpful. In a later page, we will discuss how to pick different metrics to align with goals for the project.\nRates based on total true, observed number of one classification (OP or ON):\n\nTrue positive rate (TPR): the proportion of observed positives that were correctly predicted as positive. The TPR is also known as sensitivity or recall.\n\n\\[\\begin{equation}\nTPR = \\frac{TP}{TP + FN} = \\frac{TP}{OP}\n\\end{equation}\\]\n\nTrue negative rate (TNR): the proportion of observed negatives that were correctly predicted as negative. The TNR is also known as specificity.\n\n\\[\\begin{equation}\nTNR = \\frac{TN}{TN + FP} = \\frac{TN}{ON}\n\\end{equation}\\]\n\nFalse positive rate (FPR): the proportion of observed negatives that were incorrectly predicted as positive. Note that specificity is equal to 1-FPR.\n\n\\[\\begin{equation}\nFPR = \\frac{FP}{TN + FP} = \\frac{FP}{ON}\n\\end{equation}\\]\n\nFalse negative rate (FNR): the proportion of observed positives that were incorrectly predicted as negative.\n\n\\[\\begin{equation}\nFNR = \\frac{FN}{TP + FN} = \\frac{FN}{OP}\n\\end{equation}\\]\nRates based on total predicted number of one classification (PP or PN):\n\nPositive predictive value: the proportion of predicted positives that were true observed positives. The PPV is also known as precision.\n\n\\[\\begin{equation}\nPPV = \\frac{TP}{TP + FP} = \\frac{TP}{PP}\n\\end{equation}\\]\n\nNegative predictive value:: the proportion of predicted negatives that were true observed negatives.\n\n\\[\\begin{equation}\nNPV = \\frac{TN}{TN + FN} = \\frac{TN}{PN}\n\\end{equation}\\]\n\nFalse discovery rate: is proportion of predicted positives that are false positives. The FDR is equal to 1-precision.\n\n\\[\\begin{equation}\nFDR = \\frac{FP}{FP + TP} = \\frac{FP}{PP}\n\\end{equation}\\]\nRate based on total sample size (total number of predictions):\n\nAccuracy: is proportion of all observations (units) that were correctly classified.\n\n\\[\\begin{equation}\nFDR = \\frac{TP + TN}{TP + FP + TP + TN}\n\\end{equation}\\]\nA note on accuracy: Accuracy is highly sensitive to the true probability of success. Imagine a scenario where 95% of people are truly successful. You create an algorithm or model that predicts every single person will succeed. Your model will be 95% accurate! Thus, you should be careful when using accuracy as a performance metric.\n\n\n\n Back to top"
  },
  {
    "objectID": "perform_goals.html",
    "href": "perform_goals.html",
    "title": "Prioritizing metrics that align with goals",
    "section": "",
    "text": "Which metrics should you prioritize? This choice depends on the goal for how predictions will be used to guide decision-making.\n\nGoal 1: Maximizing detection of positive cases\n\nMetric: True positive rate (sensitivity/recall)\nAdvice: Focus on this when it’s critical to identify as many positive cases as possible, even at the risk of increasing false positives.\nExample:\n\nScenario: Cancer detection where missing a positive case can be fatal.\nModel Comparison: Model A has a recall of 95%, while Model B has a recall of 89%. If maximizing recall is the sole focus, Model A is preferable.\n\nLimitation/trade-off: Prioritizing a model with a high true positive rate (sensitivity/recall) could lead to more false positives, meaning patients without cancer could be mistakenly diagnosed, leading to unnecessary stress, tests, and treatments.\n\n\n\nGoal 2: Minimizing false positives\n\nMetric: True negative rate (specificity) or false positive rate (1 - specificity)\nAdvice: Important when the cost or risk of false positives is high and it is crucial to accurately identify negative cases.\nExample:\n\nScenario: Loan approval where false positives mean approving loans for individuals likely to default.\nModel Comparison: Model A has a specificity of 90%, while Model B has a specificity of 95%. In this case, Model B might be preferred to minimize the risk of loan defaults.\n\nLimitation/trade-off: Increasing specificity could lead to more false negatives, where creditworthy applicants are denied loans, leading to loss of business opportunities. The company might become overly conservative in granting loans, impacting its competitiveness and profit margins.\n\n\n\nGoal 3: Balanced performance for both classes\n\nMetric: F1-score. The F1-score is the harmonic mean of precision and recall:\n\n\\[\\begin{equation}\nF_1 = 2\\frac{precision*recall}{precision + recall} = 2\\frac{PPV * TPR}{PPV + TPR}\n\\end{equation}\\]\nImagine we have a very rare outcome we are trying to predict (e.g. a rare form of cancer that occurs in less than 1% of the population). Note that if we predict ‘0’ for the whole sample, we have precision = 0, and our recall for the positive class is also 0. Accuracy in this case would be over 99% though! The F1 score = 0 in this case, so we know our model us useless.\n\nAdvice: Focus on the F1 score when you need a balance between identifying positive cases and minimizing false positives, especially for imbalanced datasets.\nExample:\n\nScenario: Spam email detection where both missing spam and misclassifying non-spam are problematic.\nModel Comparison: Model A has an F1-score of 0.85, while Model B has an F1-score of 0.80. Model A may be preferred for a balanced performance.\n\nLimitation/trade-off: Even with a balanced F1-score, there might still be a notable number of false positives and false negatives. Some spam might get through, and some legitimate emails might be marked as spam. Choosing the F1-score provides some balance, but not a complete elimination of errors.\n\n\n\nGoal 4: Overall accuracy\n\nMetric: Accuracy\nAdvice: Consider this when you need a general measure of performance and the classes are fairly balanced.\nExample:\n\nScenario: Image classification with multiple balanced classes.\nModel Comparison: Model A has an accuracy of 92%, while Model B has an accuracy of 89%. Model A might be preferred for higher overall accuracy.\n\nLimitation/trade-off: Overall accuracy does not provide insights into class-specific performance. A model might still have poor performance on individual classes but show high overall accuracy, especially in multi-class scenarios. The metric can be also be highly misleading for imbalanced datasets.\n\n\n\nGoal 5: Model’s ability to rank predictions\n\nMetric: AUC-ROC\nAdvice: Valuable in scenarios where it’s not just about classifying instances but also ranking them.\nExample:\n\nScenario: Predicting customer churn where ranking customers by churn risk is essential for targeted interventions.\nModel Comparison: Model A has an AUC-ROC of 0.75, while Model B has an AUC-ROC of 0.82. Model B might be preferred for its better ranking ability.\n\nLimitation/trade-off: Two models with the same AUC-ROC can have different trade-offs between identifying positives correctly and avoiding false alarms. For example, Model A might have higher precision (fewer false positives) but lower recall (more false negatives), while Model B has higher recall and lower precision, yet both could have the same AUC-ROC. AUC-ROC can also be misleading in the presence of class imbalance because it evaluates the model’s ability to distinguish between classes, without accounting for their proportions.\n\n\n\nGoal 6: Precision in top-K predictions\n\nMetric: Precision at \\(K\\)\nAdvice: Focus on this when the top few (precisely defined by \\(K\\)) predictions are more critical than the overall precision, commonly in recommendation systems.\nExample:\n\nScenario: E-commerce product recommendation where the top 10 product recommendations should be as relevant as possible.\nModel Comparison: Model A has a Precision@10 of 0.8, while Model B has a Precision@10 of 0.9. Model B would likely provide more relevant top-10 recommendations.\n\nLimitation/trade-off: Focusing on top-K precision could lead to models that are optimized for only a small subset of data, neglecting overall model performance. It might result in a narrower, less diverse set of recommendations, potentially impacting user experience.\n\n\n\nGoal 7: You are not sure yet\n\nMetric: AUC-ROC\nAdvice: Focus on this when you don’t yet know what thresholds will be applied for decision-making. The AUC-ROC captures the trade-offs between sensitivity (recall) and 1-specificity across all possible thresholds.\nExample:\n\nScenario: High school early warning system aiming to find which students are most at-risk of not graduating on time.\nModel Comparison: Model A has an AUC-ROC of 0.75, while Model B has an AUC-ROC of 0.82. Model B might be preferred for its better performance across all possible thresholds.\n\nLimitation/trade-off: Two models with the same AUC-ROC can have different precision and recall values. It does not take into account the distribution shift and class imbalance. It will be important to provide validation metrics based on a chosen threshold when decided.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "perform_thresholdinvariant.html",
    "href": "perform_thresholdinvariant.html",
    "title": "Threshold-invariant metrics",
    "section": "",
    "text": "For some performance metrics, we do not need to specify a threshold. Here we summarize two “threshold-invariant” metrics: the AUC-ROC and AUC-PR. Remember that this code chunk from 02_Training_and_Validation.Rmd (repeated below) also returns these metrics. For these metrics, each is computed for each validation fold during the \\(v\\)-fold cross-validation procedure. Then the metrics are averaged across all \\(v\\) validation folds."
  },
  {
    "objectID": "perform_thresholdinvariant.html#auc-roc",
    "href": "perform_thresholdinvariant.html#auc-roc",
    "title": "Threshold-invariant metrics",
    "section": "AUC-ROC",
    "text": "AUC-ROC\nAUC-ROC is the area under the curve (AUC) for a receiver operating characteristic (ROC) curve. The higher the value, the better the model is at making predictions.\nAn ROC curve plots FPR (1-specificity) on the \\(x\\)-axis against the TPR (sensitivity/recall) on the \\(y\\)-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as ‘1’, and all units with a predicted value below 0.95 will be classified as ‘0’. At this threshold, most of the units we classify as ‘1’ will be true observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as ‘1’, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a line that goes straight up the diagonal from 0 to 1. This line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nThe AUC-ROC measures how well predictions are ranked, rather than their absolute values. Therefore, the measure is helpful when you want to target a service, marketing campaign, intervention, etc. to those most or least likely to have a positive outcome.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.\nThis code chunk in 02_Learner_Training_and_Validation.Rmd plots ROC curves for each specified learner.\n# running model results all learners in learnerSpec (all specified in 01_Learner_Specification.Rmd)\nlearnersResults &lt;- train_learners(\n  learnerSpec = learnerSpec,\n  recipes = recipes,\n  mainRecipes = mainRecipes\n)\n\n # this returns cross-validated means and SE for AUC-PR and AUC-ROC\nmodelResults &lt;- learnersResults$modelResults\n # this is predicted probabilities, from when obs are in validation set\npredProbs    &lt;- learnersResults$predProbs"
  },
  {
    "objectID": "perform_thresholdinvariant.html#auc-pr",
    "href": "perform_thresholdinvariant.html#auc-pr",
    "title": "Threshold-invariant metrics",
    "section": "AUC-PR",
    "text": "AUC-PR\nAUC-PR is area under the curve (AUC) for a precision recall (PR) curve. For the PR curve, the true positive rate (sensitivity/recall) is on the \\(x\\)-axis and the positive predicted value (precision) is on the \\(y\\)-axis. Like in AUC ROC as well, the precision and recall are plotted as the threshold changes.\nAUC-PR is a valuable metric when there is class imbalance, the positive class is of particular interest, and both types of classification errors (false positives and false negatives) carry significant consequences. Because AUC-PR does not account for true negatives, it is often preferred over AUC-ROC when outcome classes are imbalanced. With a rare outcome, the FPR tends to remain low due to a large number of observed negatives. Recall that FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC-ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TP and FP) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nThis code chunk in 02_Learner_Training_and_Validation.Rmd plots PR curves for each specified learner.\n# running model results all learners in learnerSpec (all specified in 01_Learner_Specification.Rmd)\nlearnersResults &lt;- train_learners(\n  learnerSpec = learnerSpec,\n  recipes = recipes,\n  mainRecipes = mainRecipes\n)\n\n # this returns cross-validated means and SE for AUC-PR and AUC-ROC\nmodelResults &lt;- learnersResults$modelResults\n # this is predicted probabilities, from when obs are in validation set\npredProbs    &lt;- learnersResults$predProbs"
  },
  {
    "objectID": "perform_selectthreshold.html",
    "href": "perform_selectthreshold.html",
    "title": "Selecting a threshold",
    "section": "",
    "text": "Selecting a threshold that translates predicted probabilities into predicted classifications can be a difficult choice. The decision requires consideration of how results will be used in decision-making, as well as information about the distribution of the results. The following discussion provides some different potential approaches.\n\n1. Specify a fixed value for the threshold.\nSpecifying a value, such as 0.5, to distinguish between the classes is straightforward and may be most useful in some contexts. For example, in a marketing campaign where the target audience is balanced between interested and not interested groups, a 0.5 threshold could be used to classify customers who are likely to respond to the campaign. Note that it is important to examine the distribution of the predicted probabilities before deciding on a threshold. If the predicted probabilities are clustered primarily below 0.5, for instance, then a threshold of 0.5 will result in few units being classified as positive (‘1’ or ‘yes’ on the binary outcome). Context also matters of course. For example, when classifying cancer, even if a patient has a 0.3 probability of having cancer, you would classify them to be 1.\n\n\n2. Match the predicted proportion to the observed proportion.\nHere, the threshold is set such that the proportion of predicted positive cases is equal to the observed proportion of positives in the dataset. It involves sorting predicted probabilities and selecting a threshold that aligns the predicted and observed ratios. This approach can be more adaptive than a fixed threshold, especially for imbalanced datasets. However, it doesn’t directly consider the costs associated with different types of errors. This approach is often used when maintaining the natural class distribution in predictions is a priority. For example, in credit scoring, if 20% of the applicants historically default, this approach would set a threshold to classify the riskiest 20% of new applicants as “high risk” for default.\n\n\n3. Set the threshold equal to the the median of predicted probabilities.\nThis approach assumes that the distribution of probabilities is symmetric and aids in ensuring a balance in classification. While helping in certain scenarios to balance classifications, it’s not suitable for highly skewed or multi-modal distributions of predicted probabilities. For example, imagine a service wants to predict which current subscribers are less likely to renew their subscriptions so they can decide who to offer special renewal incentives. Given a symmetric distribution of predicted probabilities for renewal, the median threshold can be applied so that the incentive program is neither too broad (which could be costly and less effective) nor too narrow (missing out on potential non-renewals).\n\n\n4. Find a threshold that targets resource constraints.\nIt may be helpful to find a threshold that takes resource constraints into account. For instance, if resources are available to intervene with only 100 individuals, you can find the threshold to identify the top 100 most at-risk individuals. This approach makes the model operationally relevant.\n\n\n5. Target a desired positive rate.\nIn cases like rare disease detection, a high true positive rate is critical, even if it increases false positives. For example, in medical diagnostics for a rare disease, setting a threshold to ensure at least 95% of actual cases are identified, even if it results in more false positives, to ensure maximum detection of the disease. Of course, the trade-off of finding the threshold that ensures a high detection rate of positive cases can also lead to increased false alarms.\nCode chunks in 02_Learning_Training_and_Validation.Rmd compute many (but not all) of these threshold targets. We will go over this in class.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "perform_considerations.html",
    "href": "perform_considerations.html",
    "title": "Metrics considerations for validation vs. testing",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "fairness_intro.html",
    "href": "fairness_intro.html",
    "title": "Sources of bias",
    "section": "",
    "text": "Earlier, we discussed a wide range of ethical considerations that pertain to predictive analytics. Some of these considerations are related to bias, or fairness, of predictive models and how their results are used. This is an enormous, complex and extremely important topic. In this section, we provide an overview of key concepts.\nWe begin with a discussion of the potential sources of bias in predictive analytics. The development and use of predictive analytics involves several steps, each of which present different sources of bias:\n\nStep 1: Modeling to produce predicted probabilities.\nStep 2: Translating results into recommendations for action.\nStep 3: Implementing decisions based on recommendations.\n\nThe following pages discuss how bias may be introduced in each of these steps.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bias_intro.html",
    "href": "bias_intro.html",
    "title": "Sources of bias",
    "section": "",
    "text": "Earlier, we discussed a wide range of ethical considerations that pertain to predictive analytics. Some of these considerations are related to bias, or fairness, of predictive models and how their results are used. This is an enormous, complex and extremely important topic. In this section, we provide an overview of key concepts.\nWe begin with a discussion of the potential sources of bias in predictive analytics. The development and use of predictive analytics to improve services involves several steps. Each step presents different sources of bias:\n\nStep 1: Estimating predicted probabilities of the outcome.\nStep 2: Translating results into recommendations for action.\nStep 3: Implementing decisions based on recommendations.\n\nThe following pages discuss how bias may be introduced in each of these steps. To illustrate, we will use an example from the criminal justice system, when the consequences of bias are significant for the lives of people affected. In particular, we will focus on the pretrial period. Across the country, release and detention decisions for defendants in the pretrial period are increasingly guided by algorithmic risk assessments. These assessments rely on data to estimate defendants’ risks of failing to appear for a court date or of being charged with new criminal activity if released pending trial. The results are generally used by a judicial body to help determine whether a defendant will be released while waiting for a case to be resolved, and if so, under what conditions. The content is this section is summarized from Porter, Redcross, & Miratrix (2020).\nAs we discuss the potential sources of bias in pretrial assessment, we’ll focus on bias that can affect Black defendants for simplicity, although any of the ideas could be applied to other demographic subgroups.\n\n\n\n\n\n Back to topReferences\n\nPorter, K. E., Redcross, C., & Miratrix, L. (2020). Balancing promise and caution in pretrial risk assessments."
  },
  {
    "objectID": "bias_modeling.html",
    "href": "bias_modeling.html",
    "title": "Bias in predicting probabilities",
    "section": "",
    "text": "Here, we highlight three potential sources of bias in the first step of developing and using predictive analytics: estimating the predicted probabilities of the outcome of interest.\n\nBias in the data\nIn the administrative data used to develop and validate pretrial assessments, some data elements are very likely to be inherently biased. A key challenge is that crime data does not exist. Data only captures contacts with police and decisions made after that. Therefore, to the extent that police activity and subsequent steps in the criminal justice system are biased, the data are biased.\nConsider measures of criminal activity, for example. One factor leading to recorded criminal activity is the likelihood of getting arrested. This likelihood can vary by race even when the true level of criminal activity is constant. For example, individuals who live in areas with elevated amounts of policing will tend to have more arrests than individuals with the same criminal behavior who live in areas with less policing. The level of policing in neighborhoods is often correlated with race. Therefore, White and Black populations with the same actual prevalence of criminal behavior will be arrested at different rates for such behavior.\n\nThe resulting bias in the data affects both the risk factors used as predictors in a risk assessment tool and the outcomes predicted by a risk assessment tool – such as failure to appear for court or new criminal activity during the pretrial period. An assessment that depends on risk factors that are biased may perpetuate this bias because it could score a Black defendant as at higher risk than a White defendant with the same true level of risk.\nWith respect to outcomes, particularly the outcome of new criminal activity, Black defendants awaiting trial and living in highly policed areas may be more likely to be picked up for similar offenses than White defendants awaiting trial in other areas. Therefore, even if the predictors in a risk assessment are not biased, the evaluation of the risk assessment could suggest bias, as a larger proportion of Black defendants with the same risk score could have new arrests than did similar White defendants.\n\n\nBias in modeling\nEven if the risk factors and outcomes involved in a risk assessment are all measured without bias, the underlying model of a risk assessment could still potentially introduce bias. This situation can occur if the relationships between the risk factors and the outcome are different for different racial groups. Take, for example, a condition where pending charge predicts an outcome better for White defendants than Black ones (perhaps because the level of the pending charge among White people signals involvement in a relatively more serious crime level than pending charge among Black people – due to potential bias towards Blacks in the assignment of charges). If a model is then fit to all the data, which would average the two trends, pending charge for Black people would predict a higher risk score than the truth.\n\n\n\nBias in censoring\nA third source of bias in the initial phase of risk assessment—calculating the risk score—stems from “differential censoring.” This term refers to the challenge arising when predictive models are typically developed using data from defendants who were not detained pre-trial. These individuals have complete data sets, unlike their detained counterparts, for whom we lack data on whether they would have failed to appear in court or been involved in new criminal activity if released.\nThe problem intensifies when the risk assessment tools, designed from the non-detained population, are applied universally—to both those who we be detained and not. This application can lead to inaccuracies if those detained exhibit systematic differences from those not detained, leading to models that do not effectively predict the risk levels of the detained population.\nFurthermore, if detention trends vary by race, developing models from different racial subsets due to differential censoring can introduce bias. In essence, as illustrated in the picture below, a lack of complete data for detained individuals and potential systematic differences between detained and non-detained populations can undermine the accuracy and fairness of the resulting risk assessment models.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "bias_use.html",
    "href": "bias_use.html",
    "title": "Bias in predicting probabilities",
    "section": "",
    "text": "Here, we highlight three potential sources of bias in the first step of developing and using predictive analytics: estimating the predicted probabilities of the outcome of interest.\n\nBias in the data\nIn the administrative data used to develop and validate pretrial assessments, some data elements are very likely to be inherently biased. A key challenge is that crime data does not exist. Data only captures contacts with police and decisions made after that. Therefore, to the extent that police activity and subsequent steps in the CJ are biased, the data is biased.\nConsider measures of criminal activity, for example. One factor leading to recorded criminal activity is the likelihood of getting arrested. This likelihood can vary by race even when the true level of criminal activity is constant. For example, individuals who live in areas with elevated amounts of policing will tend to have more arrests than individuals with the same criminal behavior who live in areas with less policing. The level of policing in neighborhoods is often correlated with race. Therefore, White and Black populations with the same actual prevalence of criminal behavior will be arrested at different rates for such behavior.\n\nThe resulting bias in the data affects both the risk factors used as predictors in a risk assessment tool and the outcomes predicted by a risk assessment tool – such as failure to appear or new criminal activity during the pretrial period. An assessment that depends on risk factors that are biased may perpetuate this bias because it could score a Black defendant as at higher risk than a White defendant with the same true level of risk.\nWith respect to outcomes, particularly the outcome of new criminal activity, Black defendants awaiting trial and living in highly policed areas may be more likely to be picked up for similar offenses than White defendants awaiting trial in other areas. Therefore, even if the risk factors in a risk assessment are not biased, the evaluation of the risk assessment could suggest bias, as a larger proportion of Black defendants with the same risk score could have new arrests than did similar White defendants.\n\n\nBias in modeling\nEven if the risk factors and outcomes involved in a risk assessment are all measured without bias, the underlying model of a risk assessment could still potentially introduce bias. This situation can occur if the relationships between the risk factors and the outcome are different for different racial groups. Take, for example, a condition where pending charge predicts an outcome better for White defendants than Black ones (perhaps because the level of the pending charge among White people signals involvement in a relatively more serious crime level than pending charge among Black people – due to potential bias towards Blacks in the assignment of charges). If a model is then fit to all the data, which would average the two trends, pending charge for Black people would predict a higher risk score than the truth.\n ### Bias in censoring\nA third source of potential bias in the first step of risk assessment – estimating the risk score – is potential bias from what we call “differential censoring.” The idea here is that predictive models are generally fit to only those defendants not detained while awaiting trial because they are the only ones with complete data. For those who are detained, we don’t know if they would have failed to appear or been involved in NCA if they had been released. But the resulting risk assessment tool that is developed will be applied to both groups. This missing information challenge can cause multiple problems. First, if the people who are detained are systematically different from the people who are not detained, the final models may not generalize: the models may not accurately predict risk for those people who were detained. If detention patterns differ by racial group, bias may be introduced by fitting models using different subsets of the racial groups.\n\n\n\n\n\n Back to top"
  }
]
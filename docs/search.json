[
  {
    "objectID": "OptionalPAvsInfStat.html",
    "href": "OptionalPAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of all Gentoo penguins in the world? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches). To construct these estimates about our population, we use information from a sample we have collected, such as a set of 20 Gentoo penguins caught and measured by ecologists.\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top",
    "crumbs": [
      "Additional optional material",
      "Predictive Analytics vs. Inferential Statistics"
    ]
  },
  {
    "objectID": "methods_nn.html",
    "href": "methods_nn.html",
    "title": "Neural networks",
    "section": "",
    "text": "A neural network is a popular machine learning algorithm. The approach was motivated by theories about how the brain works. Neural networks are highly simplified models of the brain. Even though they are a simplification of how the brain works, they can still be quite complicated.\nImagine you’re trying to predict a student will graduate on time, using various predictors that include measures of attendance and academic performance. The neural network is built on multiple layers, with each layer having multiple nodes.\n\nInput Layer: This layer holds initial clues about the outcome. The nodes in this layer are the individual predictors.\nHidden Layers: Next, the neural network starts to process this information, combining the individual predictors in various ways to derive deeper insights. The nodes in this layer are functions of nodes in previous layers.\nOutput Layer: The output layer produces the final answer based on all the clues (nodes) and their processing. For binary outcomes like whether a student will graduate on time, the output layer often has a single node that outputs a value (typically between 0 and 1) representing the probability of the outcome being 1.\n\nAt each layer, the neural network makes adjustments to the importance of individual and combined predictors by weighting them. The weights are adjusted during training to minimize the difference between the network’s predictions and the actual outcomes.\nThe power of neural networks comes from their ability to identify and learn complex relationships and patterns in the data. Networks with more layers and nodes are more complex and are termed “deep neural networks,” leading to the field known as deep learning.\nKey tuning parameters for neural networks include the number of hidden layers, the number of nodes in each hidden layer, regularization techniques (if applied), and the number of times during the training step the algorithm iterates through the entire dataset (known as “epochs”) to refine the model parameters.\nAdvantages of neural networks\n\nGood for modeling complex relationships: Neural networks, especially deep ones, can capture complex non-linear relationships.\nCapture interactions: The method also excels at capturing interactions between features without manual creation of interaction terms.\nHigh performing in a wide variety of settings.: Neural networks, particularly deep learning models, have seen significant attention in recent years due to their performance in a wide range of complex tasks.\n\nDisadvantages of neural networks\n\nData hungry: Neural networks often require a large amount of data to generalize well. On smaller datasets, they can easily overfit. I’m not sure if they are a good fit for most applications you all are choosing for projects.\nComputationally intensive: Training can be slow, requiring specialized hardware like GPUs for faster computation.\nNot transparent: The method is hard to explain and the resulting model is hard to decipher.\nSensitive to hyperparameter turning: The values of tuning parameters can greatly affect performance and require tuning.\n\nImplementing neural networks in R\nYou can use the neuralnet package to run neural networks in R. The code templates also allow for neural networks.\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Neural networks"
    ]
  },
  {
    "objectID": "methods_naivebayes.html",
    "href": "methods_naivebayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is based on Bayes theorem from probability theory. It’s called “naive” because it makes a naive assumption that each predictor in your data is independent of the others, given the outcome. While this assumption is typically false, in practice, it still allows the algorithm to work fairly well.\nThe Naive Bayes algorithm tries to determine the likelihood of the predictors given both classes of the binary outcome (yes and no). It then uses these likelihoods and some prior knowledge about the general frequency of the outcome (an initial estimate based on the data, before considering the predictors) to make a prediction.\nSo first, the algorithm checks the frequency of your predictors given the outcome. For instance, when predicting on-time high school graduation, it may ask: How many students were chronically absent among students who graduated on time? And: How many students were chronically absent among students who did not graduate on time?1\nFor continuous predictors (e.g. attendace rate), one approach is to “bin” or discretize the variable’s values into categories, but this approach can be limiting. Therefore, the Naive Bayes algorithm relies on “kernels,” which provide a way to estimate likelihoods without discretizing the data.2\nThe next step is based on Bayes Theorem, which is:\n\\[\\begin{equation}\nP(Y=y|X=x) = \\frac{P(Y=y)P(X=x|Y=y)}{P(X=x)}\n\\end{equation}\\]\nTherefore, Naive Bayes algorithm combines the prior with the likelihoods to produce a “posterior” probability for both outcomes: the probability of yes and no given the predictors in your predictor set. The outcome with the higher posterior probability is the algorithm’s prediction if classification is the goal. Otherwise, the computed posterior probability can be directly used as the predicted probability of the outcome.\nAdvantages of Naive Bayes\nAdvantages of Naive Bayes\nImplementing of Naive Bayes in R\nThe Naive Bayes algorithm can be implemented using the naiveBayes() function in the e1071 package.",
    "crumbs": [
      "Modeling approaches",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "methods_naivebayes.html#footnotes",
    "href": "methods_naivebayes.html#footnotes",
    "title": "Naive Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote than when certain categories have zero counts, a Laplace correction is used. It involves adding 1 to both the numerator and the denominator to avoid ending up with unrealistic zero conditional probabilities.↩︎\nEssentially, a kernel is a function that “smears” or “spreads out” the observed data points over the continuous space, allowing for a smooth estimate of the likelihood of observing a particular value. That is, instead of relying on a histogram-style blocky representation of a predictor’s distribution, the data is represented using a smooth curve. Commonly used kernels are the Gaussian kernel, Epanechnikov kernel, and the Rectangular kernel, among others.↩︎",
    "crumbs": [
      "Modeling approaches",
      "Naive Bayes"
    ]
  },
  {
    "objectID": "methods_rf.html",
    "href": "methods_rf.html",
    "title": "Decision trees and Random Forest",
    "section": "",
    "text": "Decision trees\nA decision tree is a simple machine learning algorithm consisting of a set of flowchart-like nodes representing decisions, which are based on a variable’s value, and branches representing the outcomes of those decisions, which lead to either a subsequent node/decision or a final prediction. We can think of a decision tree as a series of “questions” we ask the data. Based on the answers, the algorithm makes decisions or predictions.\nThe following illustrates a very simple decision tree for predicting whether bank customers will remain with their bank throughout the next year.\n\n\n\n\n\nThis tree illustrates, starting on the right side, that if a customer has a low balance and a loan with the bank, they’re likely considering leaving, possibly due to financial constraints. However, if they have a low balance but no loan, the deciding factor might be their interaction with customer service. Many calls could indicate dissatisfaction, making them likely to leave. On the left side of the tree, we see that customers with medium to high balances are further segmented based on account type. Those with basic accounts who have had many interactions with customer service are predicted to leave. However, those with premium accounts are predicted to stay, without it depending on customer service interactions.\nWith real data, the decisions made at the various nodes in a decision tree are based on a measure of “split quality.” The exact process depends on the algorithm used, but here’s a general outline of how decisions are made at the nodes using the popular CART (Classification And Regression Trees) approach:\nSelection of node variables: For each predictor, the algorithm evaluates its ability to distinguish between outcomes = 1 and outcomes = 0. This distinction is usually measured using impurity scores, such as Gini impurity or entropy. The predictor variable that provides the best separation is selected as the node variable.\nDetermining the splits: For each potential predictor variable, the algorithm assesses every possible threshold or category as a potential split. If it’s a categorical predictor (e.g., low balance: yes or no), it directly checks the separation created by this category. If it’s a continuous predictor (e.g., account balance amount), it might consider every unique value or use binning techniques to determine the best split point. The threshold or category that gives the most distinct separation (least impurity) is chosen as the split.\nIterative process: After choosing the best variable and its split, the data is divided accordingly, resulting in two child nodes. Then the process is recursively applied to each child node until a stopping criterion is reached - like a maximum tree depth or a minimum number of samples in a node.\nPruning (for some DT algorithms): After the tree is “fully grown,” it can be pruned to remove branches that have little importance or add little predictive power. This step is done to simplify the model and improve its performance on unseen data.\nIn essence, the decisions made at each node are data-driven, aiming to improve the prediction accuracy (or reduce error) of the tree. The tree-building process aims to find the splits that most effectively segregate the data in terms of the target outcome.\n\n\nRandom Forest\nA Random Forest considers many different possible decision trees. A bootstrap sample (a random sample of the units (rows), with replacement) is taken from the data, and a decision tree is constructed. This bootstrap process is repeated many times, resulting in many different possible decision trees.\nWhen growing each tree, instead of considering all features for splitting at a node, only a random subset of the predictors is considered. This procedure introduces variability and helps “de-correlate the trees,” making the model more robust.\nTo aggregate results across all the trees in the forest, the algorithm takes an “ensemble” approach. For regression (including for predicting probabilities of a binary outcome), the average prediction of all the individual trees is the final prediction. For classification, the class (i.e., yes or no, 0 or 1) that gets the most votes from all the individual trees is the final prediction (majority voting).\nThe combination of diverse trees reduces the variance (overfitting) without increasing the bias, leading to a more accurate and robust model compared to a single decision tree.\nTuning parameters for Random Forest: There are many tuning parameters for the Random Forest algorithm. While not a comprehensive list, some of the key tuning parameters include:\n\nNumber of trees (\\(B\\)): chosen to be sufficiently large so that results are stable. \\(B\\)=500 is the default in the code templates.\nNumber of predictors to consider at each node (\\(m\\)). One rule of thumb is \\(m=\\sqrt{p}\\) where \\(p\\) is the total number of predictors.\nMinimum node size. This is the minimum number of observations that can be in a terminal node. For the code templates the default is 10.\nMaximum tree depth. This limits tree complexity. The code templates do not constrain tree depth for random forests.\n\nAdvantages of Random Forest\n\nPerformance: Because Random Forest takes an ensemble approach - by aggregating across multiple trees (i.e., “bagging”) - the algorithm reduces variance and buffers against overfitting. Because a Random Forest algorithm tends to be much less prone to overfitting than individual decision trees, it is recommended over using individual decision trees.\nFlexibility: The Random Forest algorithm works well for classification and regression. It also works well with categorical and continuous predictors. Also, it does not require transforming predictor values (e.g. mean centering or scaling). Moreover, it does not require the data analyst to specify a functional form of the relationship between predictors and the outcome and therefore excels over regression methods that assume linear relationships.\nDetects interactions: The Random Forest algorithm inherently captures interactions between variables without the data scientist needing to explicitly specify them. Moreover, the Random Forest algorithm can capture not only 2-way interactions, but also 3-way or higher interactions, as well as non-linear interactions between variables, which might be missed or hard to model in traditional regression techniques.\nHandles large data sets: The algorithm can handle large datasets efficiently. The sampling technique ensures that it can scale well with the increase in data.\n\nDisadvantages of Random Forest\n\nComputationally Intensive: While Random Forest handdle large datasets, it can be very time-consuming. Computation time depends on the number of trees.\nInterpretability: Individual decision trees, with their hierarchical structure, can be visualized and interpreted easily. However, Random Forest, being an ensemble of multiple trees, lacks this level of interpretability. “Variable importance” methods, which we will turn to later, can give some insights, however.\nTuning Required: While the Random Forest algorithm can perform reasonably well with default parameters, to get the most out of the algorithm, hyperparameter tuning is often required, which can be time-consuming.\n\nImplementing Random Forest in R\nImplementation of the Random Forest algorithm, which was developed by Leo Breiman and Adele Cutler, is done with randomForest package.\n\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Decision trees and Random Forest"
    ]
  },
  {
    "objectID": "methods_logistic.html",
    "href": "methods_logistic.html",
    "title": "Logistic regression",
    "section": "",
    "text": "For some starting context: With multiple linear regression, we predict a continuous outcome based on multiple predictors. The model for multiple linear regression looks like this:\n\\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\epsilon\\)\nwhere\nThe coefficients \\(\\beta_0\\), \\(\\beta_1\\),…,\\(\\beta_k\\) tell us the strength of the relationship between the predictors and the outcome.1\nLogistic regression can be used to predict the probability of a particular category of a binary outcome.\nIf we use a linear regression model to predict probabilities, we can get predicted values that are outside the [0,1] range, which doesn’t make sense for probabilities. To ensure predictions stay in the [0,1] range, logistic regression uses the logistic function:\n\\(P(Y=1) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_kX_k}}\\)\nwhere\nThis equation can also be written as follows:\n\\(\\log\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k\\)\nHere, the log odds (or the logarithm of the odds) of the outcome being 1 (versus 0) is expressed as a linear combination of predictor variables. As we plug in values for the predictors, the linear combination gives us the log odds, which can then be transformed to get the actual probabilities (\\(P(Y=1)\\) ).2\nUnlike many machine learning algorithms, when a data scientist uses logistic regression, they are specifying the specific functional form of the relationship between the predictors and the outcome. Specifically, with logistic regression, we are specifying that the log odds is a linear function of the predictors.\nAdvantages of Logistic Regression\nDisdvantages of Logistic Regression\nImplementation of Logistic Regression in R\nIn R, we implement logistic regression with glm() in Base R, setting family=binomial(link=\"logit\"). The code templates will do this for you.",
    "crumbs": [
      "Modeling approaches",
      "Logistic regression"
    ]
  },
  {
    "objectID": "methods_logistic.html#footnotes",
    "href": "methods_logistic.html#footnotes",
    "title": "Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo estimate the coefficient values, we minimize the sum of squared errors across all observations.↩︎\nWith logistic regression, we use maximum likelihood estimation to find the best-fitting coefficients.↩︎",
    "crumbs": [
      "Modeling approaches",
      "Logistic regression"
    ]
  },
  {
    "objectID": "data2_changesovertime.html",
    "href": "data2_changesovertime.html",
    "title": "Changes in data over time",
    "section": "",
    "text": "Keep in mind that data quality, missingness and distributions can change over time. If the changes are substantial, this can interfere with model performance over time. For example, if a predictor has more or less missingness in later years compared to missingness in earlier years, this could suggest that there some new dynamic at play - e.g., a change in how the variable is understood by those providing or entering data, a change in clients’ willingness to provide information, or a change in data integration processes. Therefore, the missingness has different meaning over time and consequently perhaps different predictive value. Exploration of trends over time could guide data scientists decisions about whether to exclude some variables from their predictor sets.\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 2",
      "Changes in data over time"
    ]
  },
  {
    "objectID": "data2_EDA.html",
    "href": "data2_EDA.html",
    "title": "Exploratory data analysies",
    "section": "",
    "text": "Exploratory data analysis (EDA) is valuable for gaining an understanding of your data before diving into predictive analytics. EDA is not technically a required step in a predictive analytics work flow. In some contexts, it may be possible to achieve excellent model performance with raw data measures and without understanding the underlying data very well. However, EDA is typically recommended and often, extremely valuable for…\nData Quality Assessment: EDA uncovers data quality issues like missing values, outliers, or implausible values, which might be the result of data entry errors. Addressing these quality concerns is imperative as they can drastically affect model accuracy. Plots of variables’ distributions and summary statistics (e.g., mean, median, standard deviation, range, quartiles, interquartile range) aid in this assessment.\nInformational Value of Variables: Variables with little variation often add negligible value to predictive models. For instance, a binary predictor that’s consistent across 99% of observations offers limited insight. Similarly, a continuous variable clustering around a singular value isn’t very informative. Again plots and statistics that summarize variables’ distributions are helpful.\nIdentification of Potential Predictors:Correlations between independent variables and the outcome of interest can hint at important predictors. However, keep in mind that variables not strongly correlated with the outcome can still be valuable in models, especially when they interact with other variables or aid in capturing complex, non-linear relationships. It is difficult or impossible to visualize interaction terms (i.e. when predictors have a different relationship with the outcome when considered together than when considered apart).\nRedundancy Detection: Tools like correlation matrices or heatmaps elucidate inter-variable correlations, highlighting redundancy. When faced with overlapping predictors, it’s beneficial to opt for the most explanatory variable, especially when aiming for model simplicity and interpretability.\nPotential Bias: EDA is instrumental in detecting biases. Visual tools, such as histograms, display group distributions. An underrepresentation of a specific group, compared to the target population, can introduce model bias. Furthermore, a disproportionate frequency of missing values or outliers for certain groups is another bias indicator. Moreover, It’s crucial to evaluate the relationship between multiple variables and protected attributes (e.g., race, gender) or other important equity attributes (e.g., income level, geographical location). Variables strongly correlated with these sensitive attributes can inadvertently become proxies, potentially leading to biased decisions. For instance, using a neighborhood-based variable that correlates closely with race might unintentionally result in decisions influenced by racial attributes. EDA is an important first step for being attentive to bias, but assessing bias of model results is also essential. We will turn to this soon.\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 2",
      "Exploratory data analysies"
    ]
  },
  {
    "objectID": "data2_missingpredictors.html",
    "href": "data2_missingpredictors.html",
    "title": "Handling missing data - predictors",
    "section": "",
    "text": "Here we focus on how to handle missingness in variables that are potential predictors of our outcome of interest. Some implementations of machine learning algorithms have a built-in way to handle missing data. Others either will not run if fed data with missing values or will drop observations with any missing values. We will address missing values before using any modeling functions in R so that the approach will not vary across modeling approaches. Addressing missing data consistently allows us to make a fair comparison of differnt models’ performances.\nThere are many different methods for managing missing data. They typically involve “filling in” - or “imputing” - missing data points with plausible values. Imputation can be a valuable approach to consider, and it is discussed below. But I would argue that imputation should not be the primary strategy for predictive analytics, at least in settings in which careful data preparation is being done.1 Instead, we should recognize that missingness may be informative. Therefore, it is valuable to capture the missingness in our modeling.",
    "crumbs": [
      "Data for PA: PART 2",
      "Handling missing data - predictors"
    ]
  },
  {
    "objectID": "data2_missingpredictors.html#footnotes",
    "href": "data2_missingpredictors.html#footnotes",
    "title": "Handling missing data - predictors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn some settings, that rely on frequent streams of big data for example or that have very minimal missing values, data preparation may need to be fully automated and imputation may be preferred.↩︎",
    "crumbs": [
      "Data for PA: PART 2",
      "Handling missing data - predictors"
    ]
  },
  {
    "objectID": "data2_overview.html",
    "href": "data2_overview.html",
    "title": "Data preparation",
    "section": "",
    "text": "This section covers best practices and considerations for preparing data for predictive analytics. The data we use for predictive analytics can often be messy...\n\nThere may be missing data values that can create confusion and errors if not handled thoughtfully;\nThere may be mistakes or nonsensical values that create noise and obfuscate underlying patterns;\nThere may be valuable information to be extracted from raw measures that may not provide clear empirical value (e.g. text data); and\nThere may be differences in data variables over time, which can threaten the performance of our models.\n\nNote that for predictive analytics, data preparation may be an iterative process. In data with a large number of predictors, it may make sense to focus first on a core set of variables that are hypothesized to be strong predictors - those specified in a benchmark predictor set or incrementally expanded predictor sets. If validation of learners with these predictor sets indicate strong performance, then the data science team may move on to model selection and testing. However, if learner performance in the validation step is lower than desired or anticipated, then the data science team can circle back and work to extract more predictive value from the raw data.\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 2",
      "Data preparation"
    ]
  },
  {
    "objectID": "data_trainvalidate.html",
    "href": "data_trainvalidate.html",
    "title": "Training and validating learners",
    "section": "",
    "text": "Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Tool Set templates you will use, but I will comment on some alternative variations.\nRecall that it is imperative to train (i.e. fit a model or build a machine learning algorithm) in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.\nWe can set aside a portion of the non-test data and designate that portion for validation. We can then use the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness. This is common and valid practice.\nWe can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to \\(v\\)-fold cross-validation, in which \\(v\\) is the number of folders or partitions - or times we repeating the training and validation. The following diagram illustrates 5-fold cross-validation:\n\n\n\n\n\nThis diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Tool Set does a combination of both. For a few metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code. (And don’t worry - the PA Tool Set implements \\(v\\)-fold cross-validation for the user. You will not need to code the procedure; you will just specify how to the cross-validation should be implemented, following the guidance below.)\n\nGuidance for implementing \\(v\\)-fold cross-validation:\n\nNumber of folds: A recommended default for \\(v\\) is 5. Higher values of \\(v\\) (more folds) may be preferable if there is a large amount of data because the average performance across the folds will be more representative of the true performance (less biased), albeit with more variability in the estimates (larger variance). Additionally, more folds result in increased computation time, which can be significant. Alternatively (although also more computationally demanding), the entire \\(v\\)-fold cross-validation procedure can be repeated multiple times. In other words, we can obtain results from 5 folds, then resample to create 5 different folds and consequently acquire another set of results from another 5 folds of the same size. This approach may be advantageous if the sample size is small, as there could be substantial variability in the metrics across folds. (However, our current tool set code templates do not support this functionality.)\nHow to specify folds: Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of \\(v\\)-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.\nStratification: In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is “yes” or “1” for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few “yes” or “1” outcomes. To protect against this, we can stratify the cross-validation. This means we can (1st) randomly partition those observations for which the outcome = 1; and then (2nd) separately randomly partition those observations for which the outcome = 0; then (3rd) pair each partition with outcome =1 with a partition with outcome = 0 to get \\(v\\) folds with equal proportions of each value of the outcome. Note that the PA Tool Set templates will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.\n\n\n\nTuning with \\(v\\)-fold cross-validation:\n\nThe PA Tool Set will automatically optimize tuning parameters. It does this with \\(v\\)-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.\nThe comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the \\(v\\) folds, will be selected as the “tuned learner.” The user will then only view results of the tuned learner going forward.\n\nNote that using the same cross-validation procedure for both parameter tuning and learner validation and comparison is not technically, completely correct. Using the same data twice for both goals is sometimes called “double-dipping.” That is, using the same data again to validate the performance of the tuned learner, we may get overly optimistic performance estimates (overfitting) because the models have already “seen” that data during the tuning process. To correct for this, there are two approaches: (1) As described above, we can set aside a subset of data for validation. In this case, we first conduct parameter tuning using \\(v\\)-fold cross-validation with the training data, but then compare the tuned learners’ performance in the held-out validation data. Alternatively, (2) we can use a nested \\(v\\)-fold cross-validation. In nested cross-validation, we have an outer loop of cross-validation for model validation, and an inner loop of cross-validation for parameter tuning. This allows you to use the same data for both tuning and validation, but in a way that prevents biased performance estimates. This approach may be preferable when there are not sufficient data to hold out a separate validation data set. Sorry, our tool set templates do not have these options at this time.\nBut although our performance in the validation may be inflated, it should be inflated for all learners, still allowing us to make valid comparisons and to select the “best” learner. Remember, it is the testing that provides the best assessment of how well our models actually perform in new, unseen data.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Training and validating learners"
    ]
  },
  {
    "objectID": "data_identify.html",
    "href": "data_identify.html",
    "title": "Identifying data",
    "section": "",
    "text": "We want to be thoughtful about the data that we will use for our proof-of-concept and that we will split for training, validating and testing. Here are some important considerations for identifying data:\n\nThe data should be representative of the population and context for the predictive analytics objective. Here we want to make sure we have data for units that are experiencing a similar context as what we expect for the units for whom we will ultimately deploy predictive analytics.\n\nRecall our earlier example that centered on predicting TANF training program participants’ risks of not finding and sustaining employment. When determining which years of past data to incorporate into our learner workflow, it is essential to examine if and when the eligibility criteria for the training program underwent significant alterations. It may be prudent to restrict our data to a timeframe in which the eligibility criteria for the training program closely resemble those currently applicable to the TANF training program.\nIn our other example, in which the focus is on predicting the risk of 10th grade students not graduating on time, it is important to investigate the graduation guidelines. If the student cohorts used to train our models had more stringent or more lenient graduation standards compared to the cohorts to which we apply the models, the models might not be widely applicable in years beyond our training-validation-testing workflow.\n\nImportant measures should be consistently available. Here we want to assess whether we have sufficient consistency in the way measures are defined and entered into data systems. For example:\n\nHave the data integrations been maintained consistently over time? Specifically, if the data we intend to use is a combination of multiple underlying data sources, have these merges been conducted consistently? If not, crucial measures may occasionally be absent. Additionally, are these integrations/merges sustainable in the future? If certain data cannot be reliably obtained and integrated on an ongoing basis, they should be omitted from the predictive analytics proof-of-concept. It is imperative to consider not only historical data but also prospective scenarios. If particular data will be unavailable when implementing predictive analytics, then a proof-of-concept relying on that data will not be authentic.\nWas there a rollout of a new data system that had early implementation challenges? If so, we may want to eliminate data from those early years in our training-validation-testing workflow.\nIs survey data part of our data universe for predictive analytics? If so, do response rates vary substantially over time? Variation in response rates could (but not necessarily) mean that different populations are being captured with each survey. We would need to investigate this.\nWere there broad changes in how measures where defined or coded? We may want to limit the data we will use to train, validate and test models to a period in which the measures are consistent to the present (time close to when any model would be deployed).\nNote that inconsistencies in some measures could have implications in measures we include in our prediction sets rather than implications for how we restrict our data for our proof-of-concept. That is, if some measures change over time, we may need to exclude just those measures from our models. If there is sufficient consistency across key predictors in data we may not need to restrict the data. We will return to measure consistency later, when we discuss data quality issues.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Identifying data"
    ]
  },
  {
    "objectID": "data_overfitting.html",
    "href": "data_overfitting.html",
    "title": "Generalizing to new data",
    "section": "",
    "text": "The objective of predictive analytics\nRecall that our objective, as shown in this simple illustration, is to use data with known outcomes to build a model that allows us to predict outcomes - or the likelihood of outcomes - in data with unknown outcomes. As a reminder, we are focusing on binary outcomes (those with just two values - yes or no), which is why we focus on predicting probabilities or likelihoods of outcomes.\n\n\n\nAvoiding “overfitting”\nHow do we make sure that our learner, or model, which is based on information in the data with known outcomes (data from the past), generalizes to new data? If our learner/model is too closely tailored to the data we use to develop it, it risks capturing incidental fluctuations or noise unique to that data in addition to meaningful statistical patterns. When a model inadvertently incorporates this noise, it excels at describing and fitting the original data. However, when confronted with new, unseen data, this same model tends to deliver inaccurate predictions. This is called “overfitting.” Overfitting refers to a model’s inability to generalize to new data - due to being constructed around specific noise rather than based on broader patterns.\nThe following simple plots display the concept of overfitting, as an overcorrection to “underfitting.” Each plot shows an attempt to capture the relationship between two measures. In the first plot, the estimated model, represented by the blue line, does not capture the trend evident in the data, shown by orange points. This is corrected in the second plot, in which the model appears to be a a good fit of the general trend. Then, in the the third plot, the model fits the data extremely well, but it overfits because it predicts almost every point - almost every random variation from the overall trend we care about.\n\nIn this first section on data for predictive analytics, we will learn how to develop, compare, and select learners/models so that we identify the one that does the “best” job in generalizing to new, unseen data. Note that “best” is in quotation marks because we have not yet discussed what how we define “best”. We will turn to that soon.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Generalizing to new data"
    ]
  },
  {
    "objectID": "poc_benchmark.html",
    "href": "poc_benchmark.html",
    "title": "Specifying a benchmark learner",
    "section": "",
    "text": "If simplicity and transparency are goals for our predictive model (these goals may not apply in all contexts), then we want to specify a simple and transparent learner as a “benchmark” to which all other, increasingly complex learners are compared.\nA benchmark learner consists of a benchmark prediction set (with a small number of measures) - and a benchmark modeling approach (one that is simple to explain, such as a decision tree or regression model).\nThe benchmark learner may align with how a service provider is already making decisions. For example, the benchmark learner may include the same measures the service currently reviews in making a decision, even if they do not create a model. Or, the benchmark may focus on measures that the service provider thinks are most important based on their expertise or based on related research.\nRemember, any measures included in the benchmark prediction set must be available at the prediction timepoint and for the prediction population.\nLet’s consider an example: Imagine we are exploring how we might use predictive analytics to help a school system improve their “early warning system,” and we have scoped the project as follows:\n\nWhat are we aiming to predict? Whether a student will not graduate from high school on-time (within 4 years of enrolling in 9th grade).\nWhen are we aiming to make predictions? After students have completed 9th grade. The analytics would be done during summer, after all 9th grade information has been entered in data systems. Prediction results would be available to educators before the start of students’ 10th grade.\nFor whom are we aiming to make predictions? All students who enrolled in 9th grade. However, we would have to exclude students who transfer outside the district during high school since we cannot know their graduation status. (Note this can create a missing data problem that we will discuss later.)\n\nHow might we define a benchmark learner for this example?\n\nThe school district has been focusing on so-called ABC indicators of attendance, behavior, and course performance to identify which students are most at-risk of not graduating on time. Therefore, a good benchmark predictor set could include the same three measures of attendance, behavior and course performance the district has used. If the district used these three measures as “indicators” (e.g. deemed a student at-risk if they passed a threshold on 2 of the 3 measures), then we could replicate the same empirical rules as a benchmark modeling approach. Alternatively, we could combine the three measures with a logistic regression model.\n\n\n\n\n Back to top",
    "crumbs": [
      "Conducting a PA proof-of-concept",
      "Specifying a benchmark learner"
    ]
  },
  {
    "objectID": "poc_importance.html",
    "href": "poc_importance.html",
    "title": "The importance of a proof-of-concept",
    "section": "",
    "text": "After we have scoped our predictive analytics project, the next step is to conduct a “proof-of-concept.” That is, before getting too invested in incorporating predictive analytics into systems of service improvement, we need to investigate the following:\n\nGiven the available data, how well can we predict our outcome of interest?\nWhen investigating this question, we will want to consider various metrics of model performance, which we will turn to in a later section. We will be concerned with the reliablity of our results. How “accurate” are they? I put “accurate” in quotations because I am referring to a general assessment of model performance, rather than the precise statistical definition of accuracy, which we will discuss soon, along with many other metrics of predictive performance.\nGiven the available data, to what extent is there bias in our predictions?\nWhen investigating this question, we will want to consider various metrics of bias, which we will also turn to in a later section. Here we will be concerned with whether and by how much model performance varies for different groups. Later, we will discuss that there are multiple definitions of bias and therefore multiple ways to measure it.\nTo what extent are complex predictive models worthwhile? Do they improve upon a simpler model or decision-making process? Do they improve upon current practice?\nWhen investigating this question, our focus is on contrasting predictive capabilities and biases across various prediction approaches. We start by looking at the simplest approach, which doesn’t necessarily involve a statistical model at all. A simple decision-making process might involve taking a small number of measures and combining and weighting them in a straightforward manner based on prior knowledge. For one version of this approach, we may set criteria that assign units into one predicted catgory or the other. For example, in the high school graduation example, perhaps any student with attendane less than 90% and with less than 20 credits is predicted to be high-risk for not graduating on time (we predict they won’t). This type of approach is sometimes called a rules-based approach or a decision tree, among other terminology.\nMoving to more complex approaches, we could plug a small number of measures into a regression model to estimate their relationship with the outcome. Finally, the most complex approaches might involve adding more measures, potentially a large number, and/or using advanced, data-driven algorithms - i.e., machine learning algorithms.\nThe crux then lies in weighing the pros and cons when comparing different models. Should a more complex approach yield superior results compared to a simpler one, it prompts a consideration: Are the improvements significant enough to offset potential drawbacks in terms of transparency and explainability? Additionally, do these improvements outweigh any potential challenges in implementing and sustaining a more intricate modeling process?\nDo stakeholders understand and trust the results from the “best” predictive model?\nWhile stakeholders should be engaged in the PA scoping process, the proof-of-concept provides another opportunity for valuable consultation. Sharing results used to investigate the above questions supports transparency and trust in the process. Stakeholders’ input and questions are essential to the goals of the proof-of-concept - to assess whether and how predictive analytics should be deployed, communicated and used to improve services.\n\n\n\nFor further consideration: Are there other questions you would want to investigate in a proof-of-concept?\n\n\n\n Back to top",
    "crumbs": [
      "Conducting a PA proof-of-concept",
      "The importance of a proof-of-concept"
    ]
  },
  {
    "objectID": "scoping_who.html",
    "href": "scoping_who.html",
    "title": "Who are we making predictions for?",
    "section": "",
    "text": "Our target population is specifically adult TANF recipients who are eligible and approved for the TANF department’s education and training program and who are required to work.\nOur data sets include a broader group of TANF applicants and participants so we need to be careful to exclude those who do not meet our criteria.\n\n\n\n Back to top",
    "crumbs": [
      "Scoping a PA project",
      "Who are we making predictions for?"
    ]
  },
  {
    "objectID": "scoping_usefindings.html",
    "href": "scoping_usefindings.html",
    "title": "How will findings be used?",
    "section": "",
    "text": "The social services department had an established practice of offering additional support to clients requiring extra assistance. This encompassed:\n\nInitiating check-in calls with clients,\nFacilitating supplementary casework sessions, and\nArranging extra workshops to enhance job search and skills.\n\nGiven the department’s constraints in terms of staff and financial resources, their aim was to effectively direct extra services to those in greatest need. They recognized that their current method of prioritizing such clients could be enhanced by distilling a wide range of potentially predictive information into a single “risk score.”\nA few things to note:\n\nThey did not want, nor did the project team recommend, for their decision-making process to become completely dependent on the risk scores.\nInstead, the risk scores could potentially provide a valuable piece of information (if we could validate good predictive performance and minimal bias).\nThe department planned to train caseworkers in how to interpret and incorporate the risk score into their decision-making and prioritizing.\nThe department also wanted to look at how risk scores varied across and within populations served by offices throughout the state. This analysis could help them understand which offices were more in need of support for reaching out to clients and for planning workshops.\n\nThis information about how the department plans to use the information can guide how we scope the predictive analytics project.\nRemember, we also want to frequently check in with our ethical considerations. What ethical considerations do we want to be careful about as we proceed?\n\n\n\n Back to top",
    "crumbs": [
      "Scoping a PA project",
      "How will findings be used?"
    ]
  },
  {
    "objectID": "paforimprovingss_context.html",
    "href": "paforimprovingss_context.html",
    "title": "Is predictive analytics a good fit for your context?",
    "section": "",
    "text": "Before embarking on a PA project, it is valuable to assess whether it is the right approach at the right time for the setting. Here are a some key considerations to determine if PA is a good fit.\n\nWill PA provide new and actionable information?\n\nDo we anticipate that the PA findings will provide information that is more reliable, more frequent or easier to understand than current information available?\nHow will findings from PA be communicated and used, and by whom?\nHow frequently do PA findings need to be updated?\n\n\n\nAre there plans and buy-in for acting on insights that come out of PA?\n\nIs there sufficient leadership and commitment from the institution?\nIs there capacity for interpreting, communicating and and acting on PA results? How will findings be communicated?\nAre there plans for how program services will be changed or how new services will be introduced based on PA results?\nTo what extent does decision making and service provision currently rely on data-driven findings? Are data-driven findings in general, and PA findings more specifically, trusted and used? If not, are there plans for addressing practice and culture to support change?\nAre there structural or process factors that impede or foster the use predictive analytics (such as dedicated data teams) or the use of results (such as dashboard systems)?\n\n\n\nData quality and systems\n\nAre there sufficient data that are readily accessible?\nAre the data sufficiently documented and understood so that reliable measures (variables to be used as outcomes or predictors) can be created for PA?\nIs there sufficient capacity for investigating and deploying PA with existing analytic systems, software and staffing?\n\n\n\nPotential risks\n\nHas their been sufficient investigation into identifying potential risks?\nAre there systems or processes in place to make sure ethical issues are attended to throughout a PA project?\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Defining PA for improving services",
      "Is predictive analytics a good fit for your context?"
    ]
  },
  {
    "objectID": "paforimprovingss_limitations.html",
    "href": "paforimprovingss_limitations.html",
    "title": "What are the limitations of PA for improving services?",
    "section": "",
    "text": "While PA has the potential to provide valuable insights for improving services, it also has many limitations that data scientists need to consider when scoping, implementing and communicating plans and results. The following provides a brief overview of some limitations of PA. We will return to these in more depth later.\n\nPredictions are only as good as the data on which they are built. If the dataset available for analysis is incomplete, inaccurate, or biased, it can lead to unreliable predictions.\nThere can be a trade-off between model performance and transparency and simplicity. Sometimes the best-performing models are complex machine learning algorithms and can therefore be difficult to explain and can be challenging to deploy and maintain. A lack of transparency can be especially problematic in certain domains with more ethical considerations.\nExternal factors may change: Predictive models rely on patterns in the past to make predictions about the future. But models may not account for unexpected events, external influences, or sudden changes in the environment that can substantially impact the outcome being predicted. As the time horizon extends further into the future, models may become unreliable. For this reason, predictive models need regular monitoring and may require frequent updating.\nPredictive analytics tells us nothing about causal relationships. Nor does it tell us that those with the lowest or highest probability of an outcome are the most likely to benefit from a new or revised service or intervention. This evaluation is determined by other analyses (e.g. rigorous evaluations), or is sometimes assumed based on the expertise of those using predictive analytics findings for decision-making.\nEthical Concerns: Predictive analytics can raise many ethical concerns. Ethics is a very important topic, which we turn to next and will return to throughout the course.\n\nMany of the lessons we learn in inferential statistics do not apply to predictive analytics. You can find a short discussion of some of the differences between predictive analytics and inferential statistics here. This is OPTIONAL.\nAnd for fun, Baba Brinkman covers some of these issues in his rap comparing and contrasting data science and statistics. (Click link below.) (Also OPTIONAL)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Defining PA for improving services",
      "What are the limitations of PA for improving services?"
    ]
  },
  {
    "objectID": "paforimprovingss_define.html",
    "href": "paforimprovingss_define.html",
    "title": "What is predictive analytics (PA)?",
    "section": "",
    "text": "Predictive analytics (PA) (or predictive modeling) is the use of historical data to forecast future outcomes.\n\n\n\n\n\n\n\nThe complexity of PA may vary. For example, PA may rely on relatively simple approaches, such as models with a small number of measures (i.e. predictors, variables). Alternatively, in settings with large amounts of data, they may involve complex machine-learning algorithms.\nPA may make predictions of binary outcomes, which take just two values such as yes or no. For example, we can predict whether students will graduate from high school on time or not. With binary outcomes, predictive analytics can also be used to estimate the probability of the outcome occurring. For example, we can predict the probability that a student will graduate from high school. This probability is between 0 and 1.\nPA may also forecast continuous outcomes. For example, we can predict the grade point average (GPA) students will achieve.\nTo narrow the scope of the class, we will focus only on binary outcomes.\n\n\n\n\n Back to top",
    "crumbs": [
      "Defining PA for improving services",
      "What is predictive analytics (PA)?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Predictive Analytics for Improving Services"
    ]
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Overview",
    "section": "",
    "text": "These materials are designed to help you understand the main topics we’ll cover in class. It’s best to read them before each class, so we can use our time to review important points and discuss any questions. I’ll let you know what to read before each class.\n\nHow to use these materials\nThe class is organized into the sections listed in the left panel of this page. Clicking the arrow to the right of each main section title will show the sub-sections. Generally, we will cover one or two main sections per class. Some sections are currently empty, but will be added as the class progresses.\nThe materials consist of text, links (to articles, blog posts, videos, etc.), and snippets of the code we will use for analyses. Please note:\n\nYou should use the hyperlinks in these materials unless otherwise noted. Most are short.\nIf hyperlinks are optional, this will be clearly stated. They provide an opportunity to do a deeper dive into a topic if you are interested.\nIn addition, you will see citations throughout. There is no expectation for you to read the cited papers unless you want to.\n\n\n\nAcknowledgements\nThese course materials were put together by me, Kristin Porter. All errors and omissions are mine. But I have also had lots of help - many sections draw on and even copy valuable text written by Zarni Htet at MDRC and Kristen Hunter at the University of South Wales, Sydney. The framework presented in this class was developed at MDRC, a nonprofit social policy research organization where I was employed for 16 years. Kristen Hunter also provided very helpful review of all these materials.\nThis is the first time I have taught this course, and I would not be surprised if there are problems. I very much welcome any and all feedback and questions at any time.\n\n\n\n\n Back to top",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "paforimprovingss_learn.html",
    "href": "paforimprovingss_learn.html",
    "title": "How can PA be used to improve services?",
    "section": "",
    "text": "Predictive analytics has a wide variety of applications, from predicting financial markets to personalized medicine. The potential of predictive analytics in the services sector is to help government programs, nonprofit service providers or businesses identify those clients who could most benefit from targeted interventions. This targeting can facilitate effective service delivery at an efficient cost.\nWhy just focus on predictive analytics for improving services? Predictive analytics has vast applications. Different applications raise different issues with respect to project scoping, ethics and data and technical considerations. We focus on improving services because it allows us to build a framework that is driven on how results will be used. Also, my background is in using predictive analytics (as well as other data science and statistical methods) to strengthen social services, so it is the application to which I have given the most thought. But at the same time, many of the lessons we will learn will be helpful for other applications as well, and there will be flexibility in the data you use for your projects.\n\nSome examples of using predictive analytics to improve social services:\nDuring my time at MDRC, the nonprofit research organization from which the framework for these materials originated, my colleagues and I explored and applied predictive analytics in various ways:\n\nPredicting K-12 academic outcomes: School districts often utilize “early warning systems” (EWS) that rely on student-level data. These systems help identify students who might be at risk of not achieving important educational milestones, such as timely graduation or passing state exams. For instance, the probability of not graduating high school is often gauged using the ABC indicators, which stand for attendance, behavior, and course performance. This indicator system represents a simpler form of predictive analytics. However, as schools increasingly collect richer, data sets with frequent data updates of many student measures, including daily attendance, exam scores, and course marks, there has been opportunity to compute more accurate, frequent and nuanced predictions of student risk.\nProviding early warnings of not reaching milestones in an employment training program. This project focused on harnessing granular, longitudinal administrative data to build a system for ongoing, advanced analytics that support the continuous improvement process at the Center for Employment Opportunities (CEO). The goal was for these early warnings to be transmitted, practically in real time, to front-line case workers and leaders. The information would be part of standard dashboards and data protocols. CEO planned to train staff members to act on this information, and to work with MDRC to design, implement, and test new interventions based on insights provided by the predictive analytics results. Unfortunately, this project was interrupted by the COVID-19 pandemic.\nIdentifying TANF participants who were most likely to find employment. TANF is the Temporary Assistance to Need Families program that provides time-limited support for families’ basic needs. The federal government provides grants to states to run the TANF program. As part of MDRC’s TANF Data Collaborative Project, a team at the Virginia Department of Social Services (VDSS) sought to develop analytic tools to help TANF case workers customize education/employment-related services to increase the likelihood of participants’ labor market success after they leave the program. The team investigated whether demographic characteristics, household compositions, receipt of other public benefits, and past education/employment-related activities could predict success, and how to construct an unbiased predictive tool using such variables.(MDRC, 2023)\nIdentifying families at risk of disengaging from a home visiting program. Child First is a home visiting program that aims to promote high-quality relationships between caregivers and children in families experiencing challenges related to caregiver mental health and child behavior. Staff members provide intensive in-home clinical services to both the caregiver and child. They also connect families to additional services such as financial and housing support, health care, and treatment for disorders such as substance abuse. To accomplish its goals, Child First must ensure that families remain consistently engaged in program services over time. When families leave before being officially discharged from the program, they receive truncated interventions that are likely to be less effective at improving outcomes. Early disengagement is also expensive, given the large, fixed costs of enrolling new families into the program. Child First prioritizes collecting high-quality data to understand the population of families it serves. For example, the program collects information on a range of family characteristics assessed at intake, including sociodemographic information on both the caregiver and child, health-related information like insurance status and child DSM-5 diagnoses, as well as several risk measures and assessments such as adverse childhood experiences (ACEs) that capture experiences with violence, abuse or neglect, and household instability. The availability of these data provided researchers with a singular opportunity to explore whether predictive analytics could be a useful tool to summarize the large amounts of information the program collects and use it to help staff members identify families at particularly high risk of early disengagement, defined as being enrolled in the program for fewer than 90 days. That information could allow Child First staff members to triage families better at intake and provide more intensive support and services to those families at risk of early disengagement.(Xia, Htet, Porter, & McCormick, 2023)\nPretrial justice: Many jurisdictions across the United States are rethinking the “front end” of the criminal justice system — the pretrial period between an arrest and the disposition of a criminal case. Often these reforms focus on the initial decisions that judges and other court stakeholders make about whether to detain individuals in jail while they are awaiting trial, and on the use of money bail as a tool for ensuring that people will show up to court hearings. In most jurisdictions, the majority of people in jail at any point in time are awaiting trial, and many are there because they cannot afford to post bail. Jurisdictions are looking for fairer, more cost-effective approaches to the pretrial phase of the system. To assist jurisdictions in making better initial decisions, the Laura and John Arnold Foundation (now Arnold Ventures) developed the Public Safety Assessment (PSA), a tool that uses data on an individual’s history with the justice system and the current offense to predict the probability that the person will show up to hearings or be arrested for a new crime if released. The PSA aims to help judges make more informed, less subjective decisions about pretrial detention. It is currently used in nearly 40 jurisdictions across the nation.(Redcross & Henderson, 2019) (Golub, Redcross, & Valentine, 2019)\n\nLater, we will return to some of the above examples to discuss various issues, including ethics, project scoping or set-up, the trade-offs of different modeling approaches, and more. Here are some other examples of predictive analytics being used in the social service sector with the goal to strengthen programs’ services:\nChild welfare: An example of PA being used in child welfare is the Allegheny Family Screener Tool. First implemented in 2016, it was developed in a partnership between researchers from Auckland University of Technology and the Allegheny County Office of Children, Youth, and Families, a Pennsylvania child welfare system. The research team wanted to use predictive analytics to help inform and improve decisions made by staff when determining whether reports of possible child abuse and neglect should be marked for further investigation, rather than replace human decision making altogether. The tool summarizes vast amounts of information across multiple databases to provide a risk score to child welfare call screeners. The researchers worked closely with the child welfare agency and partner organizations to discuss implementation of the tool and results, and feedback from community meetings informed how the tool was developed. An independent evaluation found that it increased the staff’s ability to accurately screen reports and pursue investigations. Also, the tool did not increase the rate of children screened in for investigation. That is, using it resulted in a different pool of children being identified as needing child welfare intervention, but did not substantially increase the proportion of children investigated among all children referred for maltreatment. The model and its implementation have been updated over time so that its predictions reflect contemporary information on families currently being served.(Human Services, 2019)\nLead poisoning prevention: The Chicago Department of Public Health partnered with the Data Science for Social Good initiative at the University of Chicago to help find the homes that are most likely to still contain lead-based paint hazards. From their website: “By building statistical models that predict exposure based on evidence such as the age of a house, the history of children’s exposure at that address, and economic conditions of the neighborhood, CDPH and their partners can link high-risk children and pregnant women to inspection and lead-based paint mitigation funding before any harm is done.” This integrated and innovative system will ensure resources are used most efficiently, and ultimately will mean healthier Chicago children. This short video summarizes the project.\nCriminal justice: In addition to applications in the pretrial periods, predictive analytics is being applied to other parts of the criminal justice system. For example, predictive analytics has been used for “predictive policing” based on forecasting future crime at the community level, for guiding sentencing and probation, for detecting fraud, for assessing young people’s risk of becoming involved in crime and more. As you are surely aware, bias is an enormous concern and this topic alone could take up a whole courses. We will not have the time to delve is as deep as is warranted due to time constraints, but we will discuss this topic later, and I will provide some ideas for further reading. For now, this semi-recent New York Times article provides a brief overview of some applications and concerns about bias. As an optional, longer read, you can check out this Pro-Publica story about algorithmic bias in sentencing.\nHealth care: The adoption of electronic health records (EHRs) by most US health care systems for patient care has led to an explosion of predictive analytics in health care - with applications aimed at improving health outcomes, care coordination, and quality of care. Health care systems and insurance companies harness patient demographics, insurance claims data, and clinical characteristics in EHRs to create statistical models of future health care risks and resource utilization. There are also efforts to incorporate social and behavioral determinants of health (SBDH), which include measures of diet and physical activity as well as characteristics of patients’ neighborhoods, such as food access and transportation.\nThe above examples do not capture the breadth of growing applications of using predictive analytics to improve social services. To read about some additional examples, see the following short articles and blog posts. These are OPTIONAL.\n\nAnticipatory government: Preempting problems through predictive analytics\nCatalogue of predictive models in the humanitarian sector\nWhat is predictive analytics and what could it mean for local governments?\n\n\n\nSome examples of using predictive analytics to improve consumer services:\nI do not have first-hand experience in using or reviewing predictive analytics to improve consumer services, but of course there are many! Applications in social services have been inspired by applications in consumer services. Here are a few articles that provide some good summaries and discussion. Not all applications discussed focus on estimating binary outcomes, but many, such as predicting customer churn, product purchase or customer satisfaction can be examples of predicting binary outcomes.\n\nPredictive customer insight is the future\nHow predictive analytics can improve customer experience\n\nQuestions for discussion:\n\nWhat examples do you find interesting for using predictive analytics to improve services? Why?\nWhat promise do you see in the examples you read about or know about?\nWhat concerns do you have?\n\n\n\n\n\n\n Back to topReferences\n\nGolub, C., Redcross, C., & Valentine, E. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment effects of new jersey’s criminal justice reform.\n\n\nHuman Services, A. C. D. of. (2019). Impact evaluation summary of the allegheny family screening tool.\n\n\nMDRC. (2023). Virginia pilot: TANF data collaborative.\n\n\nRedcross, C., & Henderson, B. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment.\n\n\nXia, S., Htet, Z., Porter, K. E., & McCormick, M. (2023). Exploring the value of predictive analytics for strengthening home visiting: Evidence from child first.",
    "crumbs": [
      "Defining PA for improving services",
      "How can PA be used to improve services?"
    ]
  },
  {
    "objectID": "paforimprovingss_ethics.html",
    "href": "paforimprovingss_ethics.html",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors.",
    "crumbs": [
      "Defining PA for improving services",
      "What are the ethical considerations for using PA to improve services?"
    ]
  },
  {
    "objectID": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "href": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these topics as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely, and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly considered in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveillance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors.",
    "crumbs": [
      "Defining PA for improving services",
      "What are the ethical considerations for using PA to improve services?"
    ]
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressEthical",
    "href": "paforimprovingss_ethics.html#addressEthical",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders. As much as possible, we should try to include the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices that may be helpful or necessary, depending on the project:\n\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and helps ensure that ethical considerations are thoroughly explored from various perspectives.\nCode of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nRegulatory Frameworks: Teams should be well-versed in relevant regulations and laws that may apply to their data and context, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nBias Minimization: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nProject Assessments: Conduct comprehensive PA project assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This training ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these kinds of practices (when they apply to the context), government agencies, organizations and companies can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms.",
    "crumbs": [
      "Defining PA for improving services",
      "What are the ethical considerations for using PA to improve services?"
    ]
  },
  {
    "objectID": "scoping_ex.html",
    "href": "scoping_ex.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "A motivating example\nLet’s begin with an example. This example is inspired by a project I was involved in, but I have made several modifications for illustration purposes. The project was in collaboration with a state agency overseeing the federal assistance program, Temporary Assistance for Needy Families (TANF). TANF’s primary goal is to empower families towards self-sufficiency by providing essential resources and services. Within this framework, state agencies administering the program extend a range of supportive services. These services are designed to assist families in overcoming obstacles to employment and self-reliance. Their approach revolves around tailoring assistance to the specific circumstances and needs of each family, ensuring a targeted response to diverse challenges and varying levels of need among households.\n\n\n\n\n\n\nThe agency came to the data science team with the following questions:\n\nWith a data-driven tool, could caseworkers better understand which clients were more at risk of not reaching key program outcomes? In particular, among those who were required to find employment, could they obtain reliable information about who was least likely to find employment? Note that the current practice for identifying clients who were most at-risk relied on caseworkers reviewing measures they thought were important (e.g. of previous employment history) and making a subjective decision based on their professional expertise. They wondered if predictive analytics would provide an improvement in terms of accuracy and fairness.\nCould we develop reliable risk scores that combine and weigh multiple risk factors into a single easy-to-understand number that can be incorporated into decision-making?\nCould trends in risk scores across the population of TANF clients provide new insights for program improvement and resource allocation?\n\n\n\nHow do we scope a project that helps them achieve their goals?\nNext, we will use this example to talk about project scoping. That is, we need to specify the following:\n\nHow would information about clients’ risks of adverse outcomes be used?\nWhat, exactly, are we trying to predict?\nFor whom would we be making predictions?\nWhen would we be making the predictions?\nWhen specifying what, when and for whom we are making predictions - what data are available? What do the data allow?\n\nAnswering these questions allows us to move forward in planning a potential predictive analytics project. That is what I mean by scoping - what potential project could there be? But importantly, the next step is to investigate that potential - to see if we can reliably, clearly and fairly make useful and actionable predictions. I refer to this next step as the “proof-of-concept,” which we will turn to in the next main section.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Scoping a PA project",
      "A motivating example"
    ]
  },
  {
    "objectID": "scoping_what.html",
    "href": "scoping_what.html",
    "title": "What are we trying to predict?",
    "section": "",
    "text": "It is important to be very thoughtful and precise in defining and developing a measure of the outcome to be predicted. In our motivating example, we want to predict employment, but how exactly should we define and measure employment? When specifying an outcome, the following considerations are essential:\nIn our example, we will focus on predicting just one outcome measure, but multiple measures of employment that make sense for the goals of the department could be analyzed. For each outcome, the analytic procedures would need to be repeated.\nOur outcome: The selected precise outcome measure for our example is: whether a client holds a job covered by unemployment insurance (UI) for the first three quarters after exiting the department’s education and training program for TANF participants. The measure is coded as 1 when achieved and 0 otherwise.\nThis choice was made because of the following answers to the above questions:\nHowever, after defining this measure, the project decided to code this outcome in the negative – so that we were estimating probabilities of whether someone does not achieve the employment outcome measure defined above. Therefore, our predicted probabilities represent risk scores. (I initially framed it as predicting employment because it is much easier to describe that way.)\nAlso, when discussing this project with stakeholders, we want to emphasize sensitivity to clients’ experiences. While we focus on “success” definitions that inform our measures (subsequently turned into “risk” scores), we avoid implying “failure” when clients fall short of departmental employment goals. We acknowledge the multifaceted challenges clients encounter and the program’s mission to assist those facing elevated barriers.",
    "crumbs": [
      "Scoping a PA project",
      "What are we trying to predict?"
    ]
  },
  {
    "objectID": "scoping_what.html#footnotes",
    "href": "scoping_what.html#footnotes",
    "title": "What are we trying to predict?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs summarized by the Employment and Training Administration in the U.S. Department of Labor: These data are from the Unemployment Insurance Data Base (UIDB) as well as UI-related data from outside sources (e.g., Bureau of Labor Statistics data on employment and unemployment and U.S. Department of Treasury data on state UI trust fund activities). See this link for more details.↩︎",
    "crumbs": [
      "Scoping a PA project",
      "What are we trying to predict?"
    ]
  },
  {
    "objectID": "scoping_when.html",
    "href": "scoping_when.html",
    "title": "When are we making predictions?",
    "section": "",
    "text": "Deciding when to make predictions is not always straightforward, but is critical to developing a valid prediction model. \nThe key is to balance two important considerations:\n\nFirst, we want to run predictive models early enough for the findings to be actionable. In our case, we want our caseworkers to be able to help clients early enough that they can address challenges.\nBut running predictive models later may mean we have more data measures available to include in our models and therefore potentially have more accurate models.\n\n\n\nIn our TANF program example, through conversations with the program managers and caseworkers on our project team, we learned (a) that interventions should be introduced very early, so they wanted risk scores available soon after client intake and approval for the education and training program. But we also learned (b) that there can be substantial lag in the entry of most of data collected during the intake and approval processes for TANF clients.\nWe therefore decided to specify our prediction timepoint as one month after the approval process for the education and training program.\n\nThis allowed time for most measures collected up through approval to be entered into the system.\nThis means that our models can only include measures that would feasibly be entered before our timepoint.\nIf the model is deployed, we would need to be careful to wait to apply the model to new data only at the correct timepoint – not earlier. Otherwise, we would have a lot of missing data, which could lead to poor predictions. We will discuss this topic more later.\n\n\nIt is a common mistake to not give adequate thought to when predictions are being made and to which measures are available at that time. If we ignore this issue, we might generate our risk scores (a) too early, which means our models might use measures that would not yet be available in practice, or (b) too late, which means our models might use measures that are collected very close to the outcome.\nFirst, we discuss (a), generating risk scores too early in the process. If we are not careful about understanding not just the data, but the data collection process, then we might accidentally use data that would not realistically be available at our chosen time point. If we had not spoken in depth with the program managers, we would not have known that client intake data was not immediately available, but instead could take a few weeks to be input into the system. As discussed above, if we make this mistake, our models would look effective in our test phase, but would have poor performance in practice due to missing data.\nSecond, we discuss (b), generating risk scores too late in the process. This strategy could lead to excellent performance - great accuracy - but useless predictions. To illustrate this type of mistake, consider if our data includes a measure of whether a TANF client has submitted a job application at the end of a job preparation class. Including this measure in our model could improve its predictive performance. However, because this measure is collected towards the end of the education and training program, if we wait to generate risk scores after this measure is collected (setting our timepoint at the end of the education and training program), there is little time left to take action to help those at highest risk of not finding and sustaining employment.\nThink about this issue when you read or hear about the high accuracy of predictive models. If a company boasts that it predicts an outcome with very high accuracy, it is important to ask:\n\nWhen are predictions being made?\nIs it possible that measures are collected so close to the outcome that some predictors are actually proxies for the outcome?\nWhat trade-offs are being made in terms of predictive accuracy versus having time to act on predictive analytics results?\n\n\n\n\n Back to top",
    "crumbs": [
      "Scoping a PA project",
      "When are we making predictions?"
    ]
  },
  {
    "objectID": "poc_learners.html",
    "href": "poc_learners.html",
    "title": "Defining learners",
    "section": "",
    "text": "Here we present a framework for conducting a PA proof-of-concept. The framework focuses on specifying and comparing “learners.”\nA learner is a combination of:\n\nA predictor set: a set of measures that are available at the prediction timepoint and that are potentially predictive of the outcome of interest.\nA modeling approach: a method for combining and weighting measures, e.g., regression or a particular machine learning algorithm.\n\n\n\n\n\n\nMachine learning algorithms automate model building using a series of steps that are driven by patterns in the data - rather than relying on functional forms specified by an analyst. Examples include decision trees, random forests, stepwise regression, and support vector machines, among many others.\n\n\n\n Back to top",
    "crumbs": [
      "Conducting a PA proof-of-concept",
      "Defining learners"
    ]
  },
  {
    "objectID": "poc_compare.html",
    "href": "poc_compare.html",
    "title": "Adding and comparing learners",
    "section": "",
    "text": "Once we have specified our benchmark learner, we can specify additional learners for comparison. If simplicity and transparency are goals, we will want to incrementally introduce complexity. For example, for our high school graduation example, we might take the following approach:\nThis results in a total of nine learners because we have nine combinations of predictor sets and modelling approaches. Ultimately, we will then have nine predictive models that result from the nine learners.\nNote: Machine learning algorithms typically require specifications for how they are implemented. These are called “tuning parameters.” For example, the random forest algorithm has tuning parameters that include the number of decision trees in a forest, the maximum depth of the decision trees, and several more options. Different values of tuning parameters have different implications for the performance of the algorithm. While the analyst can specify these tuning parameters, it is typically best to use a data-driven approach. We will dive more into this later. However, the point to take away here is that we will “tune” each machine learning algorithm for each predictor set. So when we ultimately compare learners, we are comparing “tuned” versions of them.",
    "crumbs": [
      "Conducting a PA proof-of-concept",
      "Adding and comparing learners"
    ]
  },
  {
    "objectID": "poc_compare.html#footnotes",
    "href": "poc_compare.html#footnotes",
    "title": "Adding and comparing learners",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDemographic information like race is excluded for ethics reasons. This decision will be discussed more in-depth when we discuss algorithmic bias.↩︎",
    "crumbs": [
      "Conducting a PA proof-of-concept",
      "Adding and comparing learners"
    ]
  },
  {
    "objectID": "data_split.html",
    "href": "data_split.html",
    "title": "Data workflow",
    "section": "",
    "text": "Here we will define the learner workflow for a PA proof-of-concept. This involves the following three steps:\n\n\n\n\n\nflowchart LR\n  A(Training) --&gt; B(Validating)\n  B --&gt; C(Testing)\n  \n  \n\n\n\n\n\n\n\n\n\nTraining: Uses data with known outcomes to apply a modeling approach (e.g., regression or a machine learning algorithm) in order to build (i.e., estimate, fit, train) a model of the relationship between predictors and the outcome of interest.\nValidating: uses new data with known outcomes (data not used for training) to apply all trained learners/models. The predictions from the trained learners/models are compared to the known outcomes so that metrics of model performance and fairness are computed. This validation can be repeated in multiple validation data sets. Looking across all results, the “best” model is selected. Criteria for a model being selected as “best” vary by context. To determine the “best” learner/model, the team will weigh different definitions and metrics of predictive performance and of fairness. We will take a deep dive into these metrics soon. The team may also weight the simplicity and transparency of the learner, as well as variability across multiple validations.\nTesting: uses new, set aside data with known outcomes (data not used for training or validation) to apply the “best” model. The predictions from the best model are compared to the known outcomes to report on predictive performance and fairness to stakeholders and to make a decision about whether the learner/model should be deployed for use.\n\nIn the following pages, we will go over how to implement these concepts, but first we will discuss considerations for selecting data that would be used across the full workflow.\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Data workflow"
    ]
  },
  {
    "objectID": "data_test.html",
    "href": "data_test.html",
    "title": "Designating data for testing",
    "section": "",
    "text": "Once we have identified the appropriate data for all steps (train, validate, test) of our PA proof-of-concept workflow, we want to work backwards. So first, we want to split off a subset that will be used for testing. Some notes about designating the testing data:\n\nThe test data will not be analyzed until we have completed learner training and validation and have selected our “best” model. We should set it aside and not touch it until we have made a final learner selection.\nIn the case that the goal of predictive analytics is to make predictions for *future* service recipients, then ideally, we want our test data set to include individuals (or other units) that are forward in time, compared to the data we will use for training and validation. This provides the best check of how well our “best” model generalizes not only to different data, but also to the future.\nIdeally, we would want data as close in time as possible to when we would deploy a predictive model. This might be tricky. For example, if our proof-of-concept is assessing possible deployment for predicting high school drop out risk of 10th graders in the 2024-2025 school year, then the best test data may be the most recent cohort of 10th graders for whom we know graduation status. This would be students who graduated in 2023 and were therefore 10th graders in the 2020-2021 academic year.\nIn some cases, we might be making predictions for a new location (e.g. for a new site, office, store). Then, we might designate testing data that is from a different location than data we will use for training and validation.\n\nUltimately, for an honest assessment of our selected, “best” model, we want to conduct testing in a test data set that best mimics how the predictive model would be deployed.\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Designating data for testing"
    ]
  },
  {
    "objectID": "data_splitconsiderations.html",
    "href": "data_splitconsiderations.html",
    "title": "Additional considerations",
    "section": "",
    "text": "How much data do you need?\nUltimately, the goal is to have a large enough training set to train the models effectively, and large enough validation and test sets to get reliable estimates of the models’ performance. The definition of “large enough” can vary based on:\n\nModel Complexity: More complex models (e.g., deep neural networks with many layers and parameters) typically require larger training datasets to avoid overfitting while simpler models (e.g., linear regression, decision trees) may require smaller training datasets.\nData Variability: If the data has a lot of variability or noise, a larger dataset may be needed to capture the underlying patterns and relationships.\nBalance between the classes of a binary outcome: This is a specific case of #2. If one class is much more common than the others, you may need a larger dataset to ensure that the model is exposed to enough examples of each class during training.\nAvailability of Data: Sometimes the amount of available data is a limiting factor. In such cases, it is important to make the best use of the available data and adjust the methods accordingly.\n\nUltimately, if the learner/model performs well in validation and generalizes well to the test data, the dataset is likely large enough. However, if the model overfits to the training data, one reason may be that the modeling approach was too complex for the size of the available data. But there may be other explanations as well - e.g. lack of data consistency over time as discussed a few pages back.\n\n\nWhat ratios should be designated for training, validation and testing?\nThere is no one-size-fits-all answer to this question. A common starting point is a 60% training, 20% validation, and 20% testing split. However, the size of your dataset may necessitate a different allocation. For instance, a very large dataset might allow for a 50% training, 25% validation, and 25% testing split. Conversely, a smaller dataset might require a larger proportion for training, such as a 70% training, 15% validation, and 15% testing split. As previously mentioned, we will be combining the training and validation data to conduct \\(v\\)-fold cross-validation.\n\n\nHow much do all these decisions matter?\n\nNavigating the maze of decisions required for training, validating, and testing can be both confusing and frustrating, especially when guidance is not clear. Here are some key takeaways:\n\nAlways set aside a test set: The most crucial practice is to reserve a test set until a final learner has been selected. This allows for a fair evaluation of your final predictive model’s performance on new, unseen data. Failing to adhere to this practice is very problematic.\n\nRelatedly, remember that metrics of model performance, which we will discuss shortly, are estimates and come with inherent uncertainty. In other words, if you measured model performance using different samples, your results would vary. This uncertainty is even more pronounced with a smaller testing sample size. Although it is often overlooked, a good practice is to estimate this uncertainty, for example, by calculating the standard error or confidence interval of the performance metric estimate.\n\nFocus on the ultimate goal: Throughout the entire process, keep the ultimate goal in mind: to find a model that generalizes well to new, unseen (and often, future) data so that we can provide insights for improving services. With this ultimate goal in mind, ensure that your test set is representative of the data you will use in deployment. Otherwise, your results may be overly optimistic, even if you adhered to the first point above. Additionally, ensure that your training and validation data are as similar as possible to the data you will ultimately use for deployment.\nHow much does the set-up of the training and validation matter?: The specific details of how you set up tuning and validation are important for model selection but probably will not completely derail your project. The worst-case scenario is that you miss the opportunity to select the “best” model, but it is unlikely that you will choose the “wrong” model. Poor performance is more likely due to the limitations of your data and context rather than the choices made in setting up cross-validation.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 1",
      "Additional considerations"
    ]
  },
  {
    "objectID": "data2_whymissing.html",
    "href": "data2_whymissing.html",
    "title": "Overview of missing data",
    "section": "",
    "text": "It is rare for data to be complete for all observations and all variables. Missing data is a challenge for just about any data analysis. In R, missing data often looks like what we see in the following example. Here, missing information is indicated by NA. Missing information may be indicated in other ways too (e.g. a specific numeric value outside a variable’s range, or ” “). These cases would need to be translated to R’s convention of using NA during data cleaning.\nIn the toy data set below, we see a missing value for the 3rd student’s math score and corresponding proficiency level. We also see a missing value for whether the 5th student graduated on time.\n\n\n  ID School MathScore MathProficiency GradOnTime\n1  1      A        40               1          1\n2  2      A        60               2          0\n3  3      B        NA              NA          1\n4  4      B        80               3          0\n5  5      B        60               2         NA\n\n\nAcross many types of data, information may be missing for a variety reasons, including:\n\nNon-Response: When individuals respond to a questionnaire, fill out an application, etc., some may choose not to answer certain questions due to the sensitivity of the topic, lack of knowledge, or other reasons. This can lead to missing values.\nData Entry Errors: Mistakes made during data entry can result in missing values. Data entry errors - due to typos, misinterpretation of handwriting, or glitches - can also lead to nonsensical information, which may need to be converted to missing.\nTechnical Issues: Technical problems such as software crashes, network failures, or sensor malfunctions can lead to missing data points, particularly in real-time data collection scenarios.\nPrivacy and Confidentiality Concerns: In some cases, data might be intentionally omitted or masked to protect individuals’ privacy or sensitive information. For instance, certain personal identifiers might be removed from the dataset.\nSkip Patterns: In surveys or questionnaires, skip patterns are used where respondents are directed to skip certain questions based on their previous responses. This can lead to missing values for respondents who don’t meet the criteria for certain questions.\nData Collection Costs: Collecting certain types of data can be expensive and time-consuming. As a result, researchers might prioritize certain data points over others, leading to missing values in less prioritized areas.\nData Migration or Integration: When merging data from different sources or migrating data to a new system, compatibility issues might result in missing values or discrepancies.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 2",
      "Overview of missing data"
    ]
  },
  {
    "objectID": "data2_missingoutcomes.html",
    "href": "data2_missingoutcomes.html",
    "title": "Missing data - outcomes",
    "section": "",
    "text": "Outcome values could be missing for multiple reasons:\n\nThe outcome has not been sufficiently defined. For example, imagine the case in which a company wants to predict whether a customer signs up for an advertised service. The binary outcome for the outcome may be equal to 1 if a customer signs up, and 0 if they decline to sign up. But what about those who haven’t indicated “yes” (1) or “no” (0) yet. The may have a missing value. It is critical here to define a window of time for observing the outcome. That is, we need to define by when the sign-up occurs (e.g. within 1 month). Then the missing values would take the value of 0, indicating that “no” the customer did not sign up for the services within 1 month of advertising it.\nAll the same reasons predictors can be missing. If an outcome is missing due to “random” typos, technical glitches or messiness, then this is probably not a problem. The corresponding unit/observation would have to be excluded from modeling. However, if an outcome variable’s missingness is nonnegligible and due to nonrandom reasons (e.g. some individuals were hesitant to report the information for the outcome variable), then this could introduce a big problem for the analytics. If the outcome is observed for a subgroup that differs from the overall population in your data in some meaningful way, then your modeling will only apply to that subgroup. And, if you can only identify that subgroup by nonmissing outcome data, then you will be applying that model to make predictions for a broader population, introducing unreliable results when the model is deployed with data in which the outcomes are unknown.\nThe outcome cannot be observed for a subgroup. This another version of #2 but occurs somewhat differently. Consider the following example: Across the country, release and detention decisions for defendants in the pretrial period — that is, the period after arrest while a criminal case is being adjudicated — are increasingly guided by risk assessments, which rely on data to estimate defendants’ risk of failing to appear for a court date or of being charged with new criminal activity if released pending trial. In this setting, predictive models are generally fit to only those defendants not detained while awaiting trial. Those detained, potentially because they were unable to make bail, are not included as there is no outcome related to new criminal activity or failure to appear (this is referred to as “censorship”). This censorship can cause multiple problems. First, if the people who are detained are systematically different from the people who are not detained, the final models may not generalize: the models may not accurately predict risk for those people who were detained. Additionally, if detention patterns differ by racial group, bias may be introduced by fitting models using different subsets of the racial groups. One of the approaches to address the censoring problem when validating pretrial risk assessments is to impute missing outcomes for detained or partly detained defendants. This imputation could include race and other characteristics not used for the final risk model. (As imputation is only used to build the model, these defendant characteristics will not, in the end, be used as risk factors by the final predictive model and thus the overall risk assessment.) As long as the imputation captures any differential relationships between detention and subgroups, the subsequent model-fitting process will not be as vulnerable to biases from censoring.\n\n\n\n\n Back to top",
    "crumbs": [
      "Data for PA: PART 2",
      "Missing data - outcomes"
    ]
  },
  {
    "objectID": "data2_extract.html",
    "href": "data2_extract.html",
    "title": "Extracting predictive value",
    "section": "",
    "text": "Here we discuss data preparation steps focused on extracting helpful information from data in order to enhance the performance of our predictive models. This set of steps is often referred to as “feature engineering” (as predictors are often referred to “features.”) The data preparation process can be guided by EDA in some cases, but it also involves domain knowledge, intuition, and experimentation to represent data in a manner that makes it more suitable or informative - often leading to improved model accuracy and generalization. After addressing missingness according to the guidance on the previous pages, data preparation, or feature engineering, involves the following:",
    "crumbs": [
      "Data for PA: PART 2",
      "Extracting predictive value"
    ]
  },
  {
    "objectID": "data2_extract.html#footnotes",
    "href": "data2_extract.html#footnotes",
    "title": "Extracting predictive value",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMean centering supports numeric stability for machine learning algorithms that involve matrix inversion or optimization by scaling down the numbers. Algorithms that rely on gradient descent can converge faster when data is centered and normalized because it shapes the cost function into a more bowl-like form, making it easier to reach the global minimum. Some algorithms assume that the data is centered around zero. For instance, LASSO and Ridge regression add penalty terms based on the magnitude of the coefficients. If the predictors are not centered, the penalty might be applied in a biased manner because it would be influenced by the scale and mean of the predictors. Also, many machine learning algorithms, such as Support Vector Machines, k-means clustering, and deep learning networks, work better or converge faster when data variables are standardized to z-scores. Mean centering and standardizing might not always be necessary or beneficial. It largely depends on the context, the nature of the data, and the specific algorithm being employed. For example, in tree-based algorithms like Decision Trees or Random Forests, mean centering usually doesn’t offer any advantage because these algorithms are scale-invariant.↩︎",
    "crumbs": [
      "Data for PA: PART 2",
      "Extracting predictive value"
    ]
  },
  {
    "objectID": "methods_overview.html",
    "href": "methods_overview.html",
    "title": "Overview",
    "section": "",
    "text": "In this section, we focus on modeling methods. We begin with a short discussion about how logistic regression can be used to predict binary outcomes. Then, we take a quick tour through some prevalent machine learning (ML) algorithms.\n\n\n\n\n\n\nDefining machine learning\nMachine learning refers to models that use iterative algorithms — that is, a series of steps that continuously adjust for better predictive performance — rather than relying on functional forms (specifications of the independent variables and the types of relationships between the variables and the outcome) specified by an analyst.\nNote that machine learning is divided into two primary categories. These categories are distinguished by the type of data they handle and their objectives.\n\nSupervised Learning: Here, algorithms are trained using labeled data. Labeled data means every unit’s set of data points includes a corresponding outcome. The primary objective is to train a model that will then make informed predictions on previously unseen data. This is our task in this class. With supervised learning, there are two different tasks:\n\nClassification: This approach is employed when the outcome is categorical. The prediction might be a ‘yes’ or ‘no’, ‘0’ or ‘1’. Though we will focus on binary classification, classification can also include multi-category outcomes, such as predicting what product a user will buy next or what movie a user might like.\nRegression: This approach is used for continuous outcomes (not the focus of this course). However, for binary outcomes, which is our primary concern, regression can estimate the probability of one outcome over the other (e.g., the probability of the outcome being ‘1’).\n\nUnsupervised learning: Here, algorithms work with unlabeled data (data without an output or outcome), aiming to find hidden structures or relationships within the data. A classic application is clustering, where data points are grouped based on inherent similarities, like customer segmentation. Unsupervised techniques can complement supervised ones, for instance, in dimensionality reduction by clustering raw predictors. However, topics related to unsupervised learning are beyond the scope of this course.\n\nTo understand the difference between supervised and unsupervised learning, consider an example of classifying images that contain either cats or dogs. In supervised learning, the algorithm is given the series of images and whether the image contains a cat or a dog. A model is trained to predict whether future images contain a cat or dog. In unsupervised learning, the algorithm is given the series of images, but is given no information about what is contained in the picture. The algorithm clusters the images in two groups, with the goal of producing one cluster of cats and one cluster of dogs, but the two groups are not characterized or labelled.\n\n\nUnderstanding machine learning algorithms\nAs you will read in the pages that follow, machine learning algorithms can vary substantially in their underlying approaches. At the same time, some have very similar underlying approaches but vary in their details. Machine learning algorithms also vary in how easy they are to understand and explain - both in terms of their empirical approach and in terms of the models they produce. This lack of transparency is why many machine learning algorithms are described as “black box” modeling methods.\nThe pages that follow give a concise overviews of select ML algorithms designed for predictive tasks (supervised learning). I don’t go into a lot of details, but instead try to provide the basic intuitions for how they work. If certain aspects seem complex, I’d advise concentrating on the listed advantages and disadvantages for each method.\nNote that the code templates implement all of the machine learning algorithms for you. This means you won’t need to grapple with specific R packages or functions that have been developed for specific algorithms. However, at the end of each algorithm’s summary, I provide a corresponding R package and function, in case you are curious about diving deeper. All of the machine learning algorithms I mention can also be implemented with tidymodels, and there are many tutorials available online.\n\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Overview"
    ]
  },
  {
    "objectID": "methods_lasso.html",
    "href": "methods_lasso.html",
    "title": "Lasso",
    "section": "",
    "text": "Lasso, which stands for “Least Absolute Shrinkage and Selection Operator,” is a “penalized” form of regression. This means that it introduces a penalty to the regression model to shrink some of the regression coefficients towards zero.\nTo illustrate, imagine you’re assembling a team for a game and each player’s skill contributes to your team’s overall performance. However, for each player you add, you need to pay a cost (like a registration fee). The Lasso penalty is akin to this cost. If a player (or a predictor in our regression context) doesn’t contribute much value, you might opt to leave them out to avoid the fee. The stronger the penalty, the more selective you’d be about who you add to your team.\nIn the context of Lasso regression, this penalty pushes less important predictors’ coefficients towards zero, effectively excluding them from the model.\nIn Lasso regression, the primary tuning parameter is \\(\\lambda\\) (lambda), which controls the strength of the penalization.\n\nWhen \\(\\lambda=0\\), no penalty is applied, and the resulting model will include all predictors.\nAs \\(\\lambda\\) increases, the penalization effect becomes stronger, and more coefficients are shrunk towards zero. For a sufficiently large value of \\(\\lambda\\), all coefficients may become exactly zero.\nThe value of \\(\\lambda\\) can be selected with cross-validation with the training data.\n\nAdvantages of Lasso\n\nFeature selection: One of the key advantages of Lasso regression is its ability to perform feature or variable selection by shrinking the coefficients of the least important predictors to zero. This feature of the algorithm can be very helpful when you have a large number of predictors, and you suspect that many of them are irrelevant or redundant.\nDealing with lots or predictors: In situations where the number of predictors (\\(p\\)) is close to or exceeds the number of observations \\(n\\), classical logistic regression might overfit or might not even run at all. Lasso can be a solution in these high-dimensional settings.\nMulticollinearity: When predictor variables are highly correlated, standard logistic regression’s estimates can be very unstable. Lasso helps to alleviate “multicollinearity” (high correlation) issues by penalizing certain coefficients and pushing them towards zero.\nInterpretability: Because Lasso can zero out coefficients, the resulting model can be more interpretable than a model with many predictors. And Lasso is more interpretable than “black-box” machine learning algorithms.\nModel Performance: If there’s a concern about overfitting due to a large number of predictors, Lasso can provide a more generalized model that might perform better on out-of-sample data compared to a non-regularized logistic regression.\n\nDisdvantages of Lasso\n\nTrue model: If the true underlying model is very sparse (i.e., very few predictors truly matter), then lasso will perform very well. However, if a large number of predictors matter, lasso may not be stable.\nChallenged with highly correlated predictors: With highly correlated predictors, lasso arbitrarily chooses which to include in the model. This arbitrary selection may not matter for predictive performance but can interfere with explainability.\n\nImplementation of Lasso in R\nIn R, we implement lasso with glmnet() in the glmnet package. The code templates will do this for you.\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Lasso"
    ]
  },
  {
    "objectID": "methods_xgboost.html",
    "href": "methods_xgboost.html",
    "title": "Gradient boosting",
    "section": "",
    "text": "Gradient boosting algorithms, like Random Forest, are built on decision trees. However, gradient boosting takes a different approach for constructing trees than the Random Forest algorithm.\nThe basic idea behind gradient boosting is to build trees sequentially rather than independently. Basically, each tree is grown to correct the errors of its predecessor. First, a simple model is used to predict the target variable. The residuals (differences between the predicted values and the true values) are then computed. For binary outcomes, we actually have “pseudo-residuals”, which are the differences between the observed outcome and the predicted probability of the positive class (i.e. the predicted probability that the outcome = 1). The next tree tries to predict the error made by the previous model. The predictions from this new tree are scaled by a factor (learning rate) and added to the existing model’s predictions. This process is like taking a step in the direction that minimizes prediction error, hence “gradient” boosting.\nThese steps of are repeated multiple times. Each new tree is fit to the residuals of the current combined ensemble of previous trees. As trees are added, the model becomes a weighted sum of all the trees. To prevent overfitting, gradient boosting introduces “regularization.” As we saw in Lasso, regularization is a technique used to add some form of penalty to the model, which discourages it from fitting too closely to the noise in the training data (overfitting). One common form of regularization is “shrinkage”, where each tree’s contribution is reduced by multiplying it with a small learning rate. Gradient boosting requires careful tuning of parameters such as tree depth, learning rate, and the number of trees.\nXGBoost (Extreme Gradient Boosting):\nThe code templates you will use, use a particular gradient boosting algorithm called XGBoost. Here are its distinctive features:\n\nRegularization: Unlike the basic gradient boosting, XGBoost includes L1 and L2 regularization terms in its cost function to buffer against overfitting. L1 regularization adds a penalty proportional to the absolute value (magnitude) of the model coefficients. L2 regularization adds a penalty proportional to the square of the model coefficients.\nEfficiency: XGBoost is designed to be highly efficient. It can utilize the power of parallel processing to build trees, making it faster than many other implementations of gradient boosting.\nEarly stopping: Instead of growing a tree to its maximum depth and then pruning, XGBoost uses “max_depth” to grow the tree and stops splitting when it no longer adds significant value. XGBoost can also halt the training process if the model’s performance on a validation set doesn’t improve after a set number of rounds, preventing potential overfitting.\n\nAdvantages of XGBoost\n\nSpeed: The XGBoost algorithm is optimized to be relatively fast for a data-driven algorithm.\n\nDisadvantages of XGBoost\n\nTuning parameter sensitivity: XGBoost typically requires careful tuning of its tuning parameters to achieve the best results.\nMemory consumption: XGBoost can be memory-intensive, especially when handling large datasets.\nHandling categorical features: While the Random Forest algorithm can directly handle categorical variables, XGBoost requires them to be transformed into a numerical format - creating multiple dummies, as discussed in “Data for PA: Part 2,” (which is sometimes called “one-hot coding”).\n\nImplementation of XGBoost in R\nThis method is implemented with the [XGBoost]{https://cran.r-project.org/web/packages/xgboost/xgboost.pdf} package in R. The code templates will do this for you.\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Gradient boosting"
    ]
  },
  {
    "objectID": "methods_svm.html",
    "href": "methods_svm.html",
    "title": "Support vector machines",
    "section": "",
    "text": "Support Vector Machines (SVMs) are a set of machine learning methods that are primarily used for classification, though they can also be used for regression.\nSVM tries to find the optimal decision boundary (or “hyperplane”) that best divides a dataset into classes. “Support Vectors” refers to the data points that lie closest to the decision boundary and are the most difficult to classify. They essentially define the position of the decision boundary. In fact, even if you were to remove all the other data points and only keep the support vectors, the position of the optimal hyperplane would not change.\nSVM algorithms aims to maximize the margin around the decision boundary. This margin is defined as the distance between the decision boundary and the nearest support vector from either class. A larger margin implies better generalization ability and a lower chance of overfitting.\nDecision boundaries may be linear (separated by a straight line in 2D, a plan in 3D or a “hyperplane” in higher dimensions) or nonlinear, which gets even more complicated. When decision boundaries are nonlinear, the algorithms map the data into a higher-dimensional space (the space with polynomial and interaction terms), where it becomes linearly separable. This step is achieved through the use of “kernels.” Kernels are functions that transform the data into the required form. There are various types of kernel functions such as linear, polynomial, radial basis function (RBF), and sigmoid. The choice of kernel is important and can influence the performance of the SVM. The PA code templates support two nonlinear kernels: polynomial and RBF.\nTuning of SVM involves a regularization parameter, which determines the trade-off between maximizing the margin (the width of decision boundary) and minimizing the classification error. A smaller regularization value creates a wider margin, which might misclassify more training data points (but might generalize better to unseen data), while larger value aims for a tighter fit to the training data. Tuning parameters also include degree of the polynomial terms (quadratic, cubic, etc).\nTraditionally, SVM is designed for classification, but SVM can be used to obtain predicted probabilities for binary outcomes. SVM returns “decision values,” which are distances from the separating hyperplane. These can be translated into probabilities, by fitting a logistic to the decision values.\nAdvantages of SVM\n\nEffective with a large number of predictors: SVM tends to perform well in high-dimensional spaces.\nEfficient: SVM tends to be memory efficient and fast in smaller data sets.\nFlexible: With the option for different kernel functions, SVM is flexible to different functional forms and can incorporate interaction effects.\n\nDisdvantages of SVM\n\nSensitive: SVM can be sensitive to choices of kernels and to values of tuning parameters.\nNeeds extra step for predicted probabilities: SVM does not directly provide probability estimates for classification, although R implementations will do this step for you.\nComputationally intensive with large data sets: With large data files, SVM can take a lot of time to run.\nLacking transparency: SVM can be hard to interpret and explain, especially when using nonlinear kernels. The transformation carried out by these kernels doesn’t have an intuitive meaning in the original feature space.\n\nImplementing of SVM in R\nSVM can be implemented with the svm() function in the e1071 package.\n\n\n\n Back to top",
    "crumbs": [
      "Modeling approaches",
      "Support vector machines"
    ]
  },
  {
    "objectID": "methods_guidance.html",
    "href": "methods_guidance.html",
    "title": "Guidance for using ML",
    "section": "",
    "text": "Now that we have a basic understanding of some prevalent machine learning algorithms, you are probably wondering how to decide which ones to use when. Here is an attempt to provide some guidance:1\nNote that standardizing/normalizing will never hurt model performance compared to not doing so, but it may make model interpretation more challenging.",
    "crumbs": [
      "Modeling approaches",
      "Guidance for using ML"
    ]
  },
  {
    "objectID": "methods_guidance.html#footnotes",
    "href": "methods_guidance.html#footnotes",
    "title": "Guidance for using ML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA note about combining the goals of predictive analytics and variable importance: Variable importance refers to the objective of measuring the predictive value (i.e. importance) of a variable in predicting the outcome - that is, the strength of the relationship between predictors and the outcome. Variable importance involves many different considerations than predictive analytics. There are many ways to define and measure variable importance, and there are many different methods for estimating variable importance. Some methods are agnostic to the ML algorithm used for prediction. Others are directly related. We will spend a little time talking about variable importance towards the end of the course. For now, I want to point out that sometimes data scientists select a particular machine learning algorithm because it allows them to simultaneously satisfy both goals of predictive analytics and variable importance. More on this later.↩︎",
    "crumbs": [
      "Modeling approaches",
      "Guidance for using ML"
    ]
  }
]
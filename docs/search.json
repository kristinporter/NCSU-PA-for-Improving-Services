[
  {
    "objectID": "performance_metrics_PLACEHOLDER.html",
    "href": "performance_metrics_PLACEHOLDER.html",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "The text below was pulled from 01_03 notebook.\n\n\nFirst, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP.\n\n\n\n\nAUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.\n\n\n\nAUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "href": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "First, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "href": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "href": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#performance",
    "href": "performance_metrics_PLACEHOLDER.html#performance",
    "title": "Understanding performance metrics",
    "section": "Performance",
    "text": "Performance\nWe now compare the learners according to both ROC AUC and PR AUC.According to the mean values of these metrics, the best learners are:\n\nAUC_ROC: r best_roc_auc\nAUC_PR: r best_pr_auc\n\nIt is common for these metrics to show different learners as the best performer. We may want to consider the results holistically to select the best learners. For example, consider the following toy result:\n\n\n\nlearner\nAUC_ROC\nAUC_PR\nMSE\n\n\n\n\nlearner_1\n0.8\n0.7\n2.9\n\n\nlearner_2\n0.7\n0.6\n2.85\n\n\n\nIf we look at all the metrics, learner_2 is the best according to MSE. However, it is only marginally better on MSE, and performs substantially worse on both AUC metrics compared to learner_1. Thus, we would probably not consider learner_2 to be one of our best performing learners.\nAfter understanding broad learner performance, it is often helpful to select a subset of the learners to examine in more detail. For example, we may select only the best learners according to our performance metrics above. Alternatively, we may want to select learners that are more interpretable, even if they are not the best performers. For example, if a simple regression model performs almost as well as a more complicated algorithm, we may still want to select the regression model even if it is not the absolute best performer on our metrics."
  },
  {
    "objectID": "scoping_usefindings.html",
    "href": "scoping_usefindings.html",
    "title": "How will findings be used?",
    "section": "",
    "text": "The department of social services already had a practice of providing extra intervention to those clients who needed them most. These included:\n\ncalling clients to check in,\noffering extra casework sessions, and\nextra workshops to help with job search and job skills.\n\nWith limited staff and financial resources, the department wanted to deliver services to who needed them the most. However, the department thought that their existing approach for identifying those clients could be improved by distilling a wide range of potentially predictive measures into a single “risk score.”\nA few things to note:\n\nThey did not want, nor did the project team recommend, for the risk scores to replace their decision-making process.\nInstead, the risk scores could potentially provide a valuable piece of information - if we could validate good predictive performance and minimal bias.\nThe department planned to train caseworkers to how to interpret and incorporate the risk score into their decisionmaking and prioritizing.\nThe department also wanted to look at how risk scores varied across and within populations served by offices throughout the state. This could help them understand which offices were more in need of support for reaching out to clients and for planning workshops.\n\nThis information about how the department plans to use the information can guide how we scope the predictive analytics projects.\nRemember, we also want to frequently check-in with our ethical considerations. What ethical considerations do we want to be careful about as we proceed?\n\n\n\n Back to top"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "This is assignment 1",
    "section": "",
    "text": "This is assignment 1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_define.html",
    "href": "paforimprovingss_define.html",
    "title": "What is predictive analytics?",
    "section": "",
    "text": "Predictive analytics (PA) (or predictive modeling) is the use historical data to forecast future outcomes.\n\n\n\n\n\n\n\nPA may rely on relatively simple approaches, such as models with a small number of measures, or they may involve complex machine-learning algorithms, taking advantage of the large amounts of data and data-driven methodologies.\nPA may make predictions of binary outcomes, which take just two values such as yes or no. For example, we can predict whether students will graduate from high school or not. With binary outcomes, predictive analytics can also be used to estimate the likelihood of the outcome occurring. For example, we can predict the likelihood or probability that a student will graduate from high school. This number likelihood is between 0 and 1.\nPA may also forecaset continuous outcomes. For example, we can predict the grade point average (GPA) students will achieve.\nTo narrow the scope of the class, we will focus only on binary outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_old.html",
    "href": "index_old.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nInsert overview of these training materials here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Course overview",
    "section": "Syllabus",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#classroom-expectations-and-norms",
    "href": "index.html#classroom-expectations-and-norms",
    "title": "Course overview",
    "section": "Classroom expectations and norms",
    "text": "Classroom expectations and norms"
  },
  {
    "objectID": "index.html#six-ways-to-get-help",
    "href": "index.html#six-ways-to-get-help",
    "title": "Course overview",
    "section": "Six ways to get help",
    "text": "Six ways to get help"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Course overview",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course materials",
    "section": "",
    "text": "These course materials are designed to give students background reading on all the key topics we will cover in the class. Ideally, students should read assigned materials before coming to class. This allows us to use class time to review main points and to have good opportunities for discussion and questions. I will provide guidance on what to read before each class.\n\nHow to use these materials\nThe class is organized into the sections (main topics) listed in the left panel of this page. Clicking the down arrow to the right of each main section title will show the sub-sections. Generally, we will cover one main section/topic per class. Material will be added as the class progresses.\nThe materials consist of text, links to articles, blog posts, videos, etc, and snippets of the code we will use for analyses. Please note:\n\nMost hyperlinks throughout these materials are required unless otherwise noted. Most are very short.\nHyperlinks that are optional should be clearly stated as such. They provide an opportunity to do a deeper dive into a topic if you are interested.\nIn addition, you will see citations throughout. There is no expectation for you to read these papers unless you want to.\n\n\n\nAcknowledgements\nThese course materials were put together by me, Kristin Porter. All errors and omissions are mine. But I have also had lots of help - many sections draw on and even copy valuable text written by Zarni Htet at MDRC and Kristen Hunter at the University of South Wales, Sydney. The framework presented in this class was developed at MDRC, a nonprofit social policy research organization where I was employed for 16 years. Kristen Hunter also provided very helpful review of all these materials.\nThis is the first time I have taught this course, and I would not be surprised if there are problems. I very much welcome any and all feedback and questions at any time.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_ex.html",
    "href": "codeofethics_ex.html",
    "title": "Examples and best practices",
    "section": "",
    "text": "Examples and best practices\n\n\n\n\n Back to top"
  },
  {
    "objectID": "course_description.html",
    "href": "course_description.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. In this class, students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\nPrerequisites: Students should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_why.html",
    "href": "codeofethics_why.html",
    "title": "Why create a code of ethics before starting PA?",
    "section": "",
    "text": "Why create a code of ethics before starting PA?\n\n\nThis is a test.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "1 Papers to maybe work into class…",
    "section": "",
    "text": "References\n\n\n\n\n\n1 Papers to maybe work into class…\nThe Dangers of Risk Prediction in the Criminal Justice System\nAbstract: Courts across the United States are using computer software to predict whether a person will commit a crime, the results of which are incorporated into bail and sentencing decisions. It is imperative that such tools be accurate and fair, but critics have charged that the software can be racially biased, favoring white defendants over Black defendants. We evaluate the claim that computer software is more accurate and fairer than people tasked with making similar decisions. We also evaluate, and explain, the presence of racial bias in these predictive algorithms.\nAn Algorithm That Grants Freedom, or Takes It Away\n2020 NYT article summarizing predictive analytic applications and bias in multiple parts of the CJ system, as well as detecting fraud in the welfare system.\nPredicting Participation in Healthy Marriage and Responsible Fatherhood Programs (and nice technical appendices) from Mathematica/OPRE\n\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "1 Assignments and project",
    "section": "",
    "text": "1 Assignments and project\n\nFor a selected scenario, code of ethics.\nFor a selected scenario, prediction target: what, when, for whom; how will results be used?\nIdentify data set. Summarize prediction target, predictors. Create meta data. Fill in first part of PA toolset?\nData preparation, create predictors. Data model.\nComplete first full page of PA toolset - specify learners.\nCompare learners in terms of performance.\nCompare learners in terms of bias.\nTest link to assignments\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html",
    "href": "paforimprovingss_ethics.html",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly throught through in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveilance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "scoping_ex.html",
    "href": "scoping_ex.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "A motivating example\nLet’s start with an example. This example is motivated by a project I worked on with a Temporary Assistance for Needy Families (TANF) state program, but I’ve modified some details for illustration purposes. TANF is a federal assistance program in the United States that aims to help families achieve self-sufficiency by providing them with the necessary resources and services. Part of what state agencies administering the program do is offer various supportive services to help families overcome barriers to employment and self-sufficiency. They aim to provide personalized assistance based on the unique circumstances and needs of each family. This approach helps address specific challenges and levels of need faced by different households.\n\n\nThe TANF agency came to my team with the following goals:\n\nWith a data-driven tool, could caseworkers better understand which clients were more at risk of not reaching key program outcomes - in particular, of not finding employment (among those who were required to) - with a data-driven tool? Note: The current practice for identifying clients who were most at-risk relied on caseworkers reviewing measures they thought were important (e.g. of previous employment history) and making a subjective experience based on their professional expertise. They wondered if predictive analytics would provide an improvement in terms of accuracy and fairness.\nCould we develop reliable risk scores that combine and weigh multiple risk factors into a single easy-to-understand number that can be incorporated into decision-making?\nCould trends in risk scores across the population of TANF clients provide new insights for program improvement and resource allocation?\n\n\n\nHow do we scope a project that helps them achieve their goals?\nHere we will talk about scoping in terms of specifying:\n\nHow would information about clients’ risks of adverse outcomes be used?\nWho are the stakeholders?\nWhat data are available?\nWhat, exactly, are we trying to predict?\nFor whom would we be making predictions?\nWhen would we be making the predictions?\n\nAnswering these questions allows us to move forward in planning a potential predictive analytics project. That is what I mean by scoping - what potential project could there be? But importantly, the next step is a to investigate that potential - to see if we can reliably, clearly and fairly make useful and actionable predictions. I refer to this next step as the “proof-of-concept,” which we will turn to in the next main section (next week).\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_learn.html",
    "href": "paforimprovingss_learn.html",
    "title": "How can predictive analytics be used to improve services?",
    "section": "",
    "text": "The potential of predictive analytics is to help government programs, nonprofit service providers or businesses identify those clients who could most benefit from targeted interventions—facilitating effective service delivery at an efficient cost.\nWhy just focus on predictive analytics for improving services? Predictive analytics has vast applications. Different applications raise different issues with respect to project scoping, ethics and data and technical considerations. We focus on improving services because it allows us to build a framework that is driven on how results will be used. Also, my background is in using predictive analytics (as well as other data science and statistical methods) to strengthen social serivces, so it is the application to which I have given the most throught. But at the same time, many of the lessons we will learn will be helpful for other applications as well, and there will be some flexibility in data you use for your projects.\n\nSome examples of using predictive analytics to improve social services:\nAt MDRC, the nonprofit research organization at which I worked for many years and where the materials from this course originated, we have used or studied predictive analytics that have aimed to:\n\nModel K-12 academic outcomes: School districts often employ “early warning systems” (EWS), using student-level data, to identify students who are at risk of not meeting key education milestones, such as graduating on time or passing state exams. For example, the risk of not graduating from high school is commonly estimated by the so-called ABC indicators of attendance, behavior, and course performance. This is a simpler approach to predictive analytics. But as schools increasingly collect richer, longitudinal data sets with frequent data updates of many student measures, including daily attendance, exam scores, and course marks, there has been opportunity to compute more accurate, frequent and nuanced predictions of student risk.\nProvide early warnings of participants’ risks of not reaching milestones in an employment training program. This project focused on harnessing granular, longitudinal administrative data to build a system for ongoing, advanced analytics that support the continuous improvement process at the Center for Employment Opportunities (CEO). The goal was for these early warnings to be transmitted, practically in real time, to front-line case workers and leaders, as part of their standard dashboards and data protocols, and CEO planned to train staff members to act on this information, and to work with MDRC to design, implement, and test new interventions based on insights provided by the predictive analytics results. Unfortunately, this project was interrupted by the COVID-19 pandemic.\nIdentify TANF participants who were most likely to find employment. As part of MDRC’s TANF Data Collaborative Project, a team at the Virginia Department of Social Services (VDSS) sought to develop analytic tools to help TANF case workers customize education/employment-related services to increase the likelihood of participants’ labor market success after they leave the program. The team investigated whether demographic characteristics, household compositions, receipt of other public benefits, and past education/employment-related activities could predict success, and how to construct an unbiased predictive tool using such variables. More details can be found here.\nIdentify families at risk of disengaging from a home visiting program. Child First is a home visiting program that aims to promote high-quality relationships between caregivers and children in families experiencing challenges related to caregiver mental health and child behavior. Staff members provide intensive in-home clinical services to both the caregiver and child. They also connect families to additional services such as financial and housing support, health care, and treatment for disorders such as substance abuse. To accomplish its goals, Child First must ensure that families remain consistently engaged in program services over time. When families leave before being officially discharged from the program, they receive truncated interventions that are likely to be less effective at improving outcomes. Early disengagement is also expensive, given the large, fixed costs of enrolling new families into the program. Child First prioritizes collecting high-quality data to understand the population of families it serves. For example, the program collects information on a range of family characteristics assessed at intake, including sociodemographic information on both the caregiver and child, health-related information like insurance status and child DSM-5 diagnoses, as well as several risk measures and assessments such as adverse childhood experiences (ACEs) that capture experiences with violence, abuse or neglect, and household instability. The availability of these data provided researchers with a singular opportunity to explore whether predictive analytics could be a useful tool to summarize the large amounts of information the program collects and use it to help staff members identify families at particularly high risk of early disengagement, defined as being enrolled in the program for fewer than 90 days. That information could allow Child First staff members to triage families better at intake and provide more intensive support and services to those families at risk of early disengagement.\nPretrial justice: Many jurisdictions across the United States are rethinking the “front end” of the criminal justice system — the pretrial period between an arrest and the disposition of a criminal case. Often these reforms focus on the initial decisions that judges and other court stakeholders make about whether to detain individuals in jail while they are awaiting trial, and on the use of money bail as a tool for ensuring that people will show up to court hearings. In most jurisdictions, the majority of people in jail at any point in time are awaiting trial, and many are there because they cannot afford to post bail. Jurisdictions are looking for fairer, more cost-effective approaches to the pretrial phase of the system. To assist jurisdictions in making better initial decisions, the Laura and John Arnold Foundation (now Arnold Ventures) developed the Public Safety Assessment (PSA), a tool that uses data on an individual’s history with the justice system and the current offense to predict the likelihood that the person will show up to hearings or be arrested for a new crime if released. The PSA aims to help judges make more informed, less subjective decisions about pretrial detention. It is currently used in nearly 40 jurisdictions across the nation.(Redcross C. & Henderson, 2019) (Golub C. A. & Valentine, 2019)\n\nLater, we will return to some of the above examples to discuss various issues, including ethics, project scopeing or set-up, the trade-offs of different modeling approaches, and more. Here are some other examples of predictive analytics being used in the social service sector with the goal to strengthen programs’ services.\nChild welfare: One example is the Allegheny Family Screener Tool, first implemented in 2016, was developed in a partnership between researchers from Auckland University of Technology and the Allegheny County Office of Children, Youth, and Families, a Pennsylvania child welfare system. The research team wanted to use predictive analytics to help inform and improve decisions made by staff when determining whether reports of possible child abuse and neglect should be marked for further investigation, rather than replace human decision making altogether. The tool summarizes vast amounts of information across multiple databases to provide a risk score to child welfare call screeners. The researchers worked closely with the child welfare agency and partner organizations to discuss implementation of the tool and results, and feedback from community meetings informed how the tool was developed. An independent evaluation found that it increased the staff’s ability to accurately screen reports and pursue investigations. Also, the tool did not increase the rate of children screened in for investigation. That is, using it resulted in a different pool of children being identified as needing child welfare intervention, but did not substantially increase the proportion of children investigated among all children referred for maltreatment. The model and its implementation have been updated over time so that its predictions reflect contemporary information on families currently being served.(Human Services, 2019)\nLead poisoning prevention: The Chicago Department of Public Health partnered with the Data Science for Social Good at the University of Chicago to help find the homes that are most likely to still contain lead-based paint hazards. From their website: “By building statistical models that predict exposure based on evidence such as the age of a house, the history of children’s exposure at that address, and economic conditions of the neighborhood, CDPH and their partners can link high-risk children and pregnant women to inspection and lead-based paint mitigation funding before any harm is done. This integrated and innovative system will ensure resources are used most efficiently, and ultimately will mean healthier Chicago children.Here’s a short video describing the project.” This short video summarizes the project.\nCriminal justice: In addition to applications in the pretrial periods, predictive analytics is being applied to other parts of the criminal justice system. For example, predictive analytics has been used for “predictive policing” based on forecasting future crime at the community level, for guiding sentencing and probation, for detecting fraud, for assessing young people’s risk of becoming involved in crime and more. As you are surely aware, bias is an enormous concern and this topic alone could take up a whole courses. We will not have the time to delve is as deep as is warranted due to time constraints, but we will discuss this topic later, and I will provide some ideas for further reading. For now, this semi-recent New York Times article provides a brief overview of some applications and concerns about bias.\nHealth care: The adoption of electronic health records (EHRs) by most US health care systems for patient care has led to an explosion of predictive analytics in health care - with applications aimed at improving health outcomes, care coordination, and quality of care. Health care systems and insurance companies harness patient demographics, insurance claims data, and clinical characteristics in EHRs to create statistical models of future health care risks and resource utilization. There are also efforts to incorporate social and behavioral determinants of health (SBDH), which include measures of diet and physical activity as well as characteristics of patients’ neighborhoods, such as food access and transportation.\nThese examples do not capture the breadth of growing applications. The following short articles and blog posts provide some additional examples:\n\nAnticipatory government: Preempting problems through predictive analytics\nCatalogue of predictive models in the humanitarian sector\nWhat is predictive analytics and what could it mean for local governments?\n\n\n\nSome examples of using predictive analytics to improve consumer services:\nI do not have first-hand experience in using or reviewing predictive analytics to improve consumer services, but of course there are MANY! Here are a few articles that I thought provided some good summaries and discussion. Not all applications discussed focus on estimating binary outcomes, but many, such as predicting customer churn, product purchase or customer satisfaction can be examples of predicting binary outcomes.\n\nPredictive customer insight is the future\nHow predictive analytics can improve customer experience\nMORE\n\nQuestions for discussion:\n\nWhat examples do you find interesting for using predictive analytics to improve services? Why?\nWhat promise do you see in the examples you read about or know about?\nWhat concerns do you have?\n\n\n\n\n\n\n Back to topReferences\n\nGolub C. A., C. Redcross, & Valentine, E. J. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment effects of new jersey’s criminal justice reform.\n\n\nHuman Services, A. C. D. of. (2019). Impact evaluation summary of the allegheny family screening tool.\n\n\nRedcross C., & Henderson, B. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment."
  },
  {
    "objectID": "paforimprovingss_limitations.html",
    "href": "paforimprovingss_limitations.html",
    "title": "What are the limitations of PA for improving services?",
    "section": "",
    "text": "MORE FORTHCOMING\n\nPredictions are only as good as the data on which they are built. If the data available for analysis is incomplete, inaccurate, or biased, it can lead to unreliable predictions.\nThere can be a trade-off between model performance and transparency and simplicity. Sometimes the best-performing models are built by complex machine learning algorithms and can therefore be difficult to explain and can be challenging to deploy and maintain. A lack of transparency can be especially problematic in certain domains more ethical considerations.\nExternal Factors and Change: Predictive models may not account for unexpected events, external influences, or sudden changes in the environment that can significantly impact the outcome being predicted. As the time horizon extends further into the future, models may become unreliable. For this reason, predictive models need regular monitoring and may require frequent updating.\nPredictive analytics tells us nothing about causal relationships. Nor does it tell us that those with the lowest or highest likelihood of an outcome are the most likely to benefit from a new or revised service or intervention.\nOverfitting: Overfitting occurs when a predictive model is trained too closely on the training data and becomes overly specific to that data, leading to poor generalization to new, unseen data. It can reduce the model’s ability to predict accurately in real-world scenarios. We will discuss overfitting later.\nEthical Concerns: Predictive analytics can raise many ethical concerns. This is a very important topic, which we turn to next and will return to throughout the course.\n\nOptional: You can find a short discussion of the differences between predictive analytics and inferential statistics here.\nAnd for fun, Baba Brinkman covers some of these issues in his rap comparing and contrasting data science and statistics. (Click link below.)\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_context.html",
    "href": "paforimprovingss_context.html",
    "title": "Is predictive analytics a good fit for your context?",
    "section": "",
    "text": "Before embarking on a PA project, it is value to assess whether it is the right approach at the right time for the setting. Here are a some key considerations to determine if PA is a good fit.\n\nWill PA provide new and actionable information?\n\nDo we anticipate that the PA findings will information that more reliable, more frequent or easier to understand than current information available?\nHow will findings from PA be communicated and used, and by whom?\nHow frequently to PA findings need to be updated to be continuously actionable?\n\n\n\nAre there plans and buy-in for acting on insights that come out of PA?\n\nIs there sufficient leadership and commitment from the institution?\nIs there capacity for interpreting, communicating and and acting on PA results? How will findings be communicated?\nAre there plans for how program services will be changed or introduced based on PA results?\nTo what extent does decision making and service provision currently rely on perceptions or data-driven findings? Are data-driven findings in general, and PA findings more specifically, trusted and used? If not, are there plans for addressing practice and culture to support change?\nAre there structural or process factors that impede or foster the use predictive analytics (such as dedicated data teams) or the use of results (such as dashboard systems)?\n\n\n\nData quality and systems\n\nIs there sufficient data that are readily accessible?\nAre the data sufficiently documented and understood so that reliable measures can be created for PA?\nIs there sufficient capacity for investigating and depoying PA with existing analytic systems, software and staffing?\n\n\n\nPotential risks\n\nHave their been sufficient processes for identifying potential risks?\nAre there systems or processes in place to make sure ethical issues are attended to throughout a PA project?\nRefer back to (addressEthical?) for a discussion of potential processes.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_optional_PAvsInfStat.html",
    "href": "paforimprovingss_optional_PAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "OptionalPAvsInfStat.html",
    "href": "OptionalPAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "issuelog.html",
    "href": "issuelog.html",
    "title": "Issue log",
    "section": "",
    "text": "Formatting of references weird, no change with edits in references.bib or adding apa-6th-edition.csl\n\n\nNotes from chat with Rick\n\nEngage Polina on reviewing output from tool\nPolina could be person to understand what TidyModels is doing.\nGoal: have it ready for deployment.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "href": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly throught through in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveilance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "href": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders - to include as much as possible the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\n\nEthical Considerations in Predictive Analytics Projects\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices to follow:\n\nUnderstanding Regulatory Frameworks: The first step is to be well-versed in relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and ensures that ethical considerations are thoroughly explored from various perspectives.\nDevelop a Code of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nMinimizing Bias: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nOutcome Assessments: Conduct comprehensive outcome assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these practices, organizations can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "scoping_who.html",
    "href": "scoping_who.html",
    "title": "Who are we making predictions for?",
    "section": "",
    "text": "Our target population is specifically adult TANF recipients who are eligible and approved for the TANF departments education and training program and who are required to work.\nOur data sets include a broader group of TANF applicants and participants so we need to be careful to exclude those who do not meet our criteria.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressEthical",
    "href": "paforimprovingss_ethics.html#addressEthical",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders - to include as much as possible the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\n\nEthical Considerations in Predictive Analytics Projects\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices to follow:\n\nUnderstanding Regulatory Frameworks: The first step is to be well-versed in relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and ensures that ethical considerations are thoroughly explored from various perspectives.\nDevelop a Code of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nMinimizing Bias: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nOutcome Assessments: Conduct comprehensive outcome assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these practices, organizations can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  },
  {
    "objectID": "course_syllabus.html",
    "href": "course_syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_grading.html",
    "href": "course_grading.html",
    "title": "Assessment and grading",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_attendancenorms.html",
    "href": "course_attendancenorms.html",
    "title": "Class attendance and participation norms",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "course_gethelp.html",
    "href": "course_gethelp.html",
    "title": "Six ways to get help",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Course information",
    "section": "",
    "text": "This section of this course website provides course information.\n\n\n\n Back to top"
  },
  {
    "objectID": "course_syllabus.html#course-details",
    "href": "course_syllabus.html#course-details",
    "title": "Syllabus",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "course_syllabus.html#course-description",
    "href": "course_syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course Description",
    "text": "Course Description\n\nDescription:\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. Students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\n\n\nPrerequisites:\nStudents should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\nLearning objectives:\n\nGain the ability to critically assess the potential value and limitations of predictive analytics for improving services when applied to a particular context and data system.\nUnderstand how results from predictive analytics can provide actionable insights for improving services.\nUnderstand how to plan and implement all steps of a predictive analytics workflow using open-source code templates. The steps include planning the analysis to match goals, exploring and preparing data, specifying and training models, and assessing the validity and bias of predictive models – all while prioritizing best-practices for ethics, transparency and replication.\nGain a high-level understanding of sta(s(cal concepts for optmizing predictive analytics and have op(onal choices to dive more deeply.\nPresent predictive analytics results using an R notebook and clearly communicate interpretations, cautions and recommendations.\n\n\n\nAbout the Data Science Academy:\nWelcome to an NCSU Data Science Academy Course! In July 2021, the universitywide and interdisciplinary Data Science Academy was launched to meet the growing needs of data science research, education and expertise in North Carolina and beyond. At NC State, Data Science is for Everyone. Data Science Academy (DSA) Courses are designed to make sure that each student can pursue appropriate level challenges through opportunities to make choices and pursue projects of interest. Whether you have never thought about data science before or bring experience and expertise, we welcome you. Our goal is that after each DSA class you want to learn more! For more on the DSA course design. \n\n\nAbout DSA research:\nTo make sure that we are providing an appropriate collection of courses with a variety of challenge levels within each course, we will be collecting data to help us build a practice of continuous improvement. The purpose of the data is to evaluate the DSA and how well we are serving our students. We hope to be able to share what we learn with other universities and researchers - we will ask your permission at the beginning of the course to be able to share your anonymized data when we communicate about the work of the DSA."
  },
  {
    "objectID": "course_syllabus.html#resources",
    "href": "course_syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThere is no textbook for this class. All reading materials are provided within this website and can be accessed in the “Course Materials” tab at the top of the page. The course materials includes text developed by the instructor as well as links to helpful reading, which may be required or optional as noted."
  },
  {
    "objectID": "course_syllabus.html#grading",
    "href": "course_syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nFORTHCOMING"
  },
  {
    "objectID": "course_syllabus.html#course-schedule",
    "href": "course_syllabus.html#course-schedule",
    "title": "Syllabus",
    "section": "Course Schedule",
    "text": "Course Schedule\nThe course schedule is linked here."
  },
  {
    "objectID": "course_syllabus.html#attendance-policy",
    "href": "course_syllabus.html#attendance-policy",
    "title": "Syllabus",
    "section": "Attendance Policy",
    "text": "Attendance Policy\nFor complete attendance and excused absence policies, please see http://policies.ncsu.edu/regulation/reg-02-20-03. For coronavirus related attendance guidance, see the “coronavirus information” section.\nAttendance Policy. Students are expected to attend class regularly. Since we only meet once a week, each class is vitally important to keep on track during the semester.\nAbsences Policy. Students should notify the instructor that they will be unable to attend class and schedule a time to meet with the instructor to review materials they missed during class time.\nMakeup Work Policy. Makeup work will be allowed if the student notifies the instructor at least one day before an assignment is due to arrange for a deadline extension. Notification given on the due date will be considered on a case-by-case basis."
  },
  {
    "objectID": "course_syllabus.html#academic-integrity",
    "href": "course_syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nStudents are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01.\nStudents are expected to produce work that they personally completed and in accordance with the instructions given for the assignment, project, test or quiz. Work copied from other students, completed by another individual, or directly taken from other sources without appropriate attribution will be in violation of academic integrity. Students are expected to follow the Code of Student Conduct (NCSU POL11.35.01) and Pack Pledge, and violations of academic integrity will be handled in accordance with the Student Discipline Procedures (NCSU REG 11.35.02).\nHonor Pledge. Your submission on any test or assignment indicates “I have neither given nor received unauthorized aid on this test or assignment.” Digital Course Components Students may be required to disclose personally identifiable information to other students in the course, via digital tools, such as email or web-postings, where relevant to the course. Examples include online discussions of class topics, and posting of student coursework. All students are expected to respect the privacy of each other by not sharing or using such information outside the course."
  },
  {
    "objectID": "course_syllabus.html#digital-course-components",
    "href": "course_syllabus.html#digital-course-components",
    "title": "Syllabus",
    "section": "Digital Course Components",
    "text": "Digital Course Components\nOur primary form of communication outside of online Zoom meetings will be through Moodle. Students are expected to monitor Moodle for announcements, assignments, and other information during the semester. Assignments. Assignments and final projects may be given and submitted through Moodle."
  },
  {
    "objectID": "course_syllabus.html#accommodations-for-disabilities",
    "href": "course_syllabus.html#accommodations-for-disabilities",
    "title": "Syllabus",
    "section": "Accommodations for Disabilities",
    "text": "Accommodations for Disabilities\nReasonable accommodations will be made for students with verifiable disabilities. In order to take advantage of available accommodations, students must register with the Disability Resource Office at Holmes Hall, Suite 304, Campus Box 7509, 919-515-7653. For more information on NC State’s policy on working with students with disabilities, please see the Academic Accommodations for Students with Disabilities Regulation (REG02.20.01). Non-Discrimination Policy NC State provides equal opportunity and affirmative action efforts, and prohibits all forms of unlawful discrimination, harassment, and retaliation (“Prohibited Conduct”) that are based upon a person’s race, color, religion, sex (including pregnancy), national origin, age (40 or older), disability, gender identity, genetic information, sexual orientation, or veteran status (individually and collectively, “Protected Status”). Additional information as to each Protected Status is included in NCSU REG 04.25.02 (Discrimination, Harassment and Retaliation Complaint Procedure). NC State’s policies and regulations covering discrimination, harassment, and retaliation may be accessed at http://policies.ncsu.edu/policy/pol-04-25-05 or https://oied.ncsu.edu/divweb/. Any person who feels that he or she has been the subject of pro hibited discrimination, harassment, or retaliation should contact the Office for Equal Opportunity (OEO) at 919-515-3148."
  },
  {
    "objectID": "course_syllabus.html#technology-requirements",
    "href": "course_syllabus.html#technology-requirements",
    "title": "Syllabus",
    "section": "Technology Requirements",
    "text": "Technology Requirements\nThis course may require particular technologies to complete coursework. Specifically, students will need R and R Studio. To avoid challenges that may arise due to different versions of packages, the instructor is providing access to code at the this link. Students will need to create accounts with their email address in order to gain access.\nIf you need access to additional technological support, please contact the Libraries’ Technology Lending Service: https://www.lib.ncsu.edu/devices."
  },
  {
    "objectID": "course_syllabus.html#resources-1",
    "href": "course_syllabus.html#resources-1",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nThese are difficult times, and academic and personal stress is a natural result. Everyone is encouraged to take care of themselves and their peers. If you need additional support, there are many resources on campus to help you: \n• Counseling Center https://counseling.dasa.ncsu.edu/ \n• Health Center https://healthypack.dasa.ncsu.edu/ \n• If you or someone you know are experiencing food, housing or financial insecurity, please see the Pack Essentials Program https://dasa.ncsu.edu/pack-essentials/ \nSupporting Fellow Students in Distress. As members of the NC State Wolfpack community, we each share a personal responsibility to express concern for one another and to ensure that this classroom and the campus as a whole remains a healthy and safe environment for learning. Occasionally, you may come across a fellow classmate whose personal behavior concerns or worries you, either for the classmate’s well-being or yours. When this is the case, I would encourage you to report this behavior to the NC State’s Students of Concern website: https://prevention.dasa.ncsu.edu/nc-state-cares. Although you can report anonymously, it is preferred that you share your contact information so they can follow-up with you personally. I may also make a referral to the CARES program if I notice you are having a difficult time. If you don’t know who to talk to or which steps to take, please don’t hesitate to come talk to me and I will help you the best I can. \nStudents are responsible for reviewing the NC State University Policies, Rules, and Regulations (PRRs) which pertain to their course rights and responsibilities, including those referenced both below and above in this syllabus:\nEqual Opportunity and Non-Discrimination Policy Statement: https://policies.ncsu.edu/policy/pol-04-25-05 with additional references at https://oied.ncsu.edu/divweb/policies/\nCode of Student Conduct: https://policies.ncsu.edu/policy/pol-11-35-01."
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "welcome.html#predictive-analytics-for-improving-services",
    "href": "welcome.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics (PA) for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "index.html#predictive-analytics-for-improving-services",
    "href": "index.html#predictive-analytics-for-improving-services",
    "title": "Predictive Analytics for Improving Services",
    "section": "",
    "text": "Welcome! Click top navigation bar to continue."
  },
  {
    "objectID": "course_syllabus.html#syllabus",
    "href": "course_syllabus.html#syllabus",
    "title": "1 Course Description",
    "section": "",
    "text": "DSC495 Special Topics in Data Science: Predictive Analytics for Improving Services\n\n\n\nSemester: Fall 2023\nSection: 613\n\n\nInstructor: Kristin Porter\nClass Meetings: Thursdays\n\n\nEmail: kristin.porter@keporterconsulting.com\nTime: 3-3:50 PM EST\n\n\nOffice hour location: Zoom link\nLocation: Zoom link\n\n\nOffice hours: TBD\nThis meeting is required.\n\n\nLast updated: 08/04/2023\nCredit hours: 1"
  },
  {
    "objectID": "scoping_what.html",
    "href": "scoping_what.html",
    "title": "What are we trying to predict?",
    "section": "",
    "text": "It is important to be very thoughtful and precise in defining and developing a measure of the outcome to be predicted. In our motivating example, we want to predict employment, but we need to precisely define a measure of employment. When defining an outcome, the following considerations are essential:\nIn our example, we will focus on predicting just one outcome measure, but multiple measures of employment that make sense for the goals of the department could be analyzed.\nThe selected precise outcome measure is: whether a client holds a job covered by unemployment insurance (UI) for the first three quarters after exiting the department’s education and training program for TANF participants. The measure is coded as 1 when achieved and 0 otherwise.\nThis choice was made because:\nHowever, after defining this measure, the project decided to code this outcome in the negative – so that we were estimating likelihoods of whether someone does not achieve the employment outcome measure defined above. Therefore, our predicted likelihoods represent risk scores.\nAlso, when discussing this project with stakeholders, we want to emphasize sensitivity to clients’ experiences. While we focus on “success” definitions that inform our measures (subsequently turned into “risk” scores), we avoid implying “failure” when clients fall short of departmental employment goals. We acknowledge the multifaceted challenges clients encounter and the program’s mission to assist those facing elevated barriers."
  },
  {
    "objectID": "scoping_when.html",
    "href": "scoping_when.html",
    "title": "When are we making predictions?",
    "section": "",
    "text": "Deciding when to make predictions not always straightforward but is critical to developing a valid prediction model.\nThe key is to balance two important considerations:\n\nFirst, we want to run predictive models early enough for the findings to be actionable. In our case, we want our caseworkers to be able to help clients early enough that they can address challenges.\nBut running predictive models later may mean we have more data measures available to include in our models and therefore potentially have more accurate models.\n\nIn our TANF program example, through conversations with the program managers and caseworkers on our project team, we learn that there can be substantial lag in the entry of most of data collected during the intake and approval processes for TANF clients.\nWe therefore decide to specify our prediction timepoint as one month after the approval process for the education and training program.\n\nThis allows time for most measures collected up through approval to be entered into the system.\nThis means that our models only include measures that are entered before our timepoint.\nIf the model is deployed, we would need to be careful to wait to apply the model to new data only at the correct timepoint – not earlier. Otherwise, we would have a lot of missing data, which could lead to poor predictions. We will discuss this more later.\n\nIt is a common mistake to not give adequate thought to when predictions are being made and to which measures are available at that time. If we ignore this issue, we might use measures that are collected very close to the outcome. This could lead to excellent performance - great accuracy - but useless predictions.\nFor example, our data may include a measure of whether a TANF client has submitted a job application at the end of a job preparation class. Including this measure in our model could improve its predictive performance. However, because the measure is collected towards the end of the education and training program, if we wait to do our modeling (set our timepoint at the end of the education and training program), then by the time we get results there is little time to take action to help those at highest risk of not finding and sustaining employment.\nThink about this issue when you read or hear about the high accuracy of predictive models. If a company boasts that it predicts an outcome with very high accuracy, one important question to ask is: When are predictions being made? Is it possible that measures are collected so close to the outcome that some predictors are actually proxies for the outcome? Or, what trade-offs are being made in terms of having time to act on predictive analytics results?\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_stakeholders.html",
    "href": "scoping_stakeholders.html",
    "title": "Who are the stakeholders?",
    "section": "",
    "text": "forthcoming\n\n\n\n Back to top"
  },
  {
    "objectID": "scoping_what.html#footnotes",
    "href": "scoping_what.html#footnotes",
    "title": "What are we trying to predict?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs summarized by the Employment and Training Administration in the U.S. Department of Labor: These data are from the Unemployment Insurance Data Base (UIDB) as well as UI-related data from outside sources (e.g., Bureau of Labor Statistics data on employment and unemployment and U.S. Department of Treasury data on state UI trust fund activities). See this link{https://edd.ca.gov/en/newsroom/facts-and-stats/dashboard/} for more details.↩︎"
  },
  {
    "objectID": "poc_compare.html",
    "href": "poc_compare.html",
    "title": "Comparing learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "poc_learners.html",
    "href": "poc_learners.html",
    "title": "Defining learers",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "poc_importance.html",
    "href": "poc_importance.html",
    "title": "A proof-of-concept framework",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "poc_benchmark.html",
    "href": "poc_benchmark.html",
    "title": "Specifying a benchmark",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "poc_addcomplexity.html",
    "href": "poc_addcomplexity.html",
    "title": "Gradually adding complexity",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_split.html",
    "href": "data_split.html",
    "title": "Splitting data for training, testing and validating",
    "section": "",
    "text": "TEXT FROM MEMO\nFirst, model training is when the chosen algorithm is applied to a training data set in order to learn the patterns present in the data. Once an algorithm is trained, it can then be used on unseen data to make new predictions. For example, training a linear regression model consists of estimating the regression coefficients. These coefficients are then applied when making new predictions. For more complex algorithms, the trained model is generally more complex than a single set of coefficients.\nSecond, validation is when learners are used to make predictions on an unseen set of data, called the “validation” set. The validation set is used to calculate evaluation metrics. In this stage, we use the evaluation results to do learner selection, and pick the best learner.\nHERE TALK ABOUT HOW WE CAN VALIDATE WITH NEW DATA OR USE CV WITH TRAINING DATA?\nFinally, testing is when the single final selected learner is used to make predictions on a final unseen set of data, the “test” set. This step gives us a realistic assessment of learner performance on unseen data.\n\n\n\n Back to top"
  },
  {
    "objectID": "data_splitconsiderations.html",
    "href": "data_splitconsiderations.html",
    "title": "Considerations for splitting data",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_overfitting.html",
    "href": "data_overfitting.html",
    "title": "Generalizing learners to new data",
    "section": "",
    "text": "Recall our goal of predictive analytics, as shown in this simply illustration: We use data with known outcomes to build a model that allows us to predict outcomes - or the likelihood of outcomes in data with unknown outcomes.\n\nHow do we make sure that our model, which is based on information in the data with known outcomes, generalizes to new data? If our model too closely tailored to the data we use to develop it, it risks not only capturing meaningful statistical patterns but also incidental fluctuations or noise unique to that dataset. When a model inadvertently incorporates this noise, it excels at describing and fitting the original data. However, when confronted with new, unseen data, this same model tends to deliver inaccurate predictions. Its inability to adapt to new data stems from being constructed around specific noise rather than encompassing broader, generalizable patterns.\nThe following simple plots display the concepts of underfitting and overfitting with an example of fitting a model that captures the relationship between two measures. In the first plot, the estimated model, represented by the blue line, does not capture the trend evident in the data. This is corrected in the second plot, in which the model appears to be a a good fit of the general trend. Then, in the the third plot, the model fits the data extremely well, but it overfits because it predicts almost every point - almost every random variation from the overall trend we care about.\n\nIn this first section on data for predictive analytics, we will learn how to develop (i.e., estimate, fit, train), compare and select learners (i.e. predictor sets plus modeling approaches) that do the best job in generalizing to new, unseen data. That is, we will learn strategies for avoiding overfitting.\n\n\n\n Back to top"
  },
  {
    "objectID": "data_validate.html",
    "href": "data_validate.html",
    "title": "Data for validating learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_train.html",
    "href": "data_train.html",
    "title": "Data for training learners",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "data_test.html",
    "href": "data_test.html",
    "title": "Data for testing learners",
    "section": "",
    "text": "Back to top"
  }
]
[
  {
    "objectID": "performance_metrics_PLACEHOLDER.html",
    "href": "performance_metrics_PLACEHOLDER.html",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "The text below was pulled from 01_03 notebook.\n\n\nFirst, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP.\n\n\n\n\nAUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting.\n\n\n\nAUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "href": "performance_metrics_PLACEHOLDER.html#review-of-binary-prediction-metrics",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "First, we review some terms related to binary predictions.\n\nthe true value for a unit (e.g., individual person) is the observed value (0 or 1).\nthe predicted probability for a unit is the estimated probability of the outcome (that the the outcome variable equals 1), generated by the model. It is a continuous number between 0 and 1.\nthe predicted probability can be converted into a predicted classification by comparing it to a selected threshold.\nthe threshold is a cutoff for classifying a positive outcome. All units with a predicted probability equal to or above the threshold are predicted to be a 1, and all units with a predicted probability below the threshold are predicted to be a 0. A common threshold of choice is 0.5.\n\nWhen we have predicted classifications, we can categorize all them into four categories:\n\na true positive (TP) is when both the predicted outcome and the observed outcome are 1.\na false positive (FP) is when the predicted outcome is 1, but the observed outcome is 0.\na true negative (TN) is when both the predicted outcome and the observed outcome are 0.\na false negative (FN) is when the predicted outcome is 0, but the observed outcome is 1.\n\nNote that the notations P and N in the four categories above refer to predicted positives and predicted negatives. The T and F preceding the P and N is a determinant of whether the predicted positives and negatives are correct or not.\nWe can define two more useful notations:\n\nOP is the number of observed positives (outcomes that are 1).\nON is the number of observed negatives (outcomes that are 0).\nPP is the number of predicted positives (predictions that are 1).\nPN is the number of predicted negatives (predictions that are 0).\n\nFinally, we can also summarize these categories into rates.\n\nthe true positive rate (TPR) is also known as sensitivity or recall and is the number of true positives over the total number of positive observed values: TP / (TP + FN) = TP / OP.\nthe true negative rate (TNR) is also known as specificity and is the number of true negatives over the total number of negative observed values: TN / (TN + FP) = TN / ON.\nthe false positive rate (FPR) is the number of false positives over the total number of negative observed values: FP / (TN + FP) = FP / ON.\nspecificity is \\(1 - FPR\\).\nthe false negative rate (FNR) is the number of false negatives over the total number of positive observed values: FN / (TP + FN) = FN / OP.\nprecision also known as positive predictive value is the number of true positives over the total number of predicted positives: TP / (TP + FP).\nnegative predictive value is the number of true negatives over the total number of predicted negatives: TN / (TN + FN).\nfalse discovery rate is the number of false positives over the total number of predicted positives: FP / (FP + TP) = FP / PP."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "href": "performance_metrics_PLACEHOLDER.html#auc-roc",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve. AUC ROC is a simple but effective metric for determining performance of binary models. The higher the value, the better the model is at making predictions.\nAn ROC curve plots false positive rate on the x-axis against the true positive rate on the y-axis, as the threshold changes. The first point in the bottom left corner of the ROC curve corresponds to the TPR and FPR at a threshold of 1. Next, imagine we change the threshold to 0.95. Then all units with a predicted value above 0.95 will be classified as 1, and all units with a predicted value below 0.95 will be classified as 0. At this threshold, most of the units we classify as 1 will be observed positives, so we expect a high true positive rate and a low false positive rate. As we decrease the threshold, we will classify more and more units as 1, which increases the false positive rate (numerator increases) and decreases the true positive rate (denominator increases). A good model has a high true positive rate and a low false positive rate, so points farther to the left and farther up are markers of a good model.\nWe usually compare a model’s curve to a dotted line that goes straight up the diagonal from 0 to 1. The dotted line corresponds to the performance of a random classifier. A random classifier picks the class of each unit using a random coin flip, without any information about the unit. Ideally, a trained model informed by the patterns in the data performs much better than a random classifier.\nIn the case of imbalanced data, when one of the categories is very rare, ROC curves can give misleading results and be overly optimistic of model performance. As a rule of thumb, if your data has 10% or less of units in one category, you may have imbalanced data. In this case, AUC of precision recall curve, discussed next, may be a better metric. For more information about imbalanced data, see our short explainer attached in the reference folder on the subject, and why ROC curves may not perform well in that setting."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "href": "performance_metrics_PLACEHOLDER.html#auc-pr",
    "title": "Understanding performance metrics",
    "section": "",
    "text": "AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve. AUC PR is a better metric than AUC ROC for imbalanced data set because AUC PR does not account for true negatives, unlike in AUC ROC. As mentioned above, AUC ROC is a trade-off measurement curve between true positive rate (on the Y-axis) and false positive rate (on the X-axis).\nIn imbalanced data, the false positive rate tends to remain low due to a large number of observed negatives. Recall that false positive rate/FPR = FP/(TN + FP). A larger number of observed negative values within the imbalanced data would imply a large denominator (because TN would be large) and hence, we would see a low false positive rate even as the number of false positives increases. Thus, AUC ROC can be a less informative and at times overly optimistic metric for imbalanced data.\nAUC PR, on the other hand, is a trade-off between precision/predictive positive rate (TP/(TP + FP)) and recall/sensitivity (TP/(TP + FN)). Precision is solely based on predicted positive values (TPs and FPs) and is unaffected if there is a large number of observed negatives. If there are few false positives, then there will be both a low false positive rate (smaller numerator for FPR) and high precision (smaller denominator for precision). If there are few false negatives, then there will be both a low false negative rate (smaller numerator for FNR) and a high recall (smaller denominator for recall). Therefore, a high AUC PR score is good because it implies low false positive and false negative rates (just like a high AUC ROC score).\nUnlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis. Precision is plotted on the Y-axis. Like in AUC ROC as well, the precision and recall are plotted on X and Y axis as the threshold changes.\nAs in shown in Figure 1.2 below, the PR plot starts at the top left corner, where the threshold is 1, and moves towards the direction of bottom right, where the threshold is 0. At thresholds close to 1, the learner classifies almost none of the observations as 1. Hence, recall (TP / (TP + FN)) would be low because there are very few predicted positives (and thus few TP). Precision (TP / (TP + FP)) would be high because there are few false positives. Towards the bottom right of plot at thresholds close to 0, the learner classifies almost all observations as 1. Hence, recall (TP / (TP + FN)) would be high because there are very few predicted negatives (and thus few FN). Precision would be equal to the proportion of observed positives (the denominator TP + FP = all observations because all are predicted to be positive).\nAs shown in Figure 1.3, a learner with a perfect AUC PR score (a perfect learner) would have a curve fitting perfectly towards the (1,1) coordinate space where you have perfect precision and recall. A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space and above the no skills classifier horizontal line.\nA no skills classifier is a learner that predicts every observation as 1. Suppose the data is extremely imbalanced, and only 0.3 percent of the total observations are 1. With the no skills classifier predicting every observation as positive, the precision score would remain the same/constant regardless of the threshold. Hence, a no skills classifier’s AUC PR curve is the horizontal line across the bottom of Figure 1.3.\nFigure 1.1 An example of ROC and precision-recall curves across different learners.\n\nFigure 1.2 An example of a PR curve.\n\nFigure 1.3 An example of a PR curve across perfect, good and no skills learners."
  },
  {
    "objectID": "performance_metrics_PLACEHOLDER.html#performance",
    "href": "performance_metrics_PLACEHOLDER.html#performance",
    "title": "Understanding performance metrics",
    "section": "Performance",
    "text": "Performance\nWe now compare the learners according to both ROC AUC and PR AUC.According to the mean values of these metrics, the best learners are:\n\nAUC_ROC: r best_roc_auc\nAUC_PR: r best_pr_auc\n\nIt is common for these metrics to show different learners as the best performer. We may want to consider the results holistically to select the best learners. For example, consider the following toy result:\n\n\n\nlearner\nAUC_ROC\nAUC_PR\nMSE\n\n\n\n\nlearner_1\n0.8\n0.7\n2.9\n\n\nlearner_2\n0.7\n0.6\n2.85\n\n\n\nIf we look at all the metrics, learner_2 is the best according to MSE. However, it is only marginally better on MSE, and performs substantially worse on both AUC metrics compared to learner_1. Thus, we would probably not consider learner_2 to be one of our best performing learners.\nAfter understanding broad learner performance, it is often helpful to select a subset of the learners to examine in more detail. For example, we may select only the best learners according to our performance metrics above. Alternatively, we may want to select learners that are more interpretable, even if they are not the best performers. For example, if a simple regression model performs almost as well as a more complicated algorithm, we may still want to select the regression model even if it is not the absolute best performer on our metrics."
  },
  {
    "objectID": "scoping_usefindings.html",
    "href": "scoping_usefindings.html",
    "title": "How will findings from PA be used?",
    "section": "",
    "text": "How will findings from PA be used?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "This is assignment 1",
    "section": "",
    "text": "This is assignment 1\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_define.html",
    "href": "paforimprovingss_define.html",
    "title": "What is predictive analytics?",
    "section": "",
    "text": "Predictive analytics is the use historical data to forecast future outcomes.\nPredictive analytics may rely on relatively simple approaches, such as models with a small number of measures, or they may involve complex machine-learning algorithms, taking advantage of the large amounts of data and data-driven methodologies.\nPredictive analytics may make predictions of binary outcomes, which take just two values such as yes or no. For example, we can predict whether students will graduate from high school or not. With binary outcomes, predictive analytics can also be used to estimate the likelihood of the outcome occurring. For example, we can predict the likelihood or probability that a student will graduate from high school. This number likelihood is between 0 and 1.\nPredictive analytics may also forecaset continuous outcomes. For example, we can predict the grade point average (GPA) students will achieve.\nTo narrow the scope of the class, we will focus only on binary outcomes.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index_old.html",
    "href": "index_old.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nInsert overview of these training materials here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Course overview",
    "section": "Syllabus",
    "text": "Syllabus"
  },
  {
    "objectID": "index.html#classroom-expectations-and-norms",
    "href": "index.html#classroom-expectations-and-norms",
    "title": "Course overview",
    "section": "Classroom expectations and norms",
    "text": "Classroom expectations and norms"
  },
  {
    "objectID": "index.html#six-ways-to-get-help",
    "href": "index.html#six-ways-to-get-help",
    "title": "Course overview",
    "section": "Six ways to get help",
    "text": "Six ways to get help"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Course overview",
    "section": "Grading",
    "text": "Grading"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course materials",
    "section": "",
    "text": "These course materials are designed to give students background reading on all the key topics we will cover in the class. Ideally, students should read assigned materials before coming to class. This allows us to use class time to review main points and to have good opportunities for discussion and questions. I will provide guidance on what to read before each class.\n\nHow to use these materials\nThe class is organized into the sections (main topics) listed in the left panel of this page. Clicking the down arrow to the right of each main section title will show the sub-sections. Generally, we will cover one main section/topic per class. The sub-sections for topics covered later in the terms will be added to the website as the class progresses.\nThe materials consist of text, links to articles, blog posts, videos, etc, and snippets of the code we will use for analyses. Please note:\n\nMost hyperlinks throughout these materials are required unless otherwise noted. Most are very short.\nHyperlinks that are optional should be clearly stated as such. They provide an opportunity to do a deeper dive into a topic if you are interested.\nIn addition, you will see citations for throughout. There is no expectation for you to read these papers unless you want to.\n\n\n\nAcknowledgements\nThese course materials were put together by me, Kristin Porter. All errors and omissions are mine. But I have also had lots of help - many sections draw on and even copy valuable text written by Zarni Htet at MDRC and Kristen Hunter at the University of South Wales, Sydney. The framework presented in this class was developed at MDRC, a nonprofit social policy research organization where I was employed for 16 years. Kristen Hunter also provided very helpful review of all these materials.\nThis is the first time I have taught this course, and I would not be surprised if there are problems. I very much welcome any and all feedback and questions at any time.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_ex.html",
    "href": "codeofethics_ex.html",
    "title": "Examples and best practices",
    "section": "",
    "text": "Examples and best practices\n\n\n\n\n Back to top"
  },
  {
    "objectID": "course_description.html",
    "href": "course_description.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nFor-profit companies, nonprofit service providers and government agencies often use predictive analytics to improve their services. Predictive analytics harnesses historical data and may incorporate machine learning to model future outcomes. In this class, students will explore the value, limitations and ethical considerations of predictive analytics when used to improve services. Using their own dataset or one provided by the instructor, students will learn and apply a practical approach for planning, implementing and assessing a predictive analytics project. The instruction will highlight topics related to data preparation, model training and selection, validation, fairness, transparency and communication of results.\nPrerequisites: Students should have familiarity with R Studio and some experience using R to manipulate data and run basic descriptive statistics.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "codeofethics_why.html",
    "href": "codeofethics_why.html",
    "title": "Why create a code of ethics before starting PA?",
    "section": "",
    "text": "Why create a code of ethics before starting PA?\n\n\nThis is a test.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "1 Papers to maybe work into class…",
    "section": "",
    "text": "References\n\n\n\n\n\n1 Papers to maybe work into class…\nThe Dangers of Risk Prediction in the Criminal Justice System\nAbstract: Courts across the United States are using computer software to predict whether a person will commit a crime, the results of which are incorporated into bail and sentencing decisions. It is imperative that such tools be accurate and fair, but critics have charged that the software can be racially biased, favoring white defendants over Black defendants. We evaluate the claim that computer software is more accurate and fairer than people tasked with making similar decisions. We also evaluate, and explain, the presence of racial bias in these predictive algorithms.\nAn Algorithm That Grants Freedom, or Takes It Away\n2020 NYT article summarizing predictive analytic applications and bias in multiple parts of the CJ system, as well as detecting fraud in the welfare system.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "1 Assignments and project",
    "section": "",
    "text": "1 Assignments and project\n\nFor a selected scenario, code of ethics.\nFor a selected scenario, prediction target: what, when, for whom; how will results be used?\nIdentify data set. Summarize prediction target, predictors. Create meta data. Fill in first part of PA toolset?\nData preparation, create predictors. Data model.\nComplete first full page of PA toolset - specify learners.\nCompare learners in terms of performance.\nCompare learners in terms of bias.\nTest link to assignments\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html",
    "href": "paforimprovingss_ethics.html",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly throught through in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveilance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "scoping_ex.html",
    "href": "scoping_ex.html",
    "title": "1 A motivating example",
    "section": "",
    "text": "Here we will describe an example (or more) that we can refer back to."
  },
  {
    "objectID": "paforimprovingss_learn.html",
    "href": "paforimprovingss_learn.html",
    "title": "How can predictive analytics be used to improve services?",
    "section": "",
    "text": "The potential of predictive analytics is to help government programs, nonprofit service providers or businesses identify those clients who could most benefit from targeted interventions—facilitating effective service delivery at an efficient cost.\nWhy just focus on predictive analytics for improving services? Predictive analytics has vast applications. Different applications raise different issues with respect to project scoping, ethics and data and technical considerations. We focus on improving services because it allows us to build a framework that is driven on how results will be used. Also, my background is in using predictive analytics (as well as other data science and statistical methods) to strengthen social serivces, so it is the application to which I have given the most throught. But at the same time, many of the lessons we will learn will be helpful for other applications as well, and there will be some flexibility in data you use for your projects.\n\nSome examples of using predictive analytics to improve social services:\nAt MDRC, the nonprofit research organization at which I worked for many years and where the materials from this course originated, we have used or studied predictive analytics that have aimed to:\n\nModel K-12 academic outcomes: School districts often employ “early warning systems” (EWS), using student-level data, to identify students who are at risk of not meeting key education milestones, such as graduating on time or passing state exams. For example, the risk of not graduating from high school is commonly estimated by the so-called ABC indicators of attendance, behavior, and course performance. This is a simpler approach to predictive analytics. But as schools increasingly collect richer, longitudinal data sets with frequent data updates of many student measures, including daily attendance, exam scores, and course marks, there has been opportunity to compute more accurate, frequent and nuanced predictions of student risk.\nProvide early warnings of participants’ risks of not reaching milestones in an employment training program. This project focused on harnessing granular, longitudinal administrative data to build a system for ongoing, advanced analytics that support the continuous improvement process at the Center for Employment Opportunities (CEO). The goal was for these early warnings to be transmitted, practically in real time, to front-line case workers and leaders, as part of their standard dashboards and data protocols, and CEO planned to train staff members to act on this information, and to work with MDRC to design, implement, and test new interventions based on insights provided by the predictive analytics results. Unfortunately, this project was interrupted by the COVID-19 pandemic.\nIdentify TANF participants who were most likely to find employment. As part of MDRC’s TANF Data Collaborative Project, a team at the Virginia Department of Social Services (VDSS) sought to develop analytic tools to help TANF case workers customize education/employment-related services to increase the likelihood of participants’ labor market success after they leave the program. The team investigated whether demographic characteristics, household compositions, receipt of other public benefits, and past education/employment-related activities could predict success, and how to construct an unbiased predictive tool using such variables. More details can be found here.\nIdentify families at risk of disengaging from a home visiting program. Child First is a home visiting program that aims to promote high-quality relationships between caregivers and children in families experiencing challenges related to caregiver mental health and child behavior. Staff members provide intensive in-home clinical services to both the caregiver and child. They also connect families to additional services such as financial and housing support, health care, and treatment for disorders such as substance abuse. To accomplish its goals, Child First must ensure that families remain consistently engaged in program services over time. When families leave before being officially discharged from the program, they receive truncated interventions that are likely to be less effective at improving outcomes. Early disengagement is also expensive, given the large, fixed costs of enrolling new families into the program. Child First prioritizes collecting high-quality data to understand the population of families it serves. For example, the program collects information on a range of family characteristics assessed at intake, including sociodemographic information on both the caregiver and child, health-related information like insurance status and child DSM-5 diagnoses, as well as several risk measures and assessments such as adverse childhood experiences (ACEs) that capture experiences with violence, abuse or neglect, and household instability. The availability of these data provided researchers with a singular opportunity to explore whether predictive analytics could be a useful tool to summarize the large amounts of information the program collects and use it to help staff members identify families at particularly high risk of early disengagement, defined as being enrolled in the program for fewer than 90 days. That information could allow Child First staff members to triage families better at intake and provide more intensive support and services to those families at risk of early disengagement.\nPretrial justice: Many jurisdictions across the United States are rethinking the “front end” of the criminal justice system — the pretrial period between an arrest and the disposition of a criminal case. Often these reforms focus on the initial decisions that judges and other court stakeholders make about whether to detain individuals in jail while they are awaiting trial, and on the use of money bail as a tool for ensuring that people will show up to court hearings. In most jurisdictions, the majority of people in jail at any point in time are awaiting trial, and many are there because they cannot afford to post bail. Jurisdictions are looking for fairer, more cost-effective approaches to the pretrial phase of the system. To assist jurisdictions in making better initial decisions, the Laura and John Arnold Foundation (now Arnold Ventures) developed the Public Safety Assessment (PSA), a tool that uses data on an individual’s history with the justice system and the current offense to predict the likelihood that the person will show up to hearings or be arrested for a new crime if released. The PSA aims to help judges make more informed, less subjective decisions about pretrial detention. It is currently used in nearly 40 jurisdictions across the nation.(Redcross C. & Henderson, 2019) (Golub C. A. & Valentine, 2019)\n\nLater, we will return to some of the above examples to discuss various issues, including ethics, project scopeing or set-up, the trade-offs of different modeling approaches, and more. Here are some other examples of predictive analytics being used in the social service sector with the goal to strengthen programs’ services.\nChild welfare: One example is the Allegheny Family Screener Tool, first implemented in 2016, was developed in a partnership between researchers from Auckland University of Technology and the Allegheny County Office of Children, Youth, and Families, a Pennsylvania child welfare system. The research team wanted to use predictive analytics to help inform and improve decisions made by staff when determining whether reports of possible child abuse and neglect should be marked for further investigation, rather than replace human decision making altogether. The tool summarizes vast amounts of information across multiple databases to provide a risk score to child welfare call screeners. The researchers worked closely with the child welfare agency and partner organizations to discuss implementation of the tool and results, and feedback from community meetings informed how the tool was developed. An independent evaluation found that it increased the staff’s ability to accurately screen reports and pursue investigations. Also, the tool did not increase the rate of children screened in for investigation. That is, using it resulted in a different pool of children being identified as needing child welfare intervention, but did not substantially increase the proportion of children investigated among all children referred for maltreatment. The model and its implementation have been updated over time so that its predictions reflect contemporary information on families currently being served.(Human Services, 2019)\nLead poisoning prevention: The Chicago Department of Public Health partnered with the Data Science for Social Good at the University of Chicago to help find the homes that are most likely to still contain lead-based paint hazards. From their website: “By building statistical models that predict exposure based on evidence such as the age of a house, the history of children’s exposure at that address, and economic conditions of the neighborhood, CDPH and their partners can link high-risk children and pregnant women to inspection and lead-based paint mitigation funding before any harm is done. This integrated and innovative system will ensure resources are used most efficiently, and ultimately will mean healthier Chicago children.Here’s a short video describing the project.” This short video summarizes the project.\nCriminal justice: In addition to applications in the pretrial periods, predictive analytics is being applied to other parts of the criminal justice system. For example, predictive analytics has been used for “predictive policing” based on forecasting future crime at the community level, for guiding sentencing and probation, for detecting fraud, for assessing young people’s risk of becoming involved in crime and more. As you are surely aware, bias is an enormous concern and this topic alone could take up a whole courses. We will not have the time to delve is as deep as is warranted due to time constraints, but we will discuss this topic later, and I will provide some ideas for further reading. For now, this semi-recent New York Times article provides a brief overview of some applications and concerns about bias.\nHealth care: The adoption of electronic health records (EHRs) by most US health care systems for patient care has led to an explosion of predictive analytics in health care - with applications aimed at improving health outcomes, care coordination, and quality of care. Health care systems and insurance companies harness patient demographics, insurance claims data, and clinical characteristics in EHRs to create statistical models of future health care risks and resource utilization. There are also efforts to incorporate social and behavioral determinants of health (SBDH), which include measures of diet and physical activity as well as characteristics of patients’ neighborhoods, such as food access and transportation.\nThese examples do not capture the breadth of growing applications. The following short articles and blog posts provide some additional examples:\n\nAnticipatory government: Preempting problems through predictive analytics\nCatalogue of predictive models in the humanitarian sector\nWhat is predictive analytics and what could it mean for local governments?\n\n\n\nSome examples of using predictive analytics to improve consumer services:\nI do not have first-hand experience in using or reviewing predictive analytics to improve consumer services, but of course there are MANY! Here are a few articles that I thought provided some good summaries and discussion. Not all applications discussed focus on estimating binary outcomes, but many, such as predicting customer churn, product purchase or customer satisfaction can be examples of predicting binary outcomes.\n\nPredictive customer insight is the future\nHow predictive analytics can improve customer experience\nMORE\n\nQuestions for discussion:\n\nWhat examples do you find interesting for using predictive analytics to improve services? Why?\nWhat promise do you see in the examples you read about or know about?\nWhat concerns do you have?\n\n\n\n\n\n\n Back to topReferences\n\nGolub C. A., C. Redcross, & Valentine, E. J. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment effects of new jersey’s criminal justice reform.\n\n\nHuman Services, A. C. D. of. (2019). Impact evaluation summary of the allegheny family screening tool.\n\n\nRedcross C., & Henderson, B. (2019). Evaluation of pretrial justice system reforms that use the public safety assessment."
  },
  {
    "objectID": "paforimprovingss_limitations.html",
    "href": "paforimprovingss_limitations.html",
    "title": "What are the limitations of PA for improving services?",
    "section": "",
    "text": "FORTHCOMING\nMAYBE INSERT TABLE WITH CONTRASTING QUESTIONS\nOptional: You can find a short discussion of the differences between predictive analytics and inferential statistics here.\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_context.html",
    "href": "paforimprovingss_context.html",
    "title": "1 Is predictive analytics a good fit for your context?",
    "section": "",
    "text": "1 Is predictive analytics a good fit for your context?\nConsiderations:\n\nHow will findings be communicated and used, and by whom?\nData availability, quality and systems.\nCapacity re communication, analytics, change management.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_optional_PAvsInfStat.html",
    "href": "paforimprovingss_optional_PAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "OptionalPAvsInfStat.html",
    "href": "OptionalPAvsInfStat.html",
    "title": "Predictive Analytics vs. Inferential Statistics",
    "section": "",
    "text": "Predictive analytics uses patterns in historical data to make predictions about future events. The goals and limitations of predictive analytics are different from those of statistical inference.\nIn inference settings, we usually aim to understand properties of a population. “An inferential data analysis quantifies whether an observed pattern will likely hold beyond the data set in hand. This is the most common statistical analysis in the formal scientific literature (Leek and Peng (2015)).” We may ask: what is the mean height of Gentoo penguins? We can construct both a point estimate (such as 30 inches) and a measure of uncertainty, which can be expressed as a standard error (2 inches), or as a confidence interval (we are 95% confident the true mean height is between 26 and 34 inches).\nStatistical inference can also be used to understand relationships between variables. For example, we might be interested in the relationship between penguin body mass and penguin height, so we run a linear regression of body mass on height. The regression gives us a coefficient, which we can interpret as: for each increase in height by 1 inch, body mass increases on average by x inches.\nIn predictive analytics, our goal is to make the most accurate prediction possible given the available data. In predictive analytics, we usually focus on point estimates, and often do not look at measures of uncertainty. Certain prediction algorithms give uncertainty estimates, such as regression, but others do not immediately provide uncertainty estimates (without modification or further more complicated algorithms). Without measures of uncertainty, we should take particular care in the use and interpretation of predictive analytics outputs. The algorithm gives its best possible guess of the outcome based on historical data, but we do not know how “confident” we are in the result. For example, consider using the above regression to predict the average body mass of a Gentoo penguin that is 32 inches tall. The regression tells us the expected body mass is 4000g, with a confidence interval of 3800 to 4200g. We then predict the average body mass of a Gentoo penguin that is 40 inches tall. For a penguin of this height, the expected body mass is 4500g, with a confidence interval of 2000g to 6000g. The confidence intervals for different heights have substantially different widths, telling us that we have different amounts of uncertainty around different predictions. In contrast, many prediction algorithms would not give us estimates of uncertainty, so we would only see the point estimates.\nNeither predictive analytics not inferential analysis should be used to make causal conclusions. “Predictive data analyses only show that you can predict one measurement from another; they do not necessarily explain why that choice of prediction works (Leek and Peng (2015)).” If we have an interpretable model, or we use a machine learning interpretation method, we can use the algorithm to better understand historical patterns and relationships in the data. For example, in the above regression, we can conclude that there is a particular historical relationship between body mass and height. However, we cannot conclude that these patterns are causal. Let’s say we aim to predict the number of ice cream sales on a future summer day. We find that the most useful predictor of the number of ice cream sales is the number of lifeguards on duty that day at the local beach. Are those extra lifeguards alone driving our ice cream sales? Probably not! Most likely weather is a confounding variable here. On hot sunny days, more people go to the beach, there are more life guards on duty, and more people buy ice cream. We have found a correlation, but we cannot conclude it is causal. To make conclusions about causal relationships, we must take an entirely different approach. To make rigorous causal conclusions, we would need to carefully design a study, ideally an experiment or in some cases a well-designed observational study. Such causal inference approaches are beyond the scope of this guide.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course overview",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "issuelog.html",
    "href": "issuelog.html",
    "title": "Issue log",
    "section": "",
    "text": "Formatting of references weird, no change with edits in references.bib or adding apa-6th-edition.csl\n\n\nNotes from chat with Rick\n\nEngage Polina on reviewing output from tool\nPolina could be person to understand what TidyModels is doing.\nGoal: have it ready for deployment.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "href": "paforimprovingss_ethics.html#a-broad-overview-of-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "",
    "text": "When planning, developing and deploying a predictive analytics project, there are many ethical issues to consider. Here is a high-level summary. We will return to these as the class progresses.\nPrivacy and Data Protection: Predictive analytics often relies on data about individuals. Collecting, storing, and analyzing this data raise privacy concerns. It’s crucial to ensure that data is handled securely and that individuals’ personally identifiable information is protected to prevent unauthorized access or misuse.\nBias and Fairness: Predictive analytics can perpetuate existing biases present in the data used for training and developing models. If historical data contains discriminatory patterns, the predictive models may inadvertently perpetuate these biases, leading to unfair outcomes for certain groups of people.\nData Quality and Representativeness: Relatedly, biases in data quality and representativeness can affect the performance of predictive models. If the data used is unrepresentative or incomplete, it can lead to inaccurate or unfair predictions.\nTransparency and Explainability: Predictive models can be complex, making it challenging to understand how they arrive at specific predictions. The lack of transparency and explainability can be problematic, especially when those predictions have significant consequences on individuals’ lives.\nInformed Consent: Related to data privacy is the issue of informed consent from individuals whose data are being used for predictive analytics. Informed consent is not always necessary but should be thoroughly throught through in many scenarios.\nAccuracy and Reliability: The accuracy and reliability of predictive analytics models are essential. Relying on inaccurate predictions could lead to detrimental decisions, especially in critical areas like healthcare or criminal justice.\nUnintended Consequences: Predictive analytics can influence human behavior and decisions. There’s a risk of unintended consequences, such hurting individuals who learn of their predictions, creating self-fulfilling prophecies, creating an environment of surveilance, or “creaming” (preferring) program participants or clients who are more likely to succeed.\nDiscrimination and Social Impact: Predictive analytics can impact individuals and communities differently. If not properly managed, it can exacerbate existing social disparities and contribute to discrimination.\nAlgorithmic Accountability: Holding the algorithms and those who develop them accountable for their predictions and decisions is a growing concern. Establishing responsibility and liability for the outcomes of predictive analytics is complex but essential.\nOverreliance on Predictions: Blindly relying on predictive analytics without human judgment and intervention can lead to overconfidence and neglect of important contextual factors."
  },
  {
    "objectID": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "href": "paforimprovingss_ethics.html#addressing-ethical-considerations",
    "title": "What are the ethical considerations for using PA to improve services?",
    "section": "Addressing ethical considerations",
    "text": "Addressing ethical considerations\nThe following are some practices that can help ensure that ethical considerations are raised, reviewed and consistently applied to all steps of a predictive analytics project. It is important for all these practices to consider multiple stakeholders - to include as much as possible the voices of all groups of individuals that are impacted or potentially impacted by the process and outcomes of predictive analytics.\n\nEthical Considerations in Predictive Analytics Projects\nPredictive analytics projects involve complex decision-making processes that can have significant impacts on individuals and society. To ensure ethical considerations are at the forefront of these projects, it is crucial to adopt a set of best practices that encompass multiple stakeholders and address potential biases and ethical implications. Here are some key practices to follow:\n\nUnderstanding Regulatory Frameworks: The first step is to be well-versed in relevant regulations and laws, such as the General Data Protection Regulation (GDPR) or similar data protection laws. These frameworks dictate how personal data should be collected, processed, and protected, ensuring that predictive analytics projects comply with legal requirements.\nMultidisciplinary Teams: Assemble diverse teams that include not only data scientists but also domain experts, ethicists, human-centered designers, and behavioral scientists. This multidisciplinary approach helps gain a comprehensive understanding of the population, program, or business involved and ensures that ethical considerations are thoroughly explored from various perspectives.\nDevelop a Code of Ethics: Establish a clear and comprehensive code of ethics specifically tailored to the predictive analytics project. This code should outline the ethical principles and guidelines that guide the entire project lifecycle, promoting responsible data use, transparency, and fairness. This is a good opportunity to bring in multiple stakeholders to co-develop a code of ethics that represents different concerns.\nTransparency in Model Development and Validation: Make model validation transparent by sharing the process and, if possible, the code used to develop the predictive models. Transparent model developement validation allows stakeholders to understand the model’s strengths and limitations, fostering trust in the project’s outcomes.\nEthical Review Board: Engage with an ethical review board to assess the potential ethical implications of the predictive analytics project, particularly in sensitive domains like healthcare, workforce hiring, or criminal justice. This board’s input and oversight ensure that the project aligns with ethical standards and respects individuals’ rights.\nIndependent External Audit: Conduct an independent audit by a third party to evaluate the project’s adherence to ethical guidelines and standards. External audits provide an impartial assessment of the project’s practices and identify areas for improvement.\nInformed Consent: Obtain explicit and informed consent from individuals before using their data for predictive analytics. Clearly communicate how their data will be used, ensuring transparency and empowering individuals to make informed decisions about their data’s usage.\nMinimizing Bias: Follow best practices for minimizing bias in the data used for training predictive models. Ensure the data is diverse and representative of the population to avoid biases and prevent unfair generalizations.\nOutcome Assessments: Conduct comprehensive outcome assessments to evaluate the potential ethical consequences of the predictive analytics project on individuals and society. This assessment aids in identifying and mitigating potential harms and ensuring responsible use of predictive insights.\nTraining on Ethical Interpretation: Provide training to individuals who will interpret, communicate, and act on the results of the predictive analytics project. This ensures that stakeholders are equipped with the knowledge and understanding needed to handle the information responsibly and ethically.\nContinuous Monitoring and Adaptation: Continuously monitor the performance and ethical implications of the predictive analytics system. Regularly assess the system’s outcomes and adapt as needed to address emerging ethical concerns. Additionally, implement feedback mechanisms for individuals to express concerns and provide input on data usage and project outcomes.\n\nBy adhering to these practices, organizations can foster a culture of ethical awareness and responsibility in predictive analytics projects, ensuring that they benefit all stakeholders while minimizing potential harms."
  }
]
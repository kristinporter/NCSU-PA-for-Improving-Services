---
title: "Training and validating learners"
---

ROUGH

Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Toolset, but I will comment on some alternative variations.

Recall that it is imperative to train in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.

We can sets aside a portion of the remaining data and designate it for validation. We can the uses the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness.

We can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to $v$**-fold cross-validation**, in which $v$ is the number of folders or partitions. The following diagram illustrates 5-fold cross-validation:

![](images/CV%20diagram.png){fig-align="center" width="8in"}

This diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Toolset does a combination of both. For a few of metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code.

The PA Toolset implements $v$-fold cross-validation for the user. The procedure does not need to be coded. Instead, the user specifies how the cross-validation should be implemented, following the guidance below:

### **Guidance for implementing** $v$**-fold cross-validation:**

-   *Number of folds:* A good default for $v$ is 5. $v$ can be increased when the sample size is large. Alternatively, the entire $v$-fold cross-validation procedure can be repeated more than once. That is, we can get results from 5 folds, then resample the folds so that we also get results from another 5 folds of the same size. This may be desirable if the sample size is small, resulting in high variability in the metrics across folds. However, our toolset currently does not allow for this.

-   *How to specify folds:* Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of $v$-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.

    -   NOTE check status of toolset for this.

-   *Stratification:* In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is "yes" or "1" for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few "yes" or "1" outcomes. To protect against this, we can stratify the cross-validation. This means we can randomly partition those observations for which the outcome = 1 and then randomly partition those observations for which the outcome = 0. Note that the PA Toolset will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.

### **Tuning with** $v$**-fold cross-validation:**

-   The PA Toolset will automatically optimize tuning parameters. It does this with $v$-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.

-   The comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the $v$ folds, will be selected as the "tuned learner." The user will then only view results of the tuned learner going forward.

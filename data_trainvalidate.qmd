---
title: "Training and validating learners"
---

Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Tool Set templates you will use, but I will comment on some alternative variations.

Recall that it is imperative to train in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.

We can set aside a portion of the non-test data and designate that portion for validation. We can then use the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness.

We can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to $v$-fold cross-validation, in which $v$ is the number of folders or partitions - or times we repeating the training and validation. The following diagram illustrates 5-fold cross-validation:

![](images/CV%20diagram.png){fig-align="center" width="8in"}

This diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Tool Set does a combination of both. For a few of metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code.

The PA Tool Set implements $v$-fold cross-validation for the user. The procedure does not need to be coded. Instead, the user specifies how the cross-validation should be implemented, following the guidance below:

### **Guidance for implementing** $v$**-fold cross-validation:**

-   *Number of folds:* A good default for $v$ is 5. $v$ can be increased when the sample size is large. Alternatively, the entire $v$-fold cross-validation procedure can be repeated more than once. That is, we can get results from 5 folds, then resample to get 5 different folds and therefore get another set of results from another 5 folds of the same size. This may be desirable if the sample size is small because there could be high variability in the metrics across folds. However, our tool set templates currently do not allow for this.

-   *How to specify folds:* Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of $v$-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.

-   *Stratification:* In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is "yes" or "1" for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few "yes" or "1" outcomes. To protect against this, we can stratify the cross-validation. This means we can (1st) randomly partition those observations for which the outcome = 1; and then (2nd) separately randomly partition those observations for which the outcome = 0; then (3rd) pair each partition with outcome =1 with a partition with outcome = 0 to get $v$ folds with equal proportions of each value of the outcome. Note that the PA Tool Set templates will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.

### **Tuning with** $v$**-fold cross-validation:**

-   The PA Tool Set will automatically optimize tuning parameters. It does this with $v$-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.

-   The comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the $v$ folds, will be selected as the "tuned learner." The user will then only view results of the tuned learner going forward.

### **An alternative approach to training and validation**

The approach of $v$-fold cross-validation has at least a few of limitations:

-   One, as mentioned above, if we use random splits of the data to create the $v$ folds, we miss the opportunity to to assess how a model built on past data performs in future data - unless we set up the folds to be aligned with time.

-   Two, we introduce "data leakage" in our validation process. Data leakage refers to the situation in which we are "peeking" at information about our validation or test data when we do our training. One way this peeking can happen is to use information from combined data when doing data preparation. For example, we may transform some predictor variables to be "mean centered," which means we subtract the mean of the predictor variable from each unit's value so that the mean of this new, transformed variable is zero. (Some machine algorithms will peform better when our measures are mean-centered.) With $v$-fold cross-validation, we rely on a single data set for training and validation even though we rely on different partitions for training vs. validation. Therefore, mean-centered predictors would have the same mean in the training subsets as in the validation subsets. As a result, training data and the subsequent model fit learn \"something\" about the validation data distribution. This can potentially lead to overfitting. We will return to the concept of data leakage in Part 2 of "Data for PA."

-   Three, with $v$-fold cross-validation, we are using the same procedure for both parameter tuning and for comparing the tuned learners. Therefore, we awing the

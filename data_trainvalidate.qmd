---
title: "Training and validating learners"
---

Once we have stored away our test data, we need to figure out how to use the rest of our data for training and validation. There are multiple approaches. Here we will go over how the training and validation are implemented in the PA Tool Set templates you will use, but I will comment on some alternative variations.

Recall that it is imperative to train in one dataset and to validate (assess performance and fairness) in a different dataset. This buffers against overfitting.

We can set aside a portion of the non-test data and designate that portion for validation. We can then use the remaining data for training. For example, we can set aside 20% of the remaining data for validation, and use the remaining 80% for training. That is, we fit all the learners with 80% of our data and compare predictions to the truth in 20% of the data, allowing us to compute various metrics of performance and fairness.

We can also repeat this process more than once. That is, we can let different portions of the data take turns for training and for validation. This procedure is referred to $v$-fold cross-validation, in which $v$ is the number of folders or partitions - or times we repeating the training and validation. The following diagram illustrates 5-fold cross-validation:

![](images/CV%20diagram.png){fig-align="center" width="8in"}

This diagram is showing that the data are split into 5 folds. Each fold takes a turn as a validation fold, while all the other folds are combined for learner training. When a fold takes a turn as a validation fold, we obtain predicted likelihoods for each observation in that fold. We can then compute metrics of learner performance and fairness for each fold. We can average the metrics across the 5 folds. Alternatively, we can use the predicted likelihoods that result across all the folds to compute metrics. Our PA Tool Set does a combination of both. For a few of metrics, it returns averages from across the folds. For remaining metrics, it relies on the predicted likelihoods. This will be reviewed when we turn to the code.

The PA Tool Set implements $v$-fold cross-validation for the user. The procedure does not need to be coded. Instead, the user specifies how the cross-validation should be implemented, following the guidance below:

### **Guidance for implementing** $v$**-fold cross-validation:**

-   *Number of folds:* A good default for $v$ is 5. Higher values of $v$ (more folds) can be preferable if you have a lot of data because the average performance across the folds will be closer to the true performance (less biased), but there will be more variability in the estimates (larger variance). More folds also leads to more computation time, which can be substantial. Alternatively, although also more computationally intensive, the entire $v$-fold cross-validation procedure can be repeated more than once. That is, we can get results from 5 folds, then resample to get 5 different folds and therefore get another set of results from another 5 folds of the same size. This may be desirable if the sample size is small because there could be high variability in the metrics across folds. (However, our tool set code templates currently do not allow for this.)

-   *How to specify folds:* Typically, the folds are randomly partitioned. However, it may be desirable to specify the folds to align with analytic goals. For example, a key limitation of $v$-fold cross-validation with random partitioning is that each fold consists of observations from different points in time. That is, if the data we are using for training and validation consists of 5 years of data, random partitioning would mean data from across all five years are used in training and in validation. Since our goal is to generalize a model over time, we might want to have each year of data serve as a validation fold.

-   *Stratification:* In some circumstances, we recommend stratifying the cross-validation. For example, if a positive or negative value of a binary outcome is rare (e.g. it is "yes" or "1" for less than 30 percent of the sample), then some folds could have an unlucky random draw with very few "yes" or "1" outcomes. To protect against this, we can stratify the cross-validation. This means we can (1st) randomly partition those observations for which the outcome = 1; and then (2nd) separately randomly partition those observations for which the outcome = 0; then (3rd) pair each partition with outcome =1 with a partition with outcome = 0 to get $v$ folds with equal proportions of each value of the outcome. Note that the PA Tool Set templates will ask if you want to stratify by a measure. If you enter a variable name, the stratified cross-validation will be automatically done for you.

### **Tuning with** $v$**-fold cross-validation:**

-   The PA Tool Set will automatically optimize tuning parameters. It does this with $v$-fold cross-validation. The code will automatically compare learner validation across a grid of different combinations of tuning parameters.

-   The comparisons will focus on just one of two metrics selected by the user - either the area under the curve of the receiver-operator curve (AUC ROC) or the area under the curve of the precision-recall curve (AUC PR). We will learn about these two measures later. The point to take away here is that the combination of tuning parameters that results in the highest AUC ROC or AUC PR, averaged across the $v$ folds, will be selected as the "tuned learner." The user will then only view results of the tuned learner going forward.

### **An alternative approach to training and validation**

Note that using the same cross-validation procedure for both parameter tuning and learner validation and comparison is not technically, completely correct. Using the same data twice for both goals is sometimes called \"double-dipping.\" That is, using the same data again to validate the performance of the tuned learner, we may get overly optimistic performance estimates (overfitting) because the models have already "seen" that data during the tuning process. To correct for this, there are two approaches:

*Kristen H: I'm confused here. Questions: (1) I guess I don't totally see how we are double dipping. If we searched across v-fold cross-validation all combinations of possible tuning parameter values + algorithm + predset (and if our selection was based only on a metric such as AUC ROC) then wouldn't we arrive at the same selection for testing as we would if we first found the optimally tuned model for each predset? We are not refitting the models/learners with the same data, we are using the CV results from the best tuned model/learner within each ML - so is this double-dipping? I'm sure I 'm missing something here. (2) Assuming we are double-dipping, is the worry that the resulting overfitting is different for different learners? That is, does it mean we could select the "wrong" learner when comparing validation performance (comparing metrics based on the v-fold CV)? Or does it just mean that the performance is inflated for all of them, so we expect even more decline in performance in the testing data (we expect worse performance anyway, especially if test data is from a different time period).*

1.  The most comon approach is to divide non-testing data into training and validation data. That is, similar to how we set aside a subset of the full data available for testing, we can set aside a subset of the remaining data for validation. In this case, we first conduct parameter tuning using $v$-fold cross-validation with the training data, but then compare the trained learners by their performance in the held-out validation data.
2.  Another less common approach is to use "nested $v$-fold cross-validation. In nested cross-validation, you have an outer loop of cross-validation for model validation, and an inner loop of cross-validation for parameter tuning. This allows you to use the same data for both tuning and validation, but in a way that prevents biased performance estimates. This approach may be preferable when there are not sufficient data to hold out a separate validation data set.

# Understanding performance metrics

In this section, we walk through the main performance metrics for comparing 
learner performance in the cross-validated training data. 
Following, you will be asked to select which metrics you wish to use for selecting
the best learner across tuning sets.
Some of these metrics will be used to evaluate the training process in this notebook,
while others will become more relevant during the next testing step.

## Review of binary prediction metrics

First, we review some terms related to binary predictions.

- the **true value** is the observed value of each unit (0 or 1).
- the **predicted probability** is the estimated probability of success, 
generated by the model (between 0 and 1).
This is the estimated likelihood that the actual value equals 1 
(as 1 indicates success in our data model).
- the predicted probability can be converted into a **predicted outcome** 
by comparing it to a selected threshold. 
- The **threshold** is a cutoff for defining success. 
All units with a predicted probability *equal to or above* the 
threshold are predicted to be a 1, and all units with a predicted 
probability *below* the threshold are predicted to be a 0.
A common threshold of choice is 0.5. 

We can categorize all predictions into four categories:

- a **true positive (TP)** is when both the predicted outcome and the observed outcome are 1.
- a **false positive (FP)** is when the predicted outcome is 1, but the observed outcome is 0.
- a **true negative (TN)** is when both the predicted outcome and the observed outcome are 0.
- a **false negative (FN)** is when the predicted outcome is 0, but the observed outcome is 1.

Note that the notations *P* and *N* in the four categories 
above refer to **predicted** positives and **predicted** negatives.
The *T* and *F* preceding the *P* and *N* is a determinant
of whether the predicted positives and negatives are correct or not.

We can define two more useful notations:

- *OP* is the number of **observed** positives (outcomes that are `1`).
- *ON* is the number of **observed** negatives (outcomes that are `0`).
- *PP* is the number of **predicted** positives (predictions that are `1`).
- *PN* is the number of **predicted** negatives (predictions that are `0`).

Finally, we can also summarize these categories into rates.

- the **true positive rate (TPR)** is also known as **sensitivity** or **recall** 
and is the number of true positives over the total number of 
positive observed values: TP / (TP + FN) = TP / OP.
- the **true negative rate (TNR)** is also known as **specificity** and 
is the number of true negatives over the total number of 
negative observed values: TN / (TN + FP) = TN / ON.
- the **false positive rate (FPR)** is the number of false positives over 
the total number of negative observed values: FP / (TN + FP) =  FP / ON.
- **specificity** is $1 - FPR$.
- the **false negative rate (FNR)** is the number of false negatives over 
the total number of positive observed values: FN / (TP + FN) = FN / OP.
- **precision** also known as **positive predictive value** is the number 
of true positives over the total number of predicted positives: TP / (TP + FP). 
- **negative predictive value** is the number of true negatives over 
the total number of predicted negatives: TN / (TN + FN).
- **false discovery rate** is the number of false positives over the
total number of predicted positives: FP / (FP + TP) = FP / PP.

## AUC ROC

AUC ROC is a metric of area under the curve (AUC) for a receiver operating characteristic (ROC) curve.
AUC ROC is a simple but effective metric for determining performance of binary models.
The higher the value, the better the model is at making predictions.

An ROC curve plots false positive rate on the x-axis against the true positive rate 
on the y-axis, *as the threshold changes.*
The first point in the bottom left corner of the ROC curve corresponds to the 
TPR and FPR at a threshold of 1.
Next, imagine we change the threshold to 0.95.
Then all units with a predicted value above 0.95 will be classified as `1`,
and all units with a predicted value below 0.95 will be classified as `0`.
At this threshold, most of the units we classify as `1` will be observed positives,
so we expect a high true positive rate and a low false positive rate.
As we decrease the threshold, we will classify more and more units as `1`,
which increases the false positive rate (numerator increases)
and decreases the true positive rate (denominator increases).
A good model has a *high true positive rate* and a *low false positive rate*, so 
points farther to the left and farther up are markers of a good model.

We usually compare a model's curve to a dotted line that goes straight up 
the diagonal from 0 to 1.
The dotted line corresponds to the performance of a random classifier.
A random classifier picks the class of each unit using a random coin flip, 
without any information about the unit.
Ideally, a trained model informed by the patterns in the data performs 
much better than a random classifier.

In the case of **imbalanced data**, when one of the categories is very rare, 
ROC curves can give misleading results and be overly optimistic of model performance.
As a rule of thumb, if your data has 10\% or less of units in one category,
you may have imbalanced data.
In this case, AUC of precision recall curve, discussed next, may be a better metric.
For more information about imbalanced data, see our short explainer attached 
in the reference folder on the subject, and why ROC curves may not 
perform well in that setting.

## AUC PR

AUC PR is a metric of area under the curve (AUC) for a precision recall (PR) curve.
AUC PR is a better metric than AUC ROC for **imbalanced data** set because AUC PR 
does not account for true negatives, unlike in AUC ROC.
As mentioned above, AUC ROC is a trade-off measurement curve between 
*true positive rate* (on the Y-axis) and *false positive rate* (on the X-axis). 

In **imbalanced data**, the *false positive rate* tends to remain *low* due to a 
large number of observed negatives. 
Recall that *false positive rate/FPR* = FP/(TN + FP). 
A larger number of observed negative values within the imbalanced data would imply 
a large denominator (because TN would be large) and hence, we would see a low false 
positive **rate** even as the **number** of false positives increases. 
Thus, AUC ROC can be a less informative and at times overly optimistic metric 
for imbalanced data.

AUC PR, on the other hand, is a trade-off between 
*precision/predictive positive rate* (TP/(TP + FP)) 
and *recall/sensitivity* (TP/(TP + FN)). 
*Precision* is solely based on predicted positive values (TPs and FPs) and is unaffected 
if there is a large number of observed negatives.
If there are few false positives, then there will be both a low false positive rate
(smaller numerator for FPR) and high precision (smaller denominator for precision).
If there are few false negatives, then there will be both a low false negative rate
(smaller numerator for FNR) and a high recall (smaller denominator for recall).
Therefore, a high AUC PR score is good because it implies low false positive
and false negative rates (just like a high AUC ROC score).

Unlike in the ROC curve, the true positive rate (recall/sensitivity) is on the X-axis.
Precision is plotted on the Y-axis.
Like in AUC ROC as well, the precision and recall are plotted on 
X and Y axis *as the threshold changes*. 

As in shown in **Figure 1.2** below, the PR plot starts at the top left 
corner, where the threshold is 1, and moves towards the direction of bottom right,
where the threshold is 0. 
At thresholds close to 1, the learner classifies almost none of the observations 
as `1`. 
Hence, recall (TP / (TP + FN)) would be low because there are very
few predicted positives (and thus few TP).
Precision (TP / (TP + FP)) would be high because there are few false positives. 
Towards the bottom right of plot at thresholds close to 0, the learner classifies almost
all observations as `1`.
Hence, recall (TP / (TP + FN)) would be high because there are very
few predicted negatives (and thus few FN).
Precision would be equal to the proportion of observed positives
(the denominator TP + FP = all observations because all are predicted to be positive).

As shown in **Figure 1.3**, a learner with a perfect AUC PR score 
(a perfect learner) would have a curve fitting perfectly towards the (1,1) 
coordinate space where you have perfect precision and recall. 
A learner with a good AUC PR score would have a curve bowing towards the (1,1) coordinate space 
and above the no skills classifier horizontal line. 

A no skills classifier is a learner that predicts every observation as `1`. 
Suppose the data is extremely imbalanced, and only **0.3** percent of the total observations are `1`.
With the no skills classifier predicting every observation as positive, the precision score would 
remain the same/constant regardless of the threshold. 
Hence, a no skills classifier's AUC PR curve is the horizontal line across the bottom of **Figure 1.3**.

**Figure 1.1** An example of ROC and precision-recall curves across different learners.

![](figures/compare.png)

**Figure 1.2** An example of a PR curve.

![](figures/auc_pr_threshold.png)

**Figure 1.3** An example of a PR curve across perfect, good and no skills learners.

![](figures/auc_pr_perfect_good_noskills.png)


# Overall learner performance

## Performance

We now compare the learners according to both ROC AUC and PR AUC.

```{r bestLearners}
best_roc_auc <- learnersMetrics$learnerName[
  which.max(learnersMetrics$mean_roc_auc)
]
best_pr_auc <- learnersMetrics$learnerName[
  which.max(learnersMetrics$mean_pr_auc)
]
```
According to the mean values of these metrics, the best learners are:

- **AUC_ROC**: `r best_roc_auc`
- **AUC_PR**: `r best_pr_auc`

It is common for these metrics to show different learners as the best performer.
We may want to consider the results holistically to select the best learners.
For example, consider the following toy result:

| learner      | AUC_ROC       | AUC_PR        | MSE |
|--------------|--------------:|--------------:|--------------:|
| learner_1    | 0.8           | 0.7           | 2.9  |
| learner_2    | 0.7           | 0.6           | 2.85 |

If we look at all the metrics, learner_2 is the best according to MSE.
However, it is only marginally better on MSE, and performs substantially worse 
on both AUC metrics compared to learner_1.
Thus, we would probably not consider learner_2 to be one of our 
best performing learners.

After understanding broad learner performance, it is often helpful to select a 
subset of the learners to examine in more detail.
For example, we may select only the best learners according to our 
performance metrics above.
Alternatively, we may want to select learners that are more interpretable, 
even if they are not the best performers.
For example, if a simple regression model performs *almost* as well as a more 
complicated algorithm, we may still want to select the regression model even 
if it is not the absolute best performer on our metrics.

# 

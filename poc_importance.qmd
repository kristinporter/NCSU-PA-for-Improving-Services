---
title: "The importance of a proof-of-concept"
---
![](images/shutterstock_poc.jpg){fig-align="center" width="8in"}

After we have scoped our predictive analytics project, the next step is to conduct a "proof-of-concept." That is, before getting too invested in incorporating predictive analytics into systems of service improvement, we need to investigate the following:

1.  **Given the available data, how well can we predict our outcome of interest?**

    When investigating this question, we will want to consider various metrics of model performance, which we will turn to in a later section. We will be concerned with how well our predictive models help us identify those who truly are at risk (or truly will realize the outcome of interest - this might not always be "risk" or conversely, "success" but I use this as a easier way to talk about it). We may also be concerned with the extent to which our model makes mistakes - either designating at-risk individuals as being not-at-risk, or not-at-risk individuals as being at-risk.
    

2.  **Given the available data, to what extent is there bias in our predictions?**

    When investigating this question, we will want to consider various metrics of bias, which we will also turn to in a later section. Here we will be concerned with whether and by how much model performance varies for different groups.

3.  **To what extent are complex predictive models worthwhile? Do they improve upon a simpler model or decision-making process?  Do they improve upon current practice?**

    When investigating this question, our focus is on contrasting predictive capabilities and potential biases across various prediction approaches. We start by looking at the simplest approach, which doesn't necessarily involve a statistical model at all. A simple decision-making process might involve taking a small number of measures and combining and weighting them in a straightforward manner based on prior knowledge. One version of this type of approach is choosing specific criteria based on these measures that put people into one predicted catgory or the other. For example, perhaps everyone who both has children and is below a certain income level are predicted to be high-risk, while all other people are considered to be low-risk. This type of approach is sometimes called a rules-based approach or a decision tree, among other terminology.
    
    Moving to more complex approaches, we could plug a small number of measures into a regression model to estimate their relationship with the outcome. Finally, the most complex approaches might involve adding more measures, potentially a large number, and/or using advanced, data-driven algorithms such as a support vector machine or random forest. 
    
    The crux then lies in weighing the pros and cons when comparing different models. Should a more complex approach yield superior results compared to a simpler one, it prompts a consideration: Are the improvements significant enough to offset potential drawbacks in terms of transparency and explainability? Additionally, do these improvements outweigh any potential challenges in implementing and sustaining a more intricate modeling process? 

4.  **Do stakeholders understand and trust the results from the "best" predictive model?**

    While stakeholders should be engaged in the PA scoping process, the proof-of-concept provides another opportunity for valuable consultation. Sharing results used to investigate the above questions supports transparency and trust in the process. Stakeholders' input and questions are essential to the goals of the proof-of-concept - to assess whether and how predictive analytics should be deployed, communicated and used to improve services.  

\

[For further consideration: Are there other questions you would want to investigate in a proof-of-concept?]{style="color:green"}


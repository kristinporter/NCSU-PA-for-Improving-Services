---
title: "Addressing bias"
html:
  css: styles.css
---

The following provides some guidance for addressing bias - suggestions for how to avoid bias when building models and for how to correct models when bias is detected. 

**Identify predictors that are likely to be less biased.** Less biased predictors in modeling lead to less biased models. Having a deep understanding of the measures in your data, including how they are collected and entreed, is important for identifying predictors that may be biased. When we discussed ethical considerations earlier in the class, we talked about the value of having subject-matter experts on the project team or as advisors. In general, researchers should consider identifying data variables that are believed to be measured similarly in the different groups of interest. For example, returning to our example of pretrial risk algorithms, data scientists might discount all minor drug possession cases and focus on convictions rather than arrests, or serious crimes,  which are less likely to be biased, when generating lists of potential risk factors. 

**Screen for predictors that have the same relationship to the outcome across different attributes (e.g., races).** Given that predictive models typically cannot specifically account for race (that is, include race as a predictor), such screening will be an important step. One way of screening is to fit initial models that include race by risk factor interactions to identify risk factors that predict outcomes differentially, and then exclude those risk factors from subsequent modeling. Data scientists can compare the overall predictive accuracy of the models fit to the restricted set of factors with the model fit to all factors to determine the cost in accuracy.

**Identify risk-assessment *outcomes* that are likely to be less biased.** Biased outcomes can affect both the construction of a predictive model and its evaluation. For example, in pretrial risk assessments, it might help to focus on chronic failure to appear for court dates rather than a single failure, as the latter might be more indicative of willful behavior rather than other factors such as lack of access to transportation. 

**To address bias that may be introduced by censoring, capture differential relationships between censoring and subgroups when imputing censored outcomes.** One of the approaches to address the censoring problem when validating predictive models is to impute missing outcomes for those units for which outcomes cannot be observed (e.g. new criminal activity among those detained during the pretrial period). This imputation could include race and other characteristics not used for the final predictive model. (As imputation is only used to build the model, these characteristics will not, in the end, be used as predictors by the final predictive model.) To the extent that the imputation captures any differential relationships between censoring and subgroups, the subsequent model-fitting process will not be as vulnerable to biases from censoring.

**Assess bias in the decision-making guidelines, in particular those focused on thresholds.** When validating predictive models, data scientists should assess the bias (and accuracy) implications of the decision-making guidelines, not just the predictive probabilities. Therefore, differences in AUC-ROC or AUC-PR are insufficient. For a reliable assessment of bias, it needs to be evaluated at the threshold(s) that drive decisions about who receives extra intervention. 

**Reassess bias over time.** The extent of bias may change over time. Because there may be changes in the population targeted by services, bias (as well as predictive performance) should be monitored frequently.


---
title: "Learner training & validation with code templates"
html:
  css: styles.css
---

![](images/barbells.jpg){fig-align="left" width="4in"}

## 02_Learner_Training_and_Validation

After making all specifications for your PA proof-of-concept in `01_Learner_Specification.Rmd`, all the learners you defined are trained in `02_Learner_Training_and_Validation.Rmd`. This happens in the code chunk replicated below. The user does not need to enter or edit any code within the chunk. Specifications that were entered in `01_Learner_Specification.Rmd` are loaded in the chunk that precedes this one and are passed into the `train_learners()` function.

```{r print_trainLearners, results='asis', echo=FALSE}
source("datasets/extract_chunk.R")
extract_chunk("datasets/02_Learner_Training_and_Validation.Rmd", "trainLearners")
code_lines <- readLines("trainLearners.R")
# Print the lines as a formatted code block
cat("```r\n")
cat(code_lines, sep = "\n")
cat("\n```\n")

#cat('<pre style="background-color: lightgray; padding: 10px; border-radius: 5px;">\n')
#cat(code_lines, sep = "\n")
#cat('\n</pre>\n')
```

As commented in the above chunk, the code returns information about couple of performance metrics abbreviated as AUC-PR and AUC-ROC. AUC-PR is the area under the curve (AUC) for a precision recall (PR) curve. AUC-ROC is the area under the curve (AUC) for a receiver operating characteristic (ROC) curve. We will return to explanations of these in a bit.

First, let's focus on the other information that is returned in the above code chunk - the predicted probabilities. For each observation (unit or row) in our dataset used for training and valiation and for each learner, the code returns a predicted probability of the outcome.

For example, imagine we are predicting whether a student will drop out of high school. Then, for each student, we get the following information, displayed for just one student.

```{r echo=FALSE}
allLearnersResults <- readRDS("~/Library/CloudStorage/OneDrive-K.E.PorterConsultingLLC/Git/NCSU-PA-for-Improving-Services/datasets/allLearnersResults.rds")
predProbs <- allLearnersResults$predProbs 
predProbs[which(predProbs$student_id==1),c("student_id","learnerName",".pred_yes","dropout")]
```

The numbers to the left (1, 8000, 15999, 23998) are the row numbers in the data.frame `predProbs` that correspond to the student with `student_id` = '1'. By examining the entries for `learnerName`, we can see that we conducted training for four different learners. These four different learners were combinations of two predictor sets ("bm" and "ms") and three modeling approaches ("glm", "lasso" and "random_forest"). Note that the predSet suffix of '_bm' signifies the "benchmark" predictor set. The suffix of '_ms' was chosen by the user for a second predictor set; it is not following any particular convention. The column `.pred_yes` tells us the probability that the outcome is equal to '1'. (While not displayed, `predProbs` also contains `pred_no`, the probability that the outcome is equal to '0'.) Finally, we also see the actual, observed outcome (`dropout`) for this student. The `predProbs` dataframe also includes all the other measures in the training/validation data; they are just not displayed here. 

The predicted probabilities listed for `.pred_yes` were estimated through the $v$-fold cross-validation procedure. Recall from the earlier section on cross-validation, that each observation takes a turn in a validation fold. After a learner was trained on the $v-1$ other folds, the resulting model was applied to the validation fold, producing a predicted probability for each observation in the validation fold.

## Summarizing the predicted probabilities

A helpful first step for understanding the validation results is to examine the distributions of the predicted probabilities. The following set of plots shows the distribution for each learner. The code chunk named "plotpredProbs" in `02_Learner_Training_and_Validation` will create these plots for you. 

As expected since most students do not drop out, the predicted probabilites tend to cluster around low values. Also, note that some learners produce greater variability than others. 

```{r plotpredProbs, warning=FALSE, message=FALSE, echo=FALSE}
source("datasets/plots.R")
source("datasets/learner_training_helpers.R")
source("datasets/performance_metrics.R")
library(tidyverse)
library(ggpubr)
modelResults <- allLearnersResults$learnersMetrics
predProbs    <- allLearnersResults$predProbs 
#learnersMetrics <- convert_metrics_wide(modelResults)
#learnersResults$learnersMetrics <- learnersMetrics
probsGraphs <- NULL
learnerNames <- unique(modelResults$learnerName)
for(i in 1:length(learnerNames))
{
  probsGraphs[[i]] <- 
    pred_prob_graph(
      predProbs = predProbs,
      learnerName = learnerNames[i]
    )
}
ggarrange(plotlist = probsGraphs)
```

Next, the notebook plots overlapping distributions of predicted probabilities for when the observed outcome = '1' and when the observed outcome = '0'. This illustrates how well each learner is able to separate out the predictive probabilities for the different classifications. 

```{r plotpredProbsTruth, warning=FALSE, message=FALSE, echo=FALSE}
learnerSpec <- readRDS("~/Library/CloudStorage/OneDrive-K.E.PorterConsultingLLC/Git/NCSU-PA-for-Improving-Services/datasets/learnerSpec.rds")
probsGraphs <- NULL
for(i in 1:length(learnerNames))
{
  probsGraphs[[i]] <- 
    pred_prob_graph(
      predProbs = predProbs,
      learnerName = learnerNames[i],
      splitCategory = learnerSpec$outcomeName
    )
}
ggarrange(plotlist = probsGraphs)
```

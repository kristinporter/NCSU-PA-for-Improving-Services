---
title: "Bias in how predicted probabilities are used"
html:
  css: styles.css
---

Here, we highlight three potential sources of bias in the first step of developing and using predictive analytics: estimating the predicted probabilities of the outcome of interest.

### Bias in the data

In the administrative data used to develop and validate pretrial assessments, some data elements are very likely to be inherently biased. A key challenge is that **crime data does not exist**. Data only captures contacts with police and decisions made after that. Therefore, to the extent that police activity and subsequent steps in the CJ are biased, the data is biased.

Consider measures of criminal activity, for example. One factor leading to recorded criminal activity is the likelihood of getting arrested. This likelihood can vary by race even when the true level of criminal activity is constant. For example, individuals who live in areas with elevated amounts of policing will tend to have more arrests than individuals with the same criminal behavior who live in areas with less policing. The level of policing in neighborhoods is often correlated with race. Therefore, White and Black populations with the same actual prevalence of criminal behavior will be arrested at different rates for such behavior.

![](images/pretrial_biasindata.png){width="7in"}

The resulting bias in the data affects both the risk factors used as predictors in a risk assessment tool and the outcomes predicted by a risk assessment tool -- such as failure to appear or new criminal activity during the pretrial period. An assessment that depends on risk factors that are biased may perpetuate this bias because it could score a Black defendant as at higher risk than a White defendant with the same true level of risk.

With respect to outcomes, particularly the outcome of new criminal activity, Black defendants awaiting trial and living in highly policed areas may be more likely to be picked up for similar offenses than White defendants awaiting trial in other areas. Therefore, even if the risk factors in a risk assessment are not biased, the evaluation of the risk assessment could suggest bias, as a larger proportion of Black defendants with the same risk score could have new arrests than did similar White defendants.

### Bias in modeling

Even if the risk factors and outcomes involved in a risk assessment are all measured without bias, the underlying model of a risk assessment could still potentially introduce bias. This situation can occur if the relationships between the risk factors and the outcome are different for different racial groups. Take, for example, a condition where pending charge predicts an outcome better for White defendants than Black ones (perhaps because the level of the pending charge among White people signals involvement in a relatively more serious crime level than pending charge among Black people -- due to potential bias towards Blacks in the assignment of charges). If a model is then fit to all the data, which would average the two trends, pending charge for Black people would predict a higher risk score than the truth.

![](images/pretrial_biasinmodel.png){width="7in"}
### Bias in censoring

A third source of potential bias in the first step of risk assessment – estimating the risk score – is potential bias from what we call "differential censoring." The idea here is that predictive models are generally fit to only those defendants not detained while awaiting trial because they are the only ones with complete data. For those who are detained, we don't know if they would have failed to appear or been involved in NCA if they had been released. But the resulting risk assessment tool that is developed will be applied to both groups. This missing information challenge can cause multiple problems. First, if the people who are detained are systematically different from the people who are not detained, the final models may not generalize: the models may not accurately predict risk for those people who were detained. If detention patterns differ by racial group, bias may be introduced by fitting models using different subsets of the racial groups. 

![](images/pretrial_biasincensoring.png){width="7in"}



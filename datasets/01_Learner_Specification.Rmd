---
title: "Learner Specification"
output:
  pdf_document:
editor_options:
  markdown:
    wrap: sentence
always_allow_html: true
---

See `Directions_for_SetUp_All_Notebooks.Rmd/.pdf` for explanations of setup specifications.
\
See `Directions_01_Learner_Spcifications.Rmd/.pdf` for directions for using the rest of this notebook.
\
Code between lines of `#---#` may need user edits.
Otherwise, no edits are needed.

# Setup

```{r Setup, warning=FALSE, message=FALSE}
#-------------------------#
dataInputPath  <- "data-r-objects/inputs"
dataOutputPath <- "data-r-objects/outputs"               
#-------------------------#

seed <- 2022

knitr::opts_chunk$set(
  cache = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 7
)

# sourcing helper scripts
source(here::here("R", "packages.R"))
source(here::here("R", "checks_01_03.R"))
source(here::here("R", "checks_helpers.R"))
```

# Step 1: Specify the prediction objective.

Enter short text description for outcome, timepoint and population.

```{r predDescription}
#-------------------------#
outcomeDescription    <- "student dropping out after 4 years"
timepointDescription  <- "when mid-semester grades are available"
populationDescription <- "all students"
#-------------------------#
```

# Step 2: Specify and load training, testing and meta data files.

Edit examples to enter file names for: (1) the data file that will be used for training and validating learners (`trainDataName`); (2) the data file that will be used later for testing the selected learner (`testDataName`); and (3) the meta file, which should apply to both `trainDataName` and `testDataName`.
All files should be R data files and therefore have extensions of ".rds".

```{r specifyPathAndFiles}
#-------------------------#
trainDataName <- "studentTrain_processed.rds"         
testDataName  <- "studentTest_processed.rds"          
metaDataName  <- "studentMeta.rds"        
#-------------------------#

modelsTable <- "Mods.rds"                       # do not edit
```

Here we load the files specified above.
No edits needed.

```{r loadFiles}
trainDat <- readRDS(here::here(dataInputPath, trainDataName))
testDat  <- readRDS(here::here(dataInputPath, testDataName))
metaDat  <- readRDS(here::here(dataInputPath, metaDataName))
availableModelingApproaches <- readRDS(here::here(dataInputPath, modelsTable))
```

# Step 3: Specify outcome measure and predictor set variables.

Enter the name of the column in the data files that holds the outcome variable.

**Reminder:** This tool set only supports binary outcomes at this time.
The outcome variable should be a factor variable with levels `0` and `1`, and with no missing values.

```{r outcomeColumn}
#-------------------------#
outcomeName <- "dropout"
#-------------------------#
```

Next, when specifying your predictor sets you **must** write predSet, underscore, then a short hand description for each one.
For example, below we start with **predSet_bm** where **bm** stands for baseline model.
The functions are expecting predictor sets specified with this convention.

Please also note that for every predictor you specify, if there is an accompanying missing indicator variable, you will have to add that in as well.

See `Directions_for_01_Learner_Specification.Rmd/.pdf` for more information. 

In the chunk below, specify the **benchmark** predictor set.


```{r specifyPredset_bm}
#-------------------------#
predSet_bm <- c("gender_SS_i",
                "prev_GPA_SS_i_s",
                "num_classes_SS_i",
                "fam_income_SS_i_s",
                "miss_gender_SS",
                "miss_prev_GPA_SS",
                "miss_num_classes_SS",
                "miss_fam_income_SS")
#-------------------------#
```

Next, specify additional predictor sets.
If you would like a predictor set to include and build on all the predictors in the benchmark set, then be sure to include add predSet_bm to your new list of variable names. (See Example 1 in the chunk below.)

If you have a large set of predictors, you may want to also select predictors programmatically, using the metafile, rather than typing in long lists of variable names. (See Example 2 in the chunk below.)

```{r specifyPredset_others}
# EXAMPLE 1
# you can specify predictor sets by hand
predSet_ms <- unique(c(predSet_bm, c("cur_GPA_MS_i","miss_cur_GPA_MS")))
print(predSet_ms) # run this line to check the set is what was intended

# EXAMPLE 2: specify predictor sets based on info in the metadata file (e.g., all variables available at timepoint MS)
#predSet_ks <- colnames(trainDat)[!is.na(metaDat$pred_MS) & metaDat$pred_MS == 1]
#print(predSet_ks)

```

Enter all predSets into a list, editing the example below.

```{r allPredsets}
#---------------------#
allPredsets <- list(
 predSet_bm = predSet_bm,
 predSet_ms = predSet_ms
)
#---------------------#
```

# Step 4: Specify modeling approaches and match with predictor sets.


A list of the available modeling approaches is printed when the chunk below is run.

```{r availableModelingApproaches}
datatable(availableModelingApproaches,
  class = "cell-border stripe",
  rownames = FALSE
)
```

In this step, specify which modeling approaches will be used in combination with each predictor set.
See the `Directions_for_01_Learner_Specification.Rmd/.pdf` for further guidance.

```{r specifyModels}
#-------------------------#
specifiedLearners<- list(
#  predSet_bm = c("glm", "lasso", "svm_rbf"),
#  predSet_ms = c("glm", "random_forest", "xgboost")
  predSet_bm = c("glm"),
  predSet_ms = c("glm", "lasso","random_forest")
)
#-------------------------#

checkSpecifyModels(
  availableModels = availableModelingApproaches$modelingApproachName,
  specifiedLearners = specifiedLearners
)
```

The learner training relies on the `tidymodels` package.
Specifications for the package include "recipes," which help users prepare the data.
However, the workflow for our predictive analytics framework advises users to prepare data as an early step, and because we allow for multiple predictor sets, recipe code can be very redundant.
Therefore, we specify only very limited recipes - just in order to make `tidymodels` work.
We simply do a check to ensure that the outcome variable is a 2-level factor.
The only edits required are to match the recipes code to incorporate all predsets specifed above.

```{r specifyRecipes}
# specify formulas for each predictor set
# edit to have one formula for each predictor set 
# edit suffix in predSet_suffix_forumla of formula name & predSet_suffix within formula
#-------------------------#
predSet_bm_formula <- as.formula(
  paste(outcomeName, "~", paste(predSet_bm, collapse = "+"))
)
predSet_ms_formula <- as.formula(
  paste(outcomeName, "~", paste(predSet_ms, collapse = "+"))
)
#-------------------------#

# create recipes for each predictor set
# edit to have one formula for each predictor set 
# edit suffix in recipe_suffix of recipe name & predSet_suffix_forumla within recipe
#-------------------------#
recipe_bm <- recipe(predSet_bm_formula, data = trainDat) %>%
  step_bin2factor({{outcomeName}})
recipe_ms <- recipe(predSet_ms_formula, data = trainDat) %>%
  step_bin2factor({{outcomeName}})
#-------------------------#

# create "main" formula and recipe
predSet_main_formula <- as.formula(
  paste(outcomeName, "~", paste(".", collapse = "+"))
)
recipe_main <- recipe(predSet_main_formula, data = trainDat)

# aggregate and save - edit to include all recipes
#-------------------------#
recipes <- list(
  predSet_bm = recipe_bm,
  predSet_ms = recipe_ms
)
#-------------------------#

# this is automated - no edits required
saveRDS(recipes, file = here::here(dataOutputPath, "recipes.rds"))
saveRDS(recipe_main, file = here::here(dataOutputPath, "recipe_main.rds"))
```

# Step 5: Specify choices for cross validation.

Here, for the cross validation procedure, we specify the following:

-   The folds: We can either choose the number of folds (e.g., 5), or we can choose a categorical variable that is a column in the data frame in order to produce folds.

-   Stratification: In some contexts, we may have an imbalanced binary outcome.
    As a rule of thumb, if your data has 30% or less of units in one category, you may consider your outcome to be imbalanced.
    In this case, you may want to stratify your sampling to ensure that each sample is representative.
    Similarly, if there is an important predictor is imbalanced (e.g. site, region, etc.), you may want to stratify on that predictor, although that is less common.
    You can specify the name of the variable in quotation marks to assign it to `stratVar`.

-   Grid size: We use cross-validation to pick the best values of tuning parameters for machine learning algorithm in combination with a predictor set.
    We test performance over a grid of possible tuning parameter values.
    The larger the grid, the slower the fitting process, but the more likely to get learners that perform well.

-   Performance metrics for parameter tuning: The tuning parameters need to be tested against a performance metrics, too.
    Depending on the balance of the data, we have two options.
    If balanced, we recommend **roc_auc**.
    If imbalanced, we recommend **pr_auc**.

```{r crossValidation}
# cross validation folds
# Examples:
# cvFolds <- 5
# cvFolds <- "region"
#-------------------------#
cvFolds <- 5
#-------------------------#

# name of variable for stratification if needed
#-------------------------#
stratVar <- NULL
#-------------------------#

# Grid size
#-------------------------#
gridSize <- 5
#-------------------------#

# Tune metrics (the options are "roc_auc" or "pr_auc")
#-------------------------#
tuneMetrics <- "roc_auc"    
#-------------------------#
```

# Step 6: Select characteristics for which to assess fairness.

The variables for comparing performance for the purpose of fairness must be categorical.
For a variable with multiple categories, such as race, you can either consider a single factor variable with multiple categories ("white", "black", etc), or a collection of binary dummy variables (race_white: 0/1, etc.).
As a first pass, we recommend starting with a multi-category factor variable.
If the multi-category factor variable doesn't show problems with fairness, you can proceed.
However, if the multi-category variable shows fairness problems, we can then look into individual dummy variables as a way to investigate exactly where the discrepancies are occurring.

```{r equityVariables}
#-------------------------#
equityVars <- c("gender_SS_i", "race_SS_i")
#-------------------------#

# right now, we only support categorical variables for equityVars
equityVarsCheck <- apply(trainDat[,equityVars], 2, function(x) { is.factor(x) || is.character(x) } )
```

# Step 7: Aggregate and save all specifications.

Here the code automatically aggregates and saves all of the above information into a list object called learnerSpec, which contains all specifications of your learners.
This information will be loaded by the next notebook in our predictive analytics pipeline - 02_01_Learner_Training.Rmd Note the aggregation below contains only information about the data measures and algorithms.
It does not write out any data.
For data security purposes, we highly recommend that users to not modify this object to include any data.

```{r learnerSpecSave}
# Save cross validation parameters in a list
cvParams <- list(
  cvFolds = cvFolds,
  seed = seed,
  gridSize = gridSize, 
  tuneMetrics = tuneMetrics,
  stratVar = stratVar
)

# Save out label descriptions
learningObjectiveLabels <- data.frame(
  outcomeDescription = outcomeDescription,
  timepointDescription = timepointDescription,
  populationDescription = populationDescription
)

# Pulling out the training data's observed outcome to estimate positive outcome proportion
obsOutcome <- as.numeric(trainDat[[outcomeName]])
# Estimate the proportion of the positive class in the training data set
positiveProportion <- sum(obsOutcome)/length(obsOutcome)

learnerSpec <- list(
    dataInputPath = dataInputPath, 
    dataOutputPath = dataOutputPath,
    trainDataName = trainDataName,
    testDataName = testDataName,
    metaDataName = metaDataName,
    learningObjectiveLabels = learningObjectiveLabels,
    outcomeName = outcomeName,
    allPredsets = allPredsets,
    specifiedLearners = specifiedLearners,
    cvParams = cvParams,
    equityVars = equityVars,
    positiveProportion = positiveProportion
)
saveRDS(learnerSpec, file = here::here(dataOutputPath, "learnerSpec.rds"))
```

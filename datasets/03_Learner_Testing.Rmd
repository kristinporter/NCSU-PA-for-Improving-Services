---
title: "Learner Testing"
output:
  pdf_document:
always_allow_html: true
---

See `Directions_for_SetUp_All_Notebooks.Rmd/.pdf` for explanations of setup specifications.
\
See `Directions_03_Learner_Testing.Rmd/.pdf` for directions for using the rest of this notebook.
\
Code between lines of `#---#` may need user edits.
Otherwise, no edits are needed.

# Setup

```{r Setup, warning=FALSE, message=FALSE}
#-------------------------#
dataInputPath  <- "data-r-objects/inputs"
dataOutputPath <- "data-r-objects/outputs"               
#-------------------------#

seed <- 2022

knitr::opts_chunk$set(
  cache = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 7
)

options(scipen = 999)

# sourcing helper scripts
source(here::here("R", "packages.R"))
source(here::here("R", "plots.R"))
source(here::here("R", "learner_training.R"))
source(here::here("R", "learner_training_helpers.R"))
source(here::here("R", "performance_metrics.R"))
source(here::here("R", "equity_metrics.R"))

# loading output from previous notebooks
learnerSpec <- readRDS(file = here::here(dataOutputPath, "learnerSpec.rds"))
trainResults   <- readRDS(file = here::here(dataOutputPath, "validateLearnersResults.rds"))
trainMetrics   <- trainResults$learnersMetrics
trainPredProbs <- trainResults$predProbs
recipes <- readRDS(file = here::here(dataOutputPath, "recipes.rds"))
mainRecipes <- readRDS(file = here::here(dataOutputPath, "recipe_main.rds"))
```


# Step 1: Data checks

## Check that data used for testing has same variables as data used for training and validation

First, we load data files and recipes (because tidymodels needs them). No edits needed. 

```{r loadData}
# load data
trainDat <- readRDS(here::here(
  learnerSpec$dataInputPath, learnerSpec$trainDataName
))
testDat <- readRDS(here::here(
  learnerSpec$dataInputPath, learnerSpec$testDataName
))

# apply recipes
trainDat <- mainRecipes %>%
              recipes::prep() %>%
              recipes::bake(new_data = NULL)

trainDat <- mainRecipes %>%
              recipes::prep() %>%
              recipes::bake(new_data = NULL)
```

Then we check if the testing data set has the same number of predictors as the training (and validation) data set.

```{r ncolCheck}
if(ncol(trainDat) == ncol(testDat)){
  print("Training and testing data sets have the same number of predictors.")
} else {
  print("Training and testing data sets have different number of predictors.
        Please check further below.")
}
```

We check if the training data set has the same variable names as the testing data set.

```{r predictorNameCheckTraining}
# boolean vector to see which training data columns are in testing data
colsTrainingInTesting <- names(trainDat) %in% names(testDat)

if(sum(colsTrainingInTesting) == ncol(trainDat)) {
  print("All the predictors of the training data set are in the testing data set.")
} else {
  leftOutColnames <- names(trainDat)[which(!colsTrainingInTesting)]
  print(paste0("The following predictors of the training data set are not in the testing data set.",
               leftOutColnames))
}
```

We check if the testing data set has the same predictor names as the training data set.

```{r predictorNameCheckTesting}
# boolean vector to see which testing data columns are in training data
colsTestingInTraining <- names(testDat) %in% names(trainDat)

if(sum(colsTestingInTraining) == ncol(testDat)) {
  print("All the predictors of the testing data set are in the training data set.")
} else {
  leftOutColnames <- names(testDat)[which(!colsTestingInTraining)]
  print(paste0("The following predictors of the testing data set are not in the training data set.",
               leftOutColnames))
}
```

If `show_varlist==TRUE`, the table below lists all variables in the training and testing data sets.

```{r colNamesTable}
#-------------#
show_varlist <- TRUE
#-------------#
# ordering alphbetically so that both lists are printed in the same order
oo.train <- order(colnames(trainDat))
oo.test <- order(colnames(testDat))

varComparison <- data.frame(trainVars = colnames(trainDat)[oo.train],
                            testVars = colnames(testDat)[oo.test])
if(show_varlist) print(varComparison)
```

## Data summaries

Here we make sure there is not a categorical variable in the test data that has a new, unseen category, as this could break learners. If catVarsTest is empty, the test is passed. 

```{r CatVars}
catVarsTrain <- trainDat %>%
  dplyr::select(which(sapply(., function(x) is.character(x) | is.factor(x))))
catVarsTest <- testDat %>%
  dplyr::select(which(sapply(., function(x) is.character(x) | is.factor(x))))

summary(catVarsTest)
```

Here we compare summaries of training and validation data and testing data. This allows for consistency checks of distributions and of missingness across data sets. 

Remember that for mean-centered variables, the mean of the training data were used. 

```{r trainSummary}
summ_train <- data.frame(var=colnames(trainDat)[oo.train])
des_train <- trainDat[oo.train] %>% describe() %>% as_tibble() %>% select("n","min","max","mean","median")
summ_train <- cbind(summ_train,des_train)
summ_train$nmiss <- apply(is.na(trainDat[oo.train]),2,sum)
summ_train$pmiss <- round(summ_train$nmiss/summ_train$n,3)
summ_train <- as.data.frame(lapply(summ_train, formatC,digits=2,format="f"))
colnames(summ_train) <-paste0(colnames(summ_train),"Train")

summ_test <- data.frame(var=colnames(testDat)[oo.test])
des_train <- trainDat[oo.test] %>% describe() %>% as_tibble() %>% select("n","min","max","mean","median")
summ_test <- cbind(summ_test,des_train)
summ_test$nmiss <- apply(is.na(testDat[oo.train]),2,sum)
summ_test$pmiss <- round(summ_test$nmiss/summ_test$n,3)
summ_test <- as.data.frame(lapply(summ_test, formatC,digits=2,format="f"))
colnames(summ_test) <-paste0(colnames(summ_test),"Test")

cbind(summ_train[,c("varTrain","minTrain","maxTrain")],summ_test[,c("varTest","minTest","maxTest")])
cbind(summ_train[,c("varTrain","medianTrain","meanTrain")],summ_test[,c("varTest","medianTest","meanTest")])
cbind(summ_train[,c("varTrain","pmissTrain")],summ_test[,c("varTest","pmissTest")])

```
# Step 2: Fit learner on full training data set and apply to test data

The code chunk below is fitting the selected learners on the whole training data set and computing performance metrics in the test data.

```{r testingRun}
testResults <- test_learners(trainResults = trainResults, 
                             learnerSpec = learnerSpec,
                             mainRecipes = mainRecipes)

testMetrics    <- testResults$testMetrics
testPredProbs  <- testResults$testPredProbs
testRocResults <- testResults$testRocResults
testPrResults  <- testResults$testPrResults
testFits       <- testResults$testFits
```


# Step 3: Review distributions of predicted probabilities

The following code chunk displays the distribution of the predicted probabilities in the test set and compares this to the distribution of the predicted probabilities from the same learner when validated (through cross-validation) in the training/validation set.
If the distributions are substantially different, it could highlight issue of overfitting.

```{r predProbs}
# collect all predicted probabilities
testPredProbs$set  <- "Test"
trainPredProbs$set <- "Train"
allPredProbs <- bind_rows(trainPredProbs, testPredProbs)
```

```{r plotpredProbs}
ggplot(allPredProbs, aes(.pred_yes)) +
  geom_histogram() +
  xlab ("Predictive probabilities") +
  ylab ("Count") +
  ggtitle(paste("Predictive probability distributions")) +
  facet_grid(set ~ learnerName) +
  scale_x_continuous(limits = c(0,1)) +
  theme (plot.title = element_text(size = 12,
                                   face = "bold",
                                   vjust = 1,
                                   hjust = 0.5))
```


```{r plotpredProbsTruth}
# ensure outcome factor for plotting
outcomeName <- learnerSpec$outcomeName
allPredProbs <- allPredProbs %>% 
  dplyr::mutate_at(outcomeName, as.character) %>% 
  dplyr::mutate_at(outcomeName, as.factor)

ggplot(allPredProbs,
       aes(x = .pred_yes, fill = get(outcomeName), alpha = 0.5)) +
  geom_density() +
  facet_grid(set ~ learnerName) +
  xlab ("Predicted Probabilities") +
  ggtitle (paste("Predicted probability distribution\n",
                  "across training data's ground truth")) +
  scale_x_continuous(limits = c(0,1)) +
  scale_fill_manual(
    name = outcomeName,
    values = c("no" = "salmon2","yes" = "seagreen2")) +
  theme (plot.title = element_text(size = 12,
                                   face = "bold",
                                   vjust = 1,
                                   hjust = 0.5)) +
  guides(alpha = "none")
```


# Step 3: Compute test performance metrics

## Performance metrics based on predicted probabilities (threshold invarient metrics)

The next code chunks displays the AUC ROC and AUC PR computed from the testing data and for comparison, also displays the AUC and AUC PR from the $v$ cross-validation (the averages across the folds). 

```{r traintestMetrics}
print(testMetrics[,c("learnerName","roc_auc","pr_auc")])
print(trainMetrics[,c("learnerName","mean_roc_auc","mean_pr_auc")])
```

The next two code chunks plot the ROC and PR curves for the test data results. 

```{r plotROCCurves}
ggplot(testRocResults,
    aes(x = 1 - specificity, y = sensitivity)) +
    geom_path(aes(color = ML, linetype = predSet)) +
    geom_abline(color = 'black') +
    coord_equal() +
    theme_bw() +
    ggtitle('ROC Curve')
```

```{r plotPRCurves}
# Pulling out the test data's observed outcome to estimate positive outcome proportion
obsOutcome <- ifelse(testDat[[learnerSpec$outcomeName]] %in% c("yes"), 1,0)
# Estimate the proportion of the positive class in the test data set
positiveProportion <- sum(obsOutcome)/length(obsOutcome)

ggplot(testPrResults,
    aes(x = recall, y = precision)) +
    geom_path(aes(color = ML, linetype = predSet)) +
    geom_hline(yintercept = positiveProportion, color = "black") +
    geom_text(data = data.frame(x = 0.25, 
                                y = positiveProportion), 
              aes(x,y), 
              label = "No skills classifier", 
              vjust = 1,
              size = 3) +
    # coord_equal() +
    theme_bw() +
    ggtitle('Precision-Recall Curve')
```
## Performance metrics based on classifications (threshold-based metrics or confusion matrix metrics)

The following code chunk computes the following metrics for thresholds
sequenced from 0 to 1 by intervals of 0.05. These are computed from the
predicted likelihoods estimated across all validation folds.

-   tpr: true positive rate, tpc: true positive count

-   tnr: true negative rate, tnc: true negative count

-   fpr: false positive rate, fpc: false positive count

-   fnr: false negative rate, fnc: false negative count

-   ppv: positive predictive value (precision)

-   pp: predicted positives

-   pn: predicted negatives

-   op: observed (true) positives

-   on: observed (true) negatives

```{r learnerMetricsThresBackground, cache = TRUE}
learnerMetricsThres <- conf_metrics_thres_learner(
    predProbs = testPredProbs,
    outcomeName = learnerSpec$outcomeName,
    thresSeq = seq(0.0, 1.00, 0.05) # No need to change unless more want metrics computed for more thresholds
) 
```

Using the above results (`learnerMetricsThres`), we now repeat the steps we followed in `02_Learner_Training_and_Validation" but just for the learner we are testing. We can view results using the same optimal threshold from the validation, and we can look at results at other thresholds. 

**(A) View metrics for user-specified and distribution-based
thresholds**

The following code chunk presents all threshold-based performance
metrics for the following thresholds for a *single, user-specified
learner*:

-   User-specified (for example, 0.5)

-   The threshold approximately equal to the median of the predicted
    probabilities. Note this threshold may not be helpful for situations
    in which the distribution of predicted probabilities is highly
    skewed or bimodal. Also note our code computes metrics from 0 to 1
    in increments of 0.05, so here we presents metrics for the threshold
    increment closest to and less than the computed threshold. For
    example, if the median = 0.12, then we present results for the
    threshold of 0.10.

-   The threshold at which the predicted proportion of observations with
    a positive outcome (outcome =1) matches the observed proportion of
    observations with a positive outcome. For example, if the outcome =
    1 for 30% of the observations, then we find the threshold at which
    30% of the observations are predited to be 1.

```{r learnerThresPresent}
#-------------------------#
thresSelect <- 0.25 # user-specified
#-------------------------#

learnerNameSelect <- trainResults$learnerNames

predProbsSelect <- testPredProbs %>%
  dplyr::filter(learnerName %in% learnerNameSelect)
thresValues <- predProbsSelect %>% 
  dplyr::summarise(
    selected_thres = thresSelect,
    median_thres = round(median(.pred_yes), 2),
    positive_proportion_thres = round(quantile(.pred_yes, probs = (max(.pred_yes) - learnerSpec$positiveProportion)),2)
  )
#print(thresValues)

learnerRows <- learnerMetricsThres %>%
  filter(learnerName == learnerNameSelect)

# threshold set at thresSelect
learnerMetricsThresSelect <- learnerRows[
  c(which(learnerRows$threshold == thresSelect),
    which.min(abs(learnerRows$threshold - thresValues$median_thres)),
    which.min(abs(learnerRows$threshold - thresValues$positive_proportion_thres))),
]

learnerMetricsThresSelect
```

**(B) View metrics for criteria-based thresholds:**

Here we can find a threshold based on a criteria, for example, related
to a resource constraint or rate of intervention.

-   Example 1: A program only has enough funds to intervene for 1000
    individuals. Then, we can find the threshold such that there are
    1000 true positive counts.

-   Example 2: For treatment of a rare cancer, we want to ensure a true
    positive rate of at least 90%.

```{r thresPicker1}
# learner name
learnerNameSelect <- trainResults$learnerNames

#-------------------------#
# name of metric to use as criteria
# options: tpr, tpc, tnr, tnc, fpr, fpc, fnr, fnc, pp, pn
metricName <- "pn" # Example 1
# metricName <- "tpr" # Example 2
# selection criteria for metric
metricNumber <- 1000 # Example 1
# metricNumber <- 0.9 # Example 2
# whether we want to consider only options below or
# above the metric number, e.g. predicted positives BELOW 100
below <- TRUE
#-------------------------#

thres1 <- pick_threshold(
  learnerMetricsThres,
  learnerNameSelect = learnerNameSelect,
  metricName = metricName,
  metricNumber = metricNumber,
  below = below
)
print(thres1)
```

This is the threshold among those for which metrics were computed (0 to
1 by increments of 0.05) that achieves the target criteria. The targeted
metric may be lower or higher. The next chunk displays the full set of
metrics at this threshold for the selected learner.

```{r thresSummary1}
thres1Row <- learnerMetricsThres %>%
  filter(learnerName == learnerNameSelect, threshold == thres1)

thres1Row
```

**(C) View a narrowed set of metrics and create a
nice plot**

Here the user can specify (i) multiple learners, (ii) multiple
thresholds, and then (iii) metrics with the choices of

-   "rate" for tpr, fpr, tnr, fnr

-   "count" for tpc, fpc, tnc, fnc

-   any single measure covered by "rate" or "count", or ppv (for
    precision) or fdr (false discovery rate)

```{r thresRange}
#-------------------------#
# thresholds for evaluation
selectedThresholds <- c(0.20, 0.25, 0.30) 
# Metric options: count or rate
metricType <- "rate"
#-------------------------#

selectedLearnersNames <- trainResults$learnerNames

if(metricType == "count")
{ selectedMetrics = c("tnc", "fnc", "fpc", "tpc")
} else if(metricType == "rate")
{ selectedMetrics = c("tnr", "fnr", "fpr", "tpr")
}

learnerMetricsThresLong <-
  learnerMetricsThres %>%
  dplyr::filter(learnerName %in% selectedLearnersNames) %>%
  dplyr::mutate(thresholdC = as.character(threshold)) %>%
  dplyr::filter(thresholdC %in% as.character(selectedThresholds)) %>%
  dplyr::select(learnerName, threshold,!!selectedMetrics) %>%
  tidyr::pivot_longer(-c(learnerName, threshold),
                      names_to = "metricType",
                      values_to = "value" ) %>%
  dplyr::mutate(threshold = as.factor(threshold))

print(learnerMetricsThresLong)
```



```{r thresRangePlot}
# THIS PLOT ONLY WORKS FOR COMPARING UP TO 4 LEARNERS. 
#-------------------------#
# Insert description of your predictive negative and positive in that order
# For e.g. c("Predicted as \n NOT graduating", "Predicted as \n graduating")
# Note: \n stands for new line
outcomeDescription <- c("Predicted as \n NOT dropping out", "Predicted as \n dropping out")
# Insert the title of the graph
graphTitle <- "Learner performance on training data"
#-------------------------#

p <- conf_mat_bar_plot(plotData = learnerMetricsThresLong,
                       metricType = metricType,
                       outcomeDescription = outcomeDescription,
                       graphTitle = graphTitle)

print(p)
```


# Step 4: Check bias and fairness


### Distributions of predicted likelihoods by characteristic

For each learner specified, we show the distributions of predicted
probabilities by category of a specified characteristic.

```{r plotpredProbsFairness1}
#-------------------------#
equityVarSelect <- "gender_SS" # limit to one
#-------------------------#

learnerNamesSelect <- trainResults$learnerNames

probsGraphs <- NULL
for(i in 1:length(learnerNamesSelect))
{
  probsGraphs[[i]] <- 
    pred_prob_graph(
      predProbs = testPredProbs,
      learnerName = learnerNamesSelect[i],
      splitCategory = equityVarSelect
    )
}
ggarrange(plotlist = probsGraphs)
```

#### Threshold-based metrics by categories of a characteristic for a selected learner

In the next chunk, we compare threshold-based performance metrics for a
(i) a specified characteristic, (ii) a specified learner, and (iii) up
to four thresholds. The metrics displayed are narrowed to "counts" or
"rates" as described above. The user also narrows the metric with the
choices of

-   "rate" for tpr, fpr, tnr, fnr

-   "count" for tpc, fpc, tnc, fnc

-   any single measure covered by "rate" or "count", or ppv (for
    precision) or fdr (false discovery rate)

```{r thresEquityPlot1}
#-------------------------#
equityVarSelect    <- "race_SS"
outcomeDescription <- c("Predicted as \n NOT dropping out", "Predicted as \n dropping out")
# Insert the title of the graph
graphTitle         <- "Learner fairness from cross-validation"
metricType         <- "fdr" 
#-------------------------#

learnerNameSelect <- trainResults$learnerNames
selectedMetrics <- get_selectedMetrics(metricType)

# subset down to just that learner
learnerPredProbs <- testPredProbs %>%
  filter(learnerName == learnerNameSelect)

learnerMetricsThresEquity <- conf_metrics_thres_learner(
    predProbs = testPredProbs,
    outcomeName = learnerSpec$outcomeName,
    thresSeq = seq(0.0, 1.00, 0.05), # Do not change. Fixed by Notebook.
    equityVar = equityVarSelect 
)

learnerMetricsThresEquityLong <-
  learnerMetricsThresEquity %>%
  dplyr::mutate(thresholdC = as.character(threshold)) %>%
  dplyr::filter(threshold %in% as.character(selectedThresholds)) %>%
  dplyr::select(learnerName, threshold, all_of(equityVarSelect), !!selectedMetrics) %>%
  tidyr::pivot_longer(-c(learnerName, threshold, all_of(equityVarSelect)),
                         names_to = "metricType",
                         values_to = "value" ) %>%
  dplyr::mutate(threshold = as.factor(threshold))

p <- conf_mat_bar_plot(plotData = learnerMetricsThresEquityLong,
                       equityVar = equityVarSelect,
                       metricType = metricType,
                       outcomeDescription = outcomeDescription,
                       graphTitle = graphTitle)

print(p)
```

#### Threshold-based metrics for all characteristics, all learners and all metrics, for selected thresholds.

Here, we select a limited number of thresholds, but look at all
threshold-based metrics, comparing categories of all characteristics
specified in `01_Learner_Specification.Rmd` for the selected, learner being tested. 

```{r thresSelect}
#-------------------------#
thresholdsSelect <- selectedThresholds # default is same as selected above, or this can be edited to specify a different set of thresholds
#-------------------------#

learnerNames <- unique(trainResults$learnerNames)

equityResults <- calc_equity_metrics_all(
  predProbs = testPredProbs,
  learnerNames = learnerNames,
  thresholdsSelect = thresholdsSelect,
  equityVars = learnerSpec$equityVars,
  outcomeName = learnerSpec$outcomeName
)
```

With the following chunk, there is the option to narrow the learners,
characteristics or thresholds among the sets specified above.

```{r fairnessDiffExploration, warning = FALSE, message = FALSE}
#-------------------------#
equityVarsSelect <- "race_SS_i"
thresholdsSelect <- selectedThresholds
#-------------------------#

learnerNameSelect <- trainResults$learnerNames

equityResultsSubset <- equityResults %>%
  filter(learnerName %in% learnerNameSelect) %>%
  filter(equityVar %in% equityVarsSelect) %>%
  filter(threshold %in% thresholdsSelect)

equityResultsSubsetRound <- round_table(equityResultsSubset) %>%
    rename(learner = learnerName, var = equityVar, cat = category, thres = threshold)

print(equityResultsSubsetRound)
```

For binary characteristics we are assessing for fairness, we can use a t-test
(two-sample proportion test) to determine whether there is a
statistically significant difference when comparing the fnr, fpr, or fdr
two class categories. For example, if `pVal_fp < 0.05`, then specified
learner has a statistically significant difference between the two
classes. Note that a value of `NaN` or `NA` denotes that a class had 0
values). For characteristics with multiple categories, we instead use
ANOVA to generate p-values.

```{r fairnessTests, warning = FALSE, message = FALSE}
#-------------------------#
equityVarsSelect <- equityVarsSelect
thresholdsSelect <- 0.3
#-------------------------#

learnersSelect   <- trainResults$learnerNames

equityTests <- calc_equity_tests_all(
  learnersSelect,
  equityVarsSelect,
  thresholdsSelect
)

print(round_table(equityTests))
```

# Step 5: Understang results

## Predictor relationships

The following code chunk assesses the covariate distributions
for observations predicted with a predicted outcome of `1` and those predicted outcome of `0`.

```{r predictorPlots}
#-------------------------#
learnerNameSelect <- trainResults$learnerNames
thresSelect <- thres1
#-------------------------#
  
trainPredProbsLearner <- trainPredProbs %>%
  filter(learnerName == learnerNameSelect)

# make predictions
trainPredProbsLearner$predCat <- 0
trainPredProbsLearner$predCat[trainPredProbsLearner$.pred_yes > thresSelect] <- 1
trainPredProbsLearner$predCat <- as.factor(as.character(trainPredProbsLearner$predCat))
  
predSet    <- unique(trainPredProbsLearner$predSet)
predictors <- learnerSpec$allPredsets[[predSet]]

predPlots <- generate_pred_plots(trainPredProbsLearner, predictors)

print(ggarrange(plotlist = predPlots))
```

## Variable importance

### Regression model variable importance

If your final, selected and tested learner is a regression model, the following code chunk provides standard regression output. 

```{r glmSummary}

glmLearnerNameSelect <- trainResults$learnerNames
if (substr(glmLearnerNameSelect,1,3)=="glm") {

trainPredProbsLearner <- trainPredProbs %>%
  filter(learnerName == learnerNameSelect)

predSet    <- unique(trainPredProbsLearner$predSet)
predictors <- learnerSpec$allPredsets[[predSet]]


# reload the raw testDat
testDat <- readRDS(here::here(
  learnerSpec$dataInputPath, learnerSpec$testDataName
))

# load the testing Dat with data preprocessing steps supplied from the recipe
recipe_ms <- recipes$predSet_ms
testDat <- recipe_ms %>%
  recipes::prep() %>%
  recipes::bake(new_data = testDat)

testDat[[learnerSpec$outcomeName]] <- base::ifelse(as.character(testDat[[learnerSpec$outcomeName]]) %in% c("yes"), 1,0)

glmFormula <- as.formula(paste(learnerSpec$outcomeName, "~", paste(predictors, collapse = "+")))
glmModel <- glm(glmFormula, data = testDat)
summary(glmModel)
}
```
 
 ### Machine learning algorithms
 
Forthcoming: code no longer works because vip package taken off CRAN

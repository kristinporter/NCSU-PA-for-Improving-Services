---
title: "Identifying data"
---

We want to be thoughtful about the data that we will use for our proof-of-concept and that we will split for training, validating and testing. Here are some important considerations for identifying data:

-   **The data should be representative of the population and context for the predictive analytics objective.** Here we want to make sure we have data for units that are experiencing a similar context as what we expect for the units for whom we will ultimately deploy predictive analytics.

    -   Recall our our earlier example focused on predicting TANF training program participants' risks of not finding and sustaining employment. When identifying which years of past data to include in our learner workflow, we would want to investigate whether and when eligibility requirements for the training program changed significantly. We may want to limit our data to a time period for which eligibility requirements for the training program are most similar to those currently eligible for the TANF training program.

    -   In our other example focused on predicting 10th grade students' risk of not graduating on time, we would want to investigate graduation guidelines. If we include student cohorts who had tougher or easier graduation standards when training our models compared to the cohorts for whom we apply the models, then the models may not generalize well in years outside of our training-validation-testing workflow.

-   **Important measures should be consistently available.** Here we want to assess whether we have sufficient consistency in the way measures are defined and entered into data systems. For example:

    -   Have the data integrations been consistently maintained over time? That is, if the data we aim to use are a merge of multiple underlying data sources, are those merges consistently done? If not, important measures may sometimes be missing for all units. Also, are these integrations/merges viable for the future as well? If certain data cannot be reliably acquired and merged, they should be excluded from the predictive analytics proof-of-concept. It is vital to consider not just historical data but also future scenarios. If specific data won't be accessible when we implement predictive analytics, then a proof-of-concept dependent on that data won't be genuine.

    -   Was there a rollout of a new data system that had early implementation challenges? If so, we may want to eliminate data from those early years.

    -   Is survey data part of our data universe for predictive analytics? If so, do response rates vary substantially over time? Variation in response rates could (but not necessarily) mean that different populations are being captured with each survey. We would need to investigate this.

    -   Were there broad changes in how measures where defined or coded? We may want to limit the data we will use to train, validate and test models to a period in which the measures are consistent to the present (time time close to when any model would be deployed).

        Note that inconsistencies in some measures could have implications in measures we include in our prediction sets rather than implications for how we restrict our data for our proof-of-concept. That is, if some measures change over time, we may need to exclude just those measures from our models. If there is sufficient consistency across key predictors in data we may not need to restrict the data. We will return to measure consistency later, when we discuss data quality issues.

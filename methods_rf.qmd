---
title: "Random forest"
editor: 
  markdown: 
    wrap: sentence
---

The building block of the random forest machine learning algorithm is a decision tree. A decision tree is a deterministic algorithm that makes a prediction based on predictor values. A series of nodes serve as decision points.

The following figure depicts a decision tree to predict the log salary of a baseball player (James et al. (2013)).  
The first node, or branch in the tree, considers how many years the player has been playing. If the player has been playing less than 4.5 years, they have a prediction of 5.11. If the player has been playing at least 4.5 years, the decision tree branches further. If their hitting average is less than 117.5, the log salary is 6, and if it is above, the log salary is 6.74.
A random forest is called thus because it considers many different possible decision trees (a forest). It has no initial assumptions of the functional form between predictor and response variables. The predictor and response variables could be related linearly, non-linearly, with interactively, etc. Thus, the random forest algorithm can be used in both classification and regression problems (@)
A boostrap sample is taken from the data, and a decision tree is constructed. Then, this process is repeated many times, resulting in many different possible decision trees. The final prediction is the average prediction across the different trees. By considering many trees instead of a single tree, the variance of the final predictions is reduced.
Random forests take a particular approach by constructing decorrelated trees. At every possible split, the algorithm only considers a random subset of the possible predictors, rather than considering all the possible predictors. This extra step ensures that the trees are different from each other. Without this approach, if there were a strong predictor driving the outcome, nearly all trees would have an early split based on that predictor, so the trees would be highly correlated. Variance is reduced more when taking the average of uncorrelated observations than when taking the average of correlated observations. To get a good fit, the algorithm needs a sufficiently large number of bootstrap samples/trees.
Tuning parameters
	The number of trees B is chosen to be sufficiently large that results are stable across repeated runs. B=500 is the default for the PA tool.
	The number of predictors to consider at each node m. One rule of thumb is m=âˆšp where p is the total number of predictors.
	The minimum node size is the minimum number of observations that can be in a terminal node. For the PA tool the default is 10.
	The maximum tree depth is the allowed maximum number of layers. It is similar to minimum node size in also limiting tree complexity. The PA tool does not constrain tree depth for random forests.
Besides the number of trees B, the other parameters can either be set to reasonable defaults, or chosen through cross validation.
Advantages
	Flexible modeling approach which can model non-linear relationships.
	Directly takes into account potential interactions between predictors.
	Robust to outliers.
	Not always necessary to choose tuning parameters; default parameter choices often provide good performance.
Disadvantages
	Difficult to interpret directly.
	Can result in overfitting.

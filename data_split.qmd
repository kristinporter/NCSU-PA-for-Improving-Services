---
title: "Data workflow"
---

First, we will define the concepts in the workflow for a PA proof-of-concept. This involves the following three steps:

```{mermaid}
flowchart LR
  A(Training) --> B(Validating)
  B --> C(Testing)
  
  
```
\

-   **Training:** Uses data with known outcomes to apply a modeling approach (e.g., regression or a machine learning algorithm) in order to build a model of the relationship between predictors and the outcome of interest.

-   **Validating:** uses *new* data with known outcomes to apply all trained models (from all learners). The predictions from the trained models are compared to the known outcomes and metrics of model performance and fairness are computed. This validation can be repeated in multiple validation data sets. Looking across all results, the "best" model is selected. Criteria for a model being selected as "best" vary by context. The analyst may weigh performance, fairness, simplicity/transparency and variability across multiple validations.

-   **Testing:** uses *new, set aside* data with know outcomes to apply the best model. The predictions from the best model are compared to the known outcomes to report predictive performance and fairness to stakeholders.


In the following pages, we will go over how to implement these concepts. 


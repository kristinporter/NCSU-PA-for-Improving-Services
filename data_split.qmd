---
title: "Data workflow"
---

Here we will define the workflow for a PA proof-of-concept. This involves the following three steps:

```{mermaid}
flowchart LR
  A(Training) --> B(Validating)
  B --> C(Testing)
  
  
```
\

-   **Training:** Uses data with known outcomes to apply a modeling approach (e.g., regression or a machine learning algorithm) in order to build a model of the relationship between predictors and the outcome of interest.

-   **Validating:** uses *new* data with known outcomes (data not used for training) to apply all trained models (from all learners). The predictions from the trained models/learners are compared to the known outcomes and metrics of model performance and fairness are computed. This validation can be repeated in multiple validation data sets. Looking across all results, the "best" model is selected. Criteria for a model being selected as "best" vary by context. To determine the "best" model/learner, the team may weigh performance (accuracy), fairness, simplicity/transparency and variability across multiple validations.

-   **Testing:** uses *new, set aside* data with know outcomes (data not used for training or validation) to apply the best model. The predictions from the best model are compared to the known outcomes to report predictive performance, fairness and simplicity/transparency to stakeholders.


In the following pages, we will go over how to implement these concepts. 


---
title: "Generalizing learners to new data"
---

Recall our goal of predictive analytics, as shown in this simply illustration: We use data with *known* outcomes to build a model that allows us to predict outcomes - or the likelihood of outcomes in data with *unknown* outcomes. 

![](images/WhatisPA.png){width="6in"}

How do we make sure that our model, which is based on information in the data with known outcomes, generalizes to new data? A challenge is that if our model is too closely tailored to the data we use to develop it, then we capture not only statistical patterns but also random fluctuations or noise that is specific to those data. When a model captures noise in underlying data, it will describe, or fit, the original data very well. But when applied to new, unseen data, this same model will make poor predictions. It will not fit new data because it is based on specific noise rather than generalizable patterns. A classic example of overfitting is when a learner perfectly fits to a rare instance. 

In this section, we will learn how to build (i.e., estimate, fit, train) compare and select models that do the best job in generalizing to new, unseen data. We will learn strategies for avoiding overfitting. 
